<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 8 Os métodos Ensemble | Fundamentos de Machine Learning</title>
  <meta name="description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 8 Os métodos Ensemble | Fundamentos de Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 8 Os métodos Ensemble | Fundamentos de Machine Learning" />
  
  <meta name="twitter:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2021-02-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ptII.html"/>
<link rel="next" href="redesneurais.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>1</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="1.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>1.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="1.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>1.2</b> A arte de uma IA</a></li>
<li class="chapter" data-level="1.3" data-path="i-a.html"><a href="i-a.html#vertentes-de-uma-ia-e-fundamentação-filosófica"><i class="fa fa-check"></i><b>1.3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>2</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="2.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>2.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dicio.html"><a href="dicio.html"><i class="fa fa-check"></i><b>3</b> Uma breve revisão</a><ul>
<li class="chapter" data-level="3.1" data-path="dicio.html"><a href="dicio.html#um-pouco-de-álgebra-linear"><i class="fa fa-check"></i><b>3.1</b> Um pouco de Álgebra Linear</a></li>
<li class="chapter" data-level="3.2" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística"><i class="fa fa-check"></i><b>3.2</b> Um pouco de Estatística</a></li>
<li class="chapter" data-level="3.3" data-path="dicio.html"><a href="dicio.html#medidasimport"><i class="fa fa-check"></i><b>3.3</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="3.3.1" data-path="dicio.html"><a href="dicio.html#medidasdep"><i class="fa fa-check"></i><b>3.3.1</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="3.3.2" data-path="dicio.html"><a href="dicio.html#medinfo"><i class="fa fa-check"></i><b>3.3.2</b> Medidas de Informação</a></li>
<li class="chapter" data-level="3.3.3" data-path="dicio.html"><a href="dicio.html#meddist"><i class="fa fa-check"></i><b>3.3.3</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="3.3.4" data-path="dicio.html"><a href="dicio.html#medidas-de-precisão"><i class="fa fa-check"></i><b>3.3.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="3.3.5" data-path="dicio.html"><a href="dicio.html#medidas-de-consistência"><i class="fa fa-check"></i><b>3.3.5</b> Medidas de consistência</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>4</b> Pré-processamento</a><ul>
<li class="chapter" data-level="4.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>4.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="4.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>4.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="4.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>4.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>4.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="4.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="4.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>4.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>4.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>5</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="5.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>5.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="5.1.1" data-path="valid.html"><a href="valid.html#overfitting"><i class="fa fa-check"></i><b>5.1.1</b> <strong>Overfitting</strong></a></li>
<li class="chapter" data-level="5.1.2" data-path="valid.html"><a href="valid.html#underfitting"><i class="fa fa-check"></i><b>5.1.2</b> <strong>Underfitting</strong></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="valid.html"><a href="valid.html#holdout"><i class="fa fa-check"></i><b>5.2</b> Validação cruzada Hold-out</a></li>
<li class="chapter" data-level="5.3" data-path="valid.html"><a href="valid.html#kfold"><i class="fa fa-check"></i><b>5.3</b> Validação Cruzada <em>K-fold</em></a></li>
<li class="chapter" data-level="5.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>5.4</b> ROC e AUC</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>6</b> Modelos de Aprendizagem I</a><ul>
<li class="chapter" data-level="6.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="6.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>6.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>6.2</b> Regressão</a><ul>
<li class="chapter" data-level="6.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>6.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="6.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>6.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="6.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>6.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="6.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>6.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>6.3</b> Gradiente Descendente (GD)</a><ul>
<li class="chapter" data-level="6.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>6.3.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regularizacao"><i class="fa fa-check"></i><b>6.4</b> Regularização</a><ul>
<li class="chapter" data-level="6.4.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#penalizacoes"><i class="fa fa-check"></i><b>6.4.1</b> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></a></li>
<li class="chapter" data-level="6.4.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#elasticnet"><i class="fa fa-check"></i><b>6.4.2</b> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>7</b> Algoritmos de Aprendizagem II</a><ul>
<li class="chapter" data-level="7.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>7.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="7.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>7.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="7.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>7.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>7.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>7.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>7.3</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>7.3.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="7.3.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>7.3.2</b> Estatísticas</a></li>
<li class="chapter" data-level="7.3.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>7.3.3</b> A ACP</a></li>
<li class="chapter" data-level="7.3.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>7.3.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>7.4</b> Análise de Agrupamentos - <em>Clusters</em></a><ul>
<li class="chapter" data-level="7.4.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>7.4.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="7.4.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>7.4.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="7.4.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>7.4.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ptII.html"><a href="ptII.html#knn-k-vizinhos-mais-próximos-k-nearest-neighbors"><i class="fa fa-check"></i><b>7.5</b> KNN: K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ptII.html"><a href="ptII.html#exknn"><i class="fa fa-check"></i><b>7.5.1</b> Exemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptIII.html"><a href="ptIII.html"><i class="fa fa-check"></i><b>8</b> Os métodos <em>Ensemble</em></a><ul>
<li class="chapter" data-level="8.1" data-path="ptIII.html"><a href="ptIII.html#bagging"><i class="fa fa-check"></i><b>8.1</b> <em>Bagging</em></a></li>
<li class="chapter" data-level="8.2" data-path="ptIII.html"><a href="ptIII.html#boost"><i class="fa fa-check"></i><b>8.2</b> <em>Boosting</em></a></li>
<li class="chapter" data-level="8.3" data-path="ptIII.html"><a href="ptIII.html#rf"><i class="fa fa-check"></i><b>8.3</b> Floresta Aleatória - <em>Random Forest</em></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="redesneurais.html"><a href="redesneurais.html"><i class="fa fa-check"></i><b>9</b> Redes Neurais</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ptIII" class="section level1">
<h1><span class="header-section-number">Capítulo 8</span> Os métodos <em>Ensemble</em></h1>
<p>Os métodos <strong><em>ensemble</em> (conjunto)</strong> são algoritmos de aprendizado que constroem um conjunto de classificadores e combinam os resultados de cada modelo para classificar um novo modelo de exemplo <span class="citation">(Dietterich <a href="#ref-dietterich2000ensemble">2000</a>)</span>, obtendo um valor final único a fim de melhorar a precisão e estabilidade do modelo. Os mais conhecidos são as técnicas de <strong><em>boosting</em></strong> <span class="citation">(Freund, Schapire, and others <a href="#ref-freund1996experiments">1996</a>)</span>, <strong><em>bagging</em></strong> <span class="citation">(Breiman <a href="#ref-breiman1996bagging">1996</a>)</span> e <strong><em>Random Forest</em></strong> <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>; Liaw, Wiener, and others <a href="#ref-liaw2002classification">2002</a>)</span> e <strong><em>Extra Trees</em></strong>.</p>
<p>Como é uma resposta agregada de outros modelos preditivos, tratamos de algoritmos mais complexos que necessitam de um custo computacional maior, mais tempo e com mais processos para que se tenha um desempenho melhor.</p>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">8.1</span> <em>Bagging</em></h2>
<p>O método <em>bagging</em> <span class="citation">(Breiman <a href="#ref-breiman1996bagging">1996</a>)</span> é um dos métodos de algoritmos de aprendizado de máquinas mais antigos. Este método utiliza amostras <em>bootstrap</em> - amostragem com reposição no qual por meio do conjunto de treinamento inicial, seleciona-se aleatoriamente exemplos para um novo subconjunto de treinamento <span class="citation">(Oshiro <a href="#ref-oshiro2013abordagem">2013</a>)</span>.</p>
<p>Na técnica <em>bagging</em>, portanto, diferentes subconjuntos <span class="math inline">\(T_k\)</span> são aleatóriamente elaborados, com reposição, a partir do original e tem como idéia básica criar classificadores a partir de um conjunto de dados de treinamento com distribuição uniforme de probabilidades. Cada amostra possui o mesmo tamanho da base de dados originais e por ser <em>bootstrap</em> alguns elementos podem aparecer repetidamente, ao passo que alguns podem ser que não estejam presentes no conjunto de treinamento. Cada subconjunto <span class="math inline">\(T_k\)</span> é utilizado para treinar um classificador diferente <span class="math inline">\(\{h_k(x)\}\)</span> e a classificação é definida pelo voto majoritário sobre todos os classificadores.</p>
<p>Conforme <span class="citation">(Oshiro <a href="#ref-oshiro2013abordagem">2013</a>)</span>,este método consiste então em combinar <span class="math inline">\(T\)</span> classificadores de <span class="math inline">\(N\)</span> amostras geradas a partir do conjunto de treinamento <span class="math inline">\(M\)</span> com <span class="math inline">\(R\)</span> elementos. Cada classificador possui <span class="math inline">\(m\)</span> elementos do conjunto de treinamento original de <span class="math inline">\(M\)</span>. Em vez de utilizar todas as observações do conjunto original do treinamento, escolhe elementos uniformemente com repetição e gerando <span class="math inline">\(k\)</span> exemplos, que representam aspectos originais da base de dados. Em cada exemplo o classificador é gerado independentemente e a classificação de um novo elemento será executada sobre cada um dos <span class="math inline">\(T\)</span> classificadores.</p>
<p>A cada tentativa <span class="math inline">\(t=1,2,...,T\)</span>, um conjunto de treinamento de tamanho <span class="math inline">\(N\)</span> é amostrado do conjunto de treinamento original com o mesmo tamanho. Também a cada tentativa, um classificador <span class="math inline">\(C_i\)</span> será gerado e no final um classificador <span class="math inline">\(C^*\)</span> será formado através da geração de <span class="math inline">\(T\)</span> classificadores obtidos em cada tentativa. Para uma amostra desconhecida, cada classificador <span class="math inline">\(C_i\)</span> retorna seu voto e por fim o classificador <span class="math inline">\(C^*\)</span> retornará a classe com o maior número de votos.</p>
<p>Vamos pensar numa aplicação deste método em árvores de decisão: primeiramente, o <em>bagging</em> faz um sorteio de todas as amostras - escolhe uma, sorteia e escolhe outra sucessivamente - com reposição com, por exemplo, 70% do total. Por isso podem vir elementos repetidos ou até mesmo omitir alguns (<em>bootstrap</em>). Temos agora um <em>dataset</em> para construirmos uma árvore de decisão. Da mesma forma, faz este processo com uma segunda árvore, uma terceira e assim por diante com um novo <em>dataset</em> aleatório com <em>bootstrap</em>. Note que temos então uma estimação para cada árvore diferente com dados diferentes sorteados. Um mesmo modelo de Aprendizado de Máquina com conjuntos diferentes. Com o voto majoritário (situação de classificação) ou uma média (como um caso de regressão) de todas as estimações dos classificadores <span class="math inline">\(C_i\)</span>, obtemos um classificador <span class="math inline">\(C^*\)</span> final. O método <em>bagging</em> é muito útil para evitar <em>overfitting</em> (ver @ref(#fitt)) com essa repetição do mesmo modelo de Aprendizado de Máquina. Importante notar que ele é um método para ser aplicado em algum modelo de Aprendizado de Máquina, por exemplo, pode-se optar pelo algoritmo de Regressão Linear, KNN, árvore de decisão ou em alguma técnica de mineração de dados.</p>
<p>O método <em>bagging</em> é muito útil para evitar <em>overfitting</em> (ver @ref(#fitt)) pois repetimos o modelo várias vezes com vários conjuntos aleatórios e situações com novos esimadores de treino. Ao entrar novos dados para o teste, ele terá um desempenho semelhante. Importante notar que ele é um método para ser aplicado em algum modelo de Aprendizado de Máquina, por exemplo, pode-se optar pelo algoritmo de Regressão Linear, KNN, árvore de decisão ou em alguma técnica de mineração de dados.</p>

</div>
<div id="boost" class="section level2">
<h2><span class="header-section-number">8.2</span> <em>Boosting</em></h2>
<p>O método <strong><em>boosting</em></strong> <span class="citation">(Freund, Schapire, and others <a href="#ref-freund1996experiments">1996</a>)</span> é um método de combinação de muitos classificadores com o propósito de fornecer uma classificação muito mais eficiente, considerado uma das ideias mais poderosas de aprendizagem nos últimos vinte anos <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-hastie2009elements">2009</a>)</span>. É um processo iterativo utilizado para ser alterado adaptativamente a distribuição de exemplos de treinamento, assim os classificadores de base tem como foco exemplos difíceis de classificar. Originalmente foi elaborado para problemas de classificação, mas pode ser muito bem aplicado para regressão.</p>
<p>O mais conhecido é o algoritmo <em>boosting</em> proposto por <span class="citation">Freund, Schapire, and others (<a href="#ref-freund1996experiments">1996</a>)</span>, o <strong><em>AdaBoost</em> (Adaptative Boosting</strong>.</p>

</div>
<div id="rf" class="section level2">
<h2><span class="header-section-number">8.3</span> Floresta Aleatória - <em>Random Forest</em></h2>
<p>A <strong>Floresta Aleatória</strong>, do inglês <strong><em>Random Forest</em></strong> <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>; Liaw, Wiener, and others <a href="#ref-liaw2002classification">2002</a>)</span>, é um classificador composto de classificadores projetados especialmente para árvores de decisão <span class="math inline">\(\{h_k(X),k=1,2,...,L.\}\)</span> onde <span class="math inline">\(T_k\)</span> são amostras aleatórias independentes e identicamente distribuídas e que cada árvore decide a classe mais popular para a entrada de <span class="math inline">\(X\)</span> (baixa correlação entre as árvores) . Vetores aleatórios são gerados a partir de uma distribuição e probabilidade fixa sobre o vetor de entrada inicial. A precisão da Floresta Aleatória é medida probabilísticamente em termos de margem do classificador, dado um conjunto de classificadores <span class="math inline">\(h_1(x), h_2(x),...,h_k(x)\)</span>, e um conjunto de treinamento aleatório a partir do vetor <span class="math inline">\(Y\)</span>, <span class="math inline">\(X\)</span> <span class="citation">(Gómez and others <a href="#ref-gomez2012random">2012</a>)</span>.</p>
<p>Como mencionado em <a href="ptII.html#decisiontree">7.2</a>, as Árvores de Decisão tendem a serem sensíveis à amostra de treinamento (ruídos). As Florestas Aleatórias buscam sanar este tipo de problema. A Floresta Aleatória é uma variação de <em>Bagging</em>, onde na construção da árvore, apenas um subconjunto aleatório das características participa da subdivisão de um nó. Pode-se melhorar a acurácia do modelo por meio da parametrização, que traz uma maior variação entre as árvores (mais estável que <em>bagging</em>).</p>
<p>Durante as construções das árvores, utiliza-se para medirmos o erro, o <strong><em>out of bag</em> (OOB)</strong>. Diferentemente dos erros tradicionais estimados (como validação cruzada, por exemplo), cada árvore de decisão dessa floresta construída a partir de um subconjunto (aleatório) de treinamento, pode ser testada com os exemplos que sobram (de fora) da classificação (exemplos <em>out of bag</em>). O próprio treinamento da Floresta Aleatória fornece uma estimativa de erro que denomina-se <strong>erro <em>out of bag</em> </strong>.</p>
<p>Um ótimo exemplo, elaborado e apresentado pela Ariane Machado Lima em uma de suas aulas online na Universidade de São Paulo (USP) sobre Florestas Aleatórias e o <em>out of bag</em> foi: imagine uma amostra de treinamento com duas classes e <span class="math inline">\(n=18\)</span> observações, como um dos parâmetros a serem definidos a quantidade de árvores de de decisões <span class="math inline">\(n\_estim=4\)</span> e amostras <em>bootstrap</em> (reposição) com <span class="math inline">\(obs=10\)</span> observações aleatórias em cada árvore que podem ou não serem repetidas.</p>
<p>Nesta amostra de treinamento será verificado quais elementos que situam-se fora de cada árvore composta por 10 observações (<em>OOB</em>) e quantas vezes fora para selecionar as árvores. Por exemplo, nesta mesma situação vamos supor que das três árvores de decisão, a observação 1 encontra-se dentro de uma árvore e <em>OOB</em> nas três restantes. Como a maioria das árvores não encontra-se esta observação específica, elas vão ditar a classificação desta observação (votação por maioria que pode acertar ou errar). Ao caso contrário da observação 2, por exemplo, encontra-se apenas em uma <em>OOB</em> e em três árvores de decisão. Portanto a <em>OOB</em> irá decidir a classificação desta observação. Assim sucessivamente até finalizar a contagem. Para as observações que não se encontram em nenhuma amostra, não serão contabilizadas no erro.</p>
<div class="figure" style="text-align: center"><span id="fig:outofbag"></span>
<img src="Figuras/outofbag.png" alt="Exemplificação de out of bag (MACHADO-LIMA 2020)." width="80%" />
<p class="caption">
Figura 8.1: Exemplificação de <em>out of bag</em> <span class="citation">(MACHADO-LIMA <a href="#ref-machadousprf">2020</a>)</span>.
</p>
</div>

<p>Para cada momento que temos 0 e 1 para classificar (ou até por regressão) cada elemento que estavam <em>OOB</em>, calcula-se a média das previsões como estimação de erro <em>out of bag</em>. Lembrando que permanece a medida de impureza na elaboração das árvores, como o índice de gini por exemplo.</p>
<p>Podemos dizer então que o erro <em>OOB</em> é o erro médio de predição em cada amostra de treinamento <span class="math inline">\(X_i\)</span>, no qual na construção de uma amostra-árvore haverá um conjunto de amostra de <em>bootstrap</em> (<em>in the bag</em>) e outro com dados não escolhidos no processo da amostragem (<em>out of bag</em>). Na construção da floresta (<span class="math inline">\(n\)</span> amostra-árvores) muitos exemplos de <em>bootstrap</em> e <em>OOB</em> elaborados. Estes conjuntos <em>OOB</em> podem ser agrupados em um conjunto de dados. Ao considerar <span class="math inline">\(Y\)</span> como a classe com a maioria dos votos, todas as vezes em que a observação foi considerada <em>OOB</em>, a proporção de vezes que <span class="math inline">\(Y\)</span> não for igual à verdadeira classe da observação, será a estimativa de erro <em>OOB</em>. Para o caso de problema de regressão, utiliza-se o método erro quadrado médio (MSE).</p>
<p>O procedimento <em>bootstrap</em> traz um melhor desempenho do modelo pois diminui a variância sem aumentar o viés, ou seja, embora as previsões de uma única árvore seja altamente sensível ao ruído em seu treinamento, a média de muitas árvores não será - desde que não sejam correlacionadas. Para estimarmos a incerteza das previsões de todas as árvores de regressão em <span class="math inline">\(x&#39;\)</span>, seu desvio padrão, podemos por meio da equação:</p>
<p><span class="math display" id="eq:desviorf">\[\begin{equation}
\sigma = \sqrt{\frac{\sum^B_{b=1}(f_b(x&#39;)-\hat{f})^2}{B-1}}
\tag{8.1}
\end{equation}\]</span></p>
<p>onde o <span class="math inline">\(B\)</span> é o número de árvores, um parâmetro livre que pode variar dependendo do conjunto de treinamento e a decisão do pesquisador.</p>
<p>Para calcularmos a importância de um atributo (<em>score</em>), para cada elemento <span class="math inline">\(e\)</span> e para cada árvore <span class="math inline">\(t\)</span>, permuta-se os valores de <span class="math inline">\(e\)</span> nos exemplos OOB de <span class="math inline">\(t\)</span>, ou seja: após permutarmos cada árvore terá uma nova votação em relação a cada elemento <span class="math inline">\(e\)</span> <em>OOB</em>, com novas classificações e novos erros <span class="math inline">\(OOB_p\)</span>. Podendo agora calcular a importância da variável <span class="math inline">\(e\)</span> e analizá-la como uma taxa de acréscimo sobre o erro.</p>
<p><span class="math display" id="eq:impc">\[\begin{equation}
\mbox{Importância de} \ e = \frac{(OOB_p-OOB)}{OOB}
\tag{8.2}
\end{equation}\]</span></p>
<p>O erro tradicional, geralmente utiliza-se todas as árvores como vantagem, porém é preciso dividir a amostra inicial em treinamento e teste. O <em>out of bag</em> utiliza toda a amostra, mas muitas vezes superestima o erro. Com a monitoração adequada dos parâmetros, como profundidade de cada árvore, número de atributos sorteados para cada divisão (com reposição), número de árvores, de nós por exemplo, pode ser melhorada esta superstimação e trazendo um bom modelo para o pesquisador.</p>
<p>Para aumentar a aleatoriedade no modelo. É possível utilizar o <em>bagging</em> em conjunto, onde cada novo conjunto de treinamento é criado por substituição a partir do novo vetor de entrada inicial. Uma nova árvore é induzida a partir de um novo conjunto de treinmaneto usando a seleção aleatória de atributos <span class="citation">(Gómez and others <a href="#ref-gomez2012random">2012</a>)</span>. Conforme <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>)</span>, o uso do <em>bagging</em> melhora o desempenho quando características aleatórias são utilizadas; este método também pode ser usado para fornecer estimativas contínuas do erro generalizado do conjunto combinado de árvores, bem como estimativas para força e correlação com o estimador <em>OOB</em>. A força pode ser interpretada como medida de desempenho para cada árvore, uma árvore com uma baixa taxa de erro é um classificador forte. Aumentando a força das árvores individuais, reduz-se a taxa de erro de uma floresta, assim como a baixa correlação tende a diminuir <span class="citation">(Oshiro <a href="#ref-oshiro2013abordagem">2013</a>)</span>.</p>
<p>Um algoritmo que se tornou bem conhecido e bastante similar à Floresta Aleatória, é o <strong><em>Extra-Trees</em></strong> <span class="citation">(Geurts, Ernst, and Wehenkel <a href="#ref-geurts2006extremely">2006</a>)</span>. Neste caso, é adicionado mais uma camada de aleatoriedade para montar as árvores. O algoritmo utiliza como estratégia aleatória, na montagem dos nós ao invés de utilizar métricas como ganho de informação. Este acréscimo de aleatoriedade faz com que tenha uma diminuição no viés com menor custo computacional e dispêndio de tempo <span class="citation">(Machado and others <a href="#ref-machado2020avaliaccao">2020</a>)</span>.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman1996bagging">
<p>Breiman, Leo. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24 (2): 123–40.</p>
</div>
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div id="ref-dietterich2000ensemble">
<p>Dietterich, Thomas G. 2000. “Ensemble Methods in Machine Learning.” In <em>International Workshop on Multiple Classifier Systems</em>, 1–15. Springer.</p>
</div>
<div id="ref-freund1996experiments">
<p>Freund, Yoav, Robert E Schapire, and others. 1996. “Experiments with a New Boosting Algorithm.” In <em>Icml</em>, 96:148–56. Citeseer.</p>
</div>
<div id="ref-geurts2006extremely">
<p>Geurts, Pierre, Damien Ernst, and Louis Wehenkel. 2006. “Extremely Randomized Trees.” <em>Machine Learning</em> 63 (1): 3–42.</p>
</div>
<div id="ref-gomez2012random">
<p>Gómez, Silvio Normey, and others. 2012. “Random Forests Estocástico.”</p>
</div>
<div id="ref-hastie2009elements">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-liaw2002classification">
<p>Liaw, Andy, Matthew Wiener, and others. 2002. “Classification and Regression by randomForest.” <em>R News</em> 2 (3): 18–22.</p>
</div>
<div id="ref-machado2020avaliaccao">
<p>Machado, Wylken dos Santos, and others. 2020. “Avaliação de Modelos de Classificação Automática de Atividades Diárias Para Dispositivos de Baixo Custo.”</p>
</div>
<div id="ref-machadousprf">
<p>MACHADO-LIMA, Ariane. 2020. “Reconhecimento de Padrões - Vídeo 4 Do Tema 9: Random Forests.” In. Universidade de São Paulo - Portal de vídeoaulas. <a href="http://eaulas.usp.br/portal/video.action?idItem=13856">http://eaulas.usp.br/portal/video.action?idItem=13856</a>.</p>
</div>
<div id="ref-oshiro2013abordagem">
<p>Oshiro, Thais Mayumi. 2013. “Uma Abordagem Para a Construção de Uma única árvore a Partir de Uma Random Forest Para Classificação de Bases de Expressão Gênica.” PhD thesis, Universidade de São Paulo.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ptII.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="redesneurais.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "github", "instagram"]
},
"fontsettings": ["white", "sepia", "night"],
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09.1-ModeloIFilho.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
