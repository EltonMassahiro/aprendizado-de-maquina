<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 9 Os métodos Ensemble | Machine Learning</title>
  <meta name="description" content="Tutorial de Machine Learning." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 9 Os métodos Ensemble | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Tutorial de Machine Learning." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 9 Os métodos Ensemble | Machine Learning" />
  
  <meta name="twitter:description" content="Tutorial de Machine Learning." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2021-01-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ptII.html"/>
<link rel="next" href="redesneurais.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#dicas-de-estudo"><i class="fa fa-check"></i><b>1.1</b> Dicas de estudo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>2</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="2.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>2.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="2.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>2.2</b> A arte de uma IA</a></li>
<li class="chapter" data-level="2.3" data-path="i-a.html"><a href="i-a.html#vertentes-de-uma-ia-e-fundamentação-filosófica"><i class="fa fa-check"></i><b>2.3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>3</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="3.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>3.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dicio.html"><a href="dicio.html"><i class="fa fa-check"></i><b>4</b> Um pouco de revisão</a><ul>
<li class="chapter" data-level="4.1" data-path="dicio.html"><a href="dicio.html#um-pouco-de-álgebra-linear"><i class="fa fa-check"></i><b>4.1</b> Um pouco de Álgebra Linear</a></li>
<li class="chapter" data-level="4.2" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística.-parte-i"><i class="fa fa-check"></i><b>4.2</b> Um pouco de Estatística. Parte I</a></li>
<li class="chapter" data-level="4.3" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística.-parte-ii"><i class="fa fa-check"></i><b>4.3</b> Um pouco de Estatística. Parte II</a></li>
<li class="chapter" data-level="4.4" data-path="dicio.html"><a href="dicio.html#multlagrange"><i class="fa fa-check"></i><b>4.4</b> Multiplicadores de Lagrange</a></li>
<li class="chapter" data-level="4.5" data-path="dicio.html"><a href="dicio.html#kkt"><i class="fa fa-check"></i><b>4.5</b> Karush-Kuhn-Tucker (KKT)</a></li>
<li class="chapter" data-level="4.6" data-path="dicio.html"><a href="dicio.html#bias"><i class="fa fa-check"></i><b>4.6</b> Bias</a></li>
<li class="chapter" data-level="4.7" data-path="dicio.html"><a href="dicio.html#medidasimport"><i class="fa fa-check"></i><b>4.7</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="4.7.1" data-path="dicio.html"><a href="dicio.html#medidasdep"><i class="fa fa-check"></i><b>4.7.1</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="4.7.2" data-path="dicio.html"><a href="dicio.html#medinfo"><i class="fa fa-check"></i><b>4.7.2</b> Medidas de Informação</a></li>
<li class="chapter" data-level="4.7.3" data-path="dicio.html"><a href="dicio.html#meddist"><i class="fa fa-check"></i><b>4.7.3</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="4.7.4" data-path="dicio.html"><a href="dicio.html#medidas-de-precisão"><i class="fa fa-check"></i><b>4.7.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="4.7.5" data-path="dicio.html"><a href="dicio.html#medidas-de-consistência"><i class="fa fa-check"></i><b>4.7.5</b> Medidas de consistência</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>5</b> Pré-processamento</a><ul>
<li class="chapter" data-level="5.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>5.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="5.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>5.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="5.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>5.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>5.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="5.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>5.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="5.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>5.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>5.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>6</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="6.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>6.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="6.1.1" data-path="valid.html"><a href="valid.html#underfitting-no-cenário-underfitting-o-desempenho-já-é-ruim-no-próprio-treinamento-de-seu-algoritmo."><i class="fa fa-check"></i><b>6.1.1</b> <strong>Underfitting</strong>: No cenário underfitting, o desempenho já é ruim no próprio treinamento de seu algoritmo.</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="valid.html"><a href="valid.html#validação-cruzada"><i class="fa fa-check"></i><b>6.2</b> Validação Cruzada</a></li>
<li class="chapter" data-level="6.3" data-path="valid.html"><a href="valid.html#como-escolher-um-bom-modelo"><i class="fa fa-check"></i><b>6.3</b> Como escolher um bom modelo?</a></li>
<li class="chapter" data-level="6.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>6.4</b> AOC e ROC</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>7</b> Algoritmos de Aprendizagem I</a><ul>
<li class="chapter" data-level="7.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>7.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="7.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>7.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>7.2</b> Regressão</a><ul>
<li class="chapter" data-level="7.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>7.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="7.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>7.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="7.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>7.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="7.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>7.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>7.3</b> Gradiente Descendente (GD)</a><ul>
<li class="chapter" data-level="7.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>7.3.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regularizacao"><i class="fa fa-check"></i><b>7.4</b> Regularização</a><ul>
<li class="chapter" data-level="7.4.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#penalizacoes"><i class="fa fa-check"></i><b>7.4.1</b> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></a></li>
<li class="chapter" data-level="7.4.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#elasticnet"><i class="fa fa-check"></i><b>7.4.2</b> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>8</b> Algoritmos de Aprendizagem II</a><ul>
<li class="chapter" data-level="8.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>8.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="8.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>8.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="8.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>8.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>8.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>8.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>8.3</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>8.3.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="8.3.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>8.3.2</b> Estatísticas</a></li>
<li class="chapter" data-level="8.3.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>8.3.3</b> A ACP</a></li>
<li class="chapter" data-level="8.3.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>8.3.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>8.4</b> Análise de Agrupamentos - <em>Clusters</em></a><ul>
<li class="chapter" data-level="8.4.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>8.4.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="8.4.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>8.4.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="8.4.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>8.4.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ptII.html"><a href="ptII.html#knn-k-vizinhos-mais-próximos-k-nearest-neighbors"><i class="fa fa-check"></i><b>8.5</b> KNN: K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</a><ul>
<li class="chapter" data-level="8.5.1" data-path="ptII.html"><a href="ptII.html#exknn"><i class="fa fa-check"></i><b>8.5.1</b> Exemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ptIII.html"><a href="ptIII.html"><i class="fa fa-check"></i><b>9</b> Os métodos <em>Ensemble</em></a><ul>
<li class="chapter" data-level="9.1" data-path="ptIII.html"><a href="ptIII.html#bagging"><i class="fa fa-check"></i><b>9.1</b> Bagging</a></li>
<li class="chapter" data-level="9.2" data-path="ptIII.html"><a href="ptIII.html#rf"><i class="fa fa-check"></i><b>9.2</b> Floresta Aleatória - <em>Random Forest</em></a></li>
<li class="chapter" data-level="9.3" data-path="ptIII.html"><a href="ptIII.html#boosting"><i class="fa fa-check"></i><b>9.3</b> grad boosting</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="redesneurais.html"><a href="redesneurais.html"><i class="fa fa-check"></i><b>10</b> Redes Neurais</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ptIII" class="section level1">
<h1><span class="header-section-number">Capítulo 9</span> Os métodos <em>Ensemble</em></h1>
<p>Os métodos <strong><em>ensemble</em> (conjunto)</strong> são algoritmos de aprendizado que constroem um conjunto de classificadores e combinam os resultados de cada modelo para classificar um novo modelo de exemplo <span class="citation">(Dietterich <a href="#ref-dietterich2000ensemble">2000</a>)</span>, obtendo um valor final único a fim de melhorar a precisão e estabilidade do modelo. Os mais conhecidos são as técnicas de <strong><em>boosting</em></strong> <span class="citation">(Freund, Schapire, and others <a href="#ref-freund1996experiments">1996</a>)</span>, <strong><em>bagging</em></strong> <span class="citation">(Breiman <a href="#ref-breiman1996bagging">1996</a>)</span> e <strong><em>Random Forest</em></strong> <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>; Liaw, Wiener, and others <a href="#ref-liaw2002classification">2002</a>)</span> e <strong><em>Extra Trees</em></strong>.</p>
<p>Como é uma resposta agregada de outros modelos preditivos, tratamos de algoritmos mais complexos que necessitam de um custo computacional maior, mais tempo e com mais processos para que se tenha um desempenho melhor.</p>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">9.1</span> Bagging</h2>

</div>
<div id="rf" class="section level2">
<h2><span class="header-section-number">9.2</span> Floresta Aleatória - <em>Random Forest</em></h2>
<p>A <strong>Floresta Aleatória</strong>, do inglês <strong><em>Random Forest</em></strong> <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>; Liaw, Wiener, and others <a href="#ref-liaw2002classification">2002</a>)</span>, é um classificador composto de classificadores projetados especialmente para árvores de decisão <span class="math inline">\(\{h_k(X),k=1,2,...,L.\}\)</span> onde <span class="math inline">\(T_k\)</span> são amostras aleatórias independentes e identicamente distribuídas e que cada árvore decide a classe mais popular para a entrada de <span class="math inline">\(X\)</span> (baixa correlação entre as árvores) . Vetores aleatórios são gerados a partir de uma distribuição e probabilidade fixa sobre o vetor de entrada inicial. A precisão da Floresta Aleatória é medida probabilísticamente em termos de margem do classificador, dado um conjunto de classificadores <span class="math inline">\(h_1(x), h_2(x),...,h_k(x)\)</span>, e um conjunto de treinamento aleatório a partir do vetor <span class="math inline">\(Y\)</span>, <span class="math inline">\(X\)</span> <span class="citation">(Gómez and others <a href="#ref-gomez2012random">2012</a>)</span>.</p>
<p>Como mencionado em <a href="ptII.html#decisiontree">8.2</a>, as Árvores de Decisão tendem a serem sensíveis à amostra de treinamento (ruídos). As Florestas Aleatórias buscam sanar este tipo de problema. A Floresta Aleatória é uma variação de <em>Bagging</em>, onde na construção da árvore, apenas um subconjunto aleatório das características participa da subdivisão de um nó. Pode-se melhorar a acurácia do modelo por meio da parametrização, que traz uma maior variação entre as árvores (mais estável que <em>bagging</em>).</p>
<p>Durante as construções das árvores, utiliza-se para medirmos o erro, o <strong><em>out of bag</em> (OOB)</strong>. Diferentemente dos erros tradicionais estimados (como validação cruzada, por exemplo), cada árvore de decisão dessa floresta construída a partir de um subconjunto (aleatório) de treinamento, pode ser testada com os exemplos que sobram (de fora) da classificação (exemplos <em>out of bag</em>). O próprio treinamento da Floresta Aleatória fornece uma estimativa de erro que denomina-se <strong>erro <em>out of bag</em> </strong>.</p>
<p>Um ótimo exemplo, elaborado e apresentado pela Ariane Machado Lima em uma de suas aulas online na Universidade de São Paulo (USP) sobre Florestas Aleatórias e o <em>out of bag</em> foi: imagine uma amostra de treinamento com duas classes e <span class="math inline">\(n=18\)</span> observações, como um dos parâmetros a serem definidos a quantidade de árvores de de decisões <span class="math inline">\(n\_estim=4\)</span> e amostras <em>bootstrap</em> (reposição) com <span class="math inline">\(obs=10\)</span> observações aleatórias em cada árvore que podem ou não serem repetidas.</p>
<p>Nesta amostra de treinamento será verificado quais elementos que situam-se fora de cada árvore composta por 10 observações (<em>OOB</em>) e quantas vezes fora para selecionar as árvores. Por exemplo, nesta mesma situação vamos supor que das três árvores de decisão, a observação 1 encontra-se dentro de uma árvore e <em>OOB</em> nas três restantes. Como a maioria das árvores não encontra-se esta observação específica, elas vão ditar a classificação desta observação (votação por maioria que pode acertar ou errar). Ao caso contrário da observação 2, por exemplo, encontra-se apenas em uma <em>OOB</em> e em três árvores de decisão. Portanto a <em>OOB</em> irá decidir a classificação desta observação. Assim sucessivamente até finalizar a contagem. Para as observações que não se encontram em nenhuma amostra, não serão contabilizadas no erro.</p>
<div class="figure" style="text-align: center"><span id="fig:outofbag"></span>
<img src="Figuras/outofbag.png" alt="Exemplificação de out of bag (MACHADO-LIMA 2020)." width="80%" />
<p class="caption">
Figura 9.1: Exemplificação de <em>out of bag</em> <span class="citation">(MACHADO-LIMA <a href="#ref-machadousprf">2020</a>)</span>.
</p>
</div>

<p>Para cada momento que temos 0 e 1 para classificar (ou até por regressão) cada elemento que estavam <em>OOB</em>, calcula-se a média das previsões como estimação de erro <em>out of bag</em>. Lembrando que permanece a medida de impureza na elaboração das árvores, como o índice de gini por exemplo.</p>
<p>O procedimento <em>bootstrap</em> traz um melhor desempenho do modelo pois diminui a variância sem aumentaro viés, ou seja, embora as previsões de uma única árvore seja altamente sensível ao ruído em seu treinamento, a média de muitas árvores não será - desde que não sejam correlacionadas. Para estimarmos a incerteza das previsões de todas as árvores de regressão em <span class="math inline">\(x&#39;\)</span>, seu desvio padrão, podemos por meio da equação:</p>
<p><span class="math display" id="eq:desviorf">\[\begin{equation}
\sigma = \sqrt{\frac{\sum^B_{b=1}(f_b(x&#39;)-\hat{f})^2}{B-1}}
\tag{9.1}
\end{equation}\]</span></p>
<p>onde o <span class="math inline">\(B\)</span> é o número de árvores, um parâmetro livre que pode variar dependendo do conjunto de treinamento e a decisão do pesquisador.</p>
<p>Para calcularmos a importância de um atributo (<em>score</em>), para cada elemento <span class="math inline">\(e\)</span> e para cada árvore <span class="math inline">\(t\)</span>, permuta-se os valores de <span class="math inline">\(e\)</span> nos exemplos OOB de <span class="math inline">\(t\)</span>, ou seja: após permutarmos cada árvore terá uma nova votação em relação a cada elemento <span class="math inline">\(e\)</span> <em>OOB</em>, com novas classificações e novos erros <span class="math inline">\(OOB_p\)</span>. Podendo agora calcular a importância da variável <span class="math inline">\(e\)</span> e analizá-la como uma taxa de acréscimo sobre o erro.</p>
<p><span class="math display" id="eq:impc">\[\begin{equation}
\mbox{Importância de} \ e = \frac{(OOB_p-OOB)}{OOB}
\tag{9.2}
\end{equation}\]</span></p>
<p>O erro tradicional, geralmente utiliza-se todas as árvores como vantagem, porém é preciso dividir a amostra inicial em treinamento e teste. O <em>out of bag</em> utiliza toda a amostra, mas muitas vezes superestima o erro. Com a monitoração adequada dos parâmetros, como profundidade de cada árvore, número de atributos sorteados para cada divisão (com reposição), número de árvores, de nós por exemplo, pode ser melhorada esta superstimação e trazendo um bom modelo para o pesquisador.</p>
<p>Para aumentar a aleatoriedade no modelo. É possível utilizar o <em>bagging</em> em conjunto, onde cada novo conjunto de treinamento é criado por substituição a partir do novo vetor de entrada inicial. Uma nova árvore é induzida a partir de um novo conjunto de treinmaneto usando a seleção aleatória de atributos <span class="citation">(Gómez and others <a href="#ref-gomez2012random">2012</a>)</span>. Conforme <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>)</span>, o uso do <em>bagging</em> melhora o desempenho quando características aleatórias são utilizadas; este método também pode ser usado para fornecer estimativas contínuas do erro generalizado do conjunto combinado de árvores, bem como estimativas para força e correlação com o estimador <em>OOB</em>. A força pode ser interpretada como medida de desempenho para cada árvore. Aumentando a força das árvores individuais, reduz-se a taxa de erro de um flores, assim como a baixa correlação tende a diminuir <span class="citation">(Oshiro <a href="#ref-oshiro2013abordagem">2013</a>)</span>.</p>
<p>Um algoritmo que se tornou bem conhecido e bastante similar à Floresta Aleatória, é o <strong><em>Extra-Trees</em></strong> <span class="citation">(Geurts, Ernst, and Wehenkel <a href="#ref-geurts2006extremely">2006</a>)</span>. Neste caso, é adicionado mais uma camada de aleatoriedade para montar as árvores. O algoritmo utiliza como estratégia aleatória, na montagem dos nós ao invés de utilizar métricas como ganho de informação. Este acréscimo de aleatoriedade faz com que tenha uma diminuição no viés com menor custo computacional e dispêndio de tempo <span class="citation">(Machado and others <a href="#ref-machado2020avaliaccao">2020</a>)</span>.</p>
</div>
<div id="boosting" class="section level2">
<h2><span class="header-section-number">9.3</span> grad boosting</h2>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman1996bagging">
<p>Breiman, Leo. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24 (2): 123–40.</p>
</div>
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div id="ref-dietterich2000ensemble">
<p>Dietterich, Thomas G. 2000. “Ensemble Methods in Machine Learning.” In <em>International Workshop on Multiple Classifier Systems</em>, 1–15. Springer.</p>
</div>
<div id="ref-freund1996experiments">
<p>Freund, Yoav, Robert E Schapire, and others. 1996. “Experiments with a New Boosting Algorithm.” In <em>Icml</em>, 96:148–56. Citeseer.</p>
</div>
<div id="ref-geurts2006extremely">
<p>Geurts, Pierre, Damien Ernst, and Louis Wehenkel. 2006. “Extremely Randomized Trees.” <em>Machine Learning</em> 63 (1): 3–42.</p>
</div>
<div id="ref-gomez2012random">
<p>Gómez, Silvio Normey, and others. 2012. “Random Forests Estocástico.”</p>
</div>
<div id="ref-liaw2002classification">
<p>Liaw, Andy, Matthew Wiener, and others. 2002. “Classification and Regression by randomForest.” <em>R News</em> 2 (3): 18–22.</p>
</div>
<div id="ref-machado2020avaliaccao">
<p>Machado, Wylken dos Santos, and others. 2020. “Avaliação de Modelos de Classificação Automática de Atividades Diárias Para Dispositivos de Baixo Custo.”</p>
</div>
<div id="ref-machadousprf">
<p>MACHADO-LIMA, Ariane. 2020. “Reconhecimento de Padrões - Vídeo 4 Do Tema 9: Random Forests.” In. Universidade de São Paulo - Portal de vídeoaulas. <a href="http://eaulas.usp.br/portal/video.action?idItem=13856">http://eaulas.usp.br/portal/video.action?idItem=13856</a>.</p>
</div>
<div id="ref-oshiro2013abordagem">
<p>Oshiro, Thais Mayumi. 2013. “Uma Abordagem Para a Construção de Uma única árvore a Partir de Uma Random Forest Para Classificação de Bases de Expressão Gênica.” PhD thesis, Universidade de São Paulo.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ptII.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="redesneurais.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09.1-ModeloIFilho.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
