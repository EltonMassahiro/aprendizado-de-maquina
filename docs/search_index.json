[
["ptII.html", "Capítulo 7 Algoritmos de Aprendizagem - Parte II 7.1 SVM 7.2 Árvores de Decisão 7.3 Elastic Net 7.4 KNN 7.5 K-means 7.6 Análise de Componentes Principais 7.7 Clusters 7.8 AOC e ROC 7.9 modelos nivel III 7.10 grad boosting -&gt; estudar boosting e bagging dentro de emseamble 7.11 Redes Neurais", " Capítulo 7 Algoritmos de Aprendizagem - Parte II 7.1 SVM 7.2 Árvores de Decisão 7.3 Elastic Net 7.4 KNN 7.5 K-means 7.6 Análise de Componentes Principais A Análise de Componentens Principais, popularmente conhecida como ACP ou PCA (Principal Component Analysis), em inglês, foi introduzida por Pearson (1901) e fundamentada no artigo de Hotelling (1933). É uma análise multivariada que tem como objetivo explicar a estrutura de variância e covariância de um vetor aleatório, composto por \\(p\\)-variáveis aleatórias, através da construção de combinações lineares das variáveis originais que são chamadas de componentes principais e não correlacionadas entre si (Mingoti 2007). É uma técnica bastante utilizada em diversas áreas do conhecimento, como a biologia, a agronomia, a zootécnica, a ecologia, a engenharia florestal, a medicina, a economia, entre outras áreas. Muitos sugerem o seu uso quando o volume de dados ou variáveis é grande possibilitando reduzir a dimensão da matriz de dados que compõem o conjunto de variáveis resposta com apenas poucos componentes, ou seja, \\(p\\) variáveis originais substituídas por \\(k\\) (sendo \\(k &lt; p\\)) componentes principais não correlacionadas. Vamos supor um conjunto de dados em apenas duas dimensões \\((x, y)\\) e que pode ser plotado em um plano cartesiano. Podemos verificar pelo seu comportamento que possuem alta correlação positiva. Figura 7.1: Gráfico bidimensional \\(x\\) por \\(y\\). Mas se quisermos descobrir a variação do conjunto de dados, o ACP busca encontrar um novo sistema de coordenadas em que cada ponto tem um novo valor \\((x, y)\\). Os eixos não representam algo físico, mas representam combinações de \\(x\\) e \\(y\\) que denominamos “componentes principais”, escolhidas para analisar a variação do eixo. Observe que rotacionamos o gráfico na Figura 7.2 e que após a ACP, podemos verificar a possibilidade de dercartar a componente referente ao eixo \\(y\\), visto que a componente do eixo \\(x\\) explica 99,30% da variação total dos dados, ou seja, o primeiro componente tem uma maior dispersão (variância). Possibilitando pela componente principal do eixo \\(x\\), analisar e até mesmo classificar as observações, como por exemplo, a observação 1 e 2 como um conjunto e a 3, 4 e 5 como um segundo conjunto. Figura 7.2: Gráfico de \\(x\\) por \\(y\\) rotacionado. Com mais dimensões, o ACP torna-se ainda mais útil pois possibilita observarmos o conjunto de dados num melhor ângulo. Figura 7.3: Gráfico tridimensional, em Powell, Victor and Lehe, Lewis (2014). Portanto, a ACP assume que os dados originais estão representados por características (variáveis) correlacionadas com o objetivo de transformar essas variáveis em novas (componentes principais) por meio de mudança de base do espaço vetorial que não sejam correlacionadas entre si e que estas novas variáveis (menores que as originais) retenha a maior parte da variação apresentada pelas originais, tornando possível a classificação. A suposição de normalidade não é requisito para sua técnica, mas ainda sim é conveniente padronizar (5.2.2) cada variável, permitindo que todas as variáveis tenham o mesmo peso para evitarmos viés de escala (Hongyu, Sandanielo, and Oliveira Junior 2016). A padronização das variáveis do vetor pelas respectivas médias e desvios padrões, gera novas variáveis centradas em zero e com variâncias iguais a 1. Assim, as componentes principais são determinadas a partir da matriz de covariâncias das variáveis originais padronizadas (Mingoti 2007). Agora que sabemos o que é ACP, vamos apresentar alguns conceitos de Álgebra Linear e Estatísticas para compreendermos como é aplicado este método. 7.6.1 Autovalores e Autovetores Caso ainda não tenha muito contato com a Álgebra Linear, recomendo buscar algumas literaturas a respeito. Em 1.2 encontra-se sobre Escalar, Vetores, Espaço Vetorial e Transformação Linear que serão tratadas neste tópico. Dado uma matriz \\(A_{mxn}\\) que define uma transformação linear (não muda sua dimensão), existem vetores onde sua orientação não é afetada por esta transformação, os autovetores. Figura 7.4: \\(u\\) é um autovetor de \\(T\\), porém \\(v\\) não. Um vetor é dito ser autovetor da matriz \\(A_{mxn}\\) se a transformação linear deste vetor \\(T(u)\\) é colinear a este vetor, ou seja, \\(A_{mxn}\\vec{u}=\\lambda \\vec{u}\\). Sendo que \\(\\lambda\\) é um escalar e chamado de autovalor da matriz correspondente ao autovetor. Para encontrarmos o autovetor: \\[\\begin{equation} A_{mxn}\\vec{u}=\\lambda \\vec{u} \\\\ A_{mxn}\\vec{u}-\\lambda \\vec{u}=0 \\\\ (A_{mxn}-\\lambda l)\\vec{u}=0 \\tag{7.1} \\end{equation}\\] esta equação tem solução trivial, ou seja, diferentes da nula \\((\\vec{v}\\neq 0 )\\) se e somente se, seu determinante é zero. Conhecido como Equação caracterísica e sua solução são os autovalores: \\[\\begin{equation} \\mbox{Eq. Característica}\\ \\ det(A_{mxn}-\\lambda l)=0 \\tag{7.2} \\end{equation}\\] Note também que toda transformação linear (matriz) em um espaço vetorial complexo (números imaginários) tem, pelo menos, um autovetor (real ou complexo). 7.6.1.1 Exemplo Vamos considerar um operador linear \\(T: R^2 \\rightarrow R^2\\). Com \\(T(x,y)=(4x+5y,2x+2y)\\). Quais são os autovalores a matriz \\(A=\\begin{bmatrix} 4 &amp;5 \\\\ 2 &amp;2 \\end{bmatrix}\\)? Vamos resolver a equação característica \\(det(A_{mxn}-\\lambda l)=0\\). \\[det(A_{mxn}-\\lambda l)=\\begin{bmatrix} 4 &amp;5 \\\\ 2 &amp;2 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 &amp;0 \\\\ 0 &amp;1 \\end{bmatrix} = \\begin{bmatrix} 4-\\lambda &amp;5 \\\\ 2 &amp;2-\\lambda \\end{bmatrix}\\] Com \\(det(A_{mxn}-\\lambda l)=0:\\) \\[(4-\\lambda)(2-\\lambda)-10=0 \\\\ \\lambda^2-6\\lambda-2=0 \\\\ \\mbox{resolvendo a equação: } \\ \\lambda_1 \\approx 6,32 \\ \\ \\mbox{e} \\ \\ \\lambda_2 \\approx -0,32\\] 7.6.2 Estatísticas Alguns conceitos de Estatísticas são fundamentais para que se entenda a ACP: Covariância x Correlação: como apresentado em 1.2, a covariância é semelhante à correlação (ver 5.2.2) entre duas variáveis, no entanto, elas diferem que os coeficientes de correlação são padronizados. Isso faz com que um relacionamento linear varie entre \\(-1 \\leq \\rho \\leq 1\\). A correlação mede tanto a força como a direção da relação linear entre duas variáveis. Ao caso da covariância os valores não são padronizados. Assim, a covariância pode variar de \\(-\\infty \\leq Cov (x,y) \\leq \\infty\\) demonstrando quanto \\(x\\) e \\(y\\) mudam juntas. Portanto o valor para uma relação linear ideal depende muito dos dados. Como os dados não são padronizados, é difícil determinar a força da relação entre as variáveis. Note que o coeficiente de correlação é uma função da covariância: \\[\\rho_{x,y}=\\frac{cov(x,y)}{\\sigma_x \\sigma_y}\\] Uma covariância positiva sempre resulta em uma correlação positiva e uma covariância negativa sempre resulta em uma correlação negativa. Quando temos um vetor de \\(n\\) variáveis em vez de apenas duas, iremos obter uma matriz de covariâncias ou correlação. Contendo em sua diagonal a variância \\(\\sigma^2\\), pois \\(cov(x_i,x_i)=\\sigma^2(x_i)\\), por exemplo: \\[\\begin{bmatrix} cov_{1,1} &amp;cov_{1,2=2,1} &amp;cov_{1,3=3,1} \\\\ cov_{1,1=2,1} &amp;cov_{2,2} &amp;cov_{2,3=3,2} \\\\ cov_{3,1=1,3} &amp;cov_{2,3=3,2} &amp; cov_{3,3} \\end{bmatrix} = \\begin{bmatrix} var_{1} &amp;cov_{1,2=2,1} &amp;cov_{1,3=3,1} \\\\ cov_{1,1=2,1} &amp;var_{2} &amp;cov_{2,3=3,2} \\\\ cov_{3,1=1,3} &amp;cov_{2,3=3,2} &amp; var_{3} \\end{bmatrix}\\] 7.6.3 A ACP Agora que compreendemos alguns conceitos importantes, podemos entender melhor a metodologia da ACP. Assumindo que os dados originais estão representados por variáveis correlacionadas (etapa de pré-processamento), ou seja, não independentes. Vamos ao objetivo de transformar essas \\(p\\) variáveis em outras novas \\(k\\) (com \\(k&lt;p\\)) de ordem decrescesnte de variabilidade e que não sejam correlacionadas e que as primeiras novas variáveis retenham a maior parte da variação apresentadas pelas originais a fim de podermos classificá-los. Dado um vetor \\(\\vec{u}\\) aleatório com \\(p\\) variáveis originais. O primeiro componente principal \\(y_1\\), como dito que deve ser ordem decrescente de variabilidade, será uma combinação linear do vetor \\(\\vec{u}\\) de forma que a variância \\(var(y_1)=\\sigma^2_{y_{1}}\\) seja a máxima (maior possível), ou melhor, precisamos encontrar um vetor \\(\\vec{\\beta^1}\\) tal que \\(y_1=(\\vec{\\beta^1})^T \\vec{u}\\) e \\(var(y_1=(\\vec{\\beta^1})^T \\vec{u}\\) seja máxima. De mesmo modo para \\(y_2\\) e um vetor \\(\\vec{\\beta^2}\\) e assim sucessivamente para \\(p\\) variáveis em seu banco de dados. Ao caso do Exemplo 7.6.1.1 de Autovalores e Autovetores, foi definida a transformação linear \\(T(x,y)=(4x+5y,2x+2y)\\) com as duas respectivas componentes \\(4x+5y\\) e \\(2x+2y\\). 4 e 5 da primeira componente refere-se, por exemplo, como o vetor \\(\\vec{\\beta^1}\\) que multiplicado pelos vetores originais \\(x\\) e \\(y\\), temos um novo componente desse novo espaço \\(4x+5y\\). Da mesma forma à segunda componente \\(2x+2y\\) com um \\(\\vec{\\beta^2}\\). Para facilitar a compreensão, vamos utilizar um exemplo com duas variáveis (\\(R^2\\)): AQUI VOU ARRUMAR E COLOCAR COMO TA EM MINGOTI! - Queremos encontrar a primeira componente principal \\(y_1\\), de modo que \\(var(y_1)\\) seja máxima, ou seja, encontrar um vetor \\(\\vec{\\beta^1}\\) tal que \\(y_1=(\\vec{\\beta^1})^T \\vec{u}\\) e \\(var(y_1=(\\vec{\\beta^1})^T \\vec{u}\\) seja máxima. \\[var(y_1)=var((\\vec{\\beta^1})^T\\vec{u})=var(\\vec{\\beta^1_1}\\vec{u}_1+\\vec{\\beta^1_2}\\vec{u}_2)\\\\ = (\\vec{\\beta^1_1})^2var(\\vec{u_1})+(\\vec{\\beta^1_2})^2var(\\vec{u_2})+2\\vec{\\beta^1_1}\\vec{\\beta^1_2}Cov(\\vec{u_1}\\vec{u_2})\\\\ = (\\vec{\\beta^1})^TK_{\\vec{u}} \\vec{\\beta^1}\\] Os maiores autovalores são os que orientam o sinal, os demais podem ser descartados. Porém quantos componentes principais devemos utilizar? Precisamos verificar a proporção da variação total dos dados originais que uma componente pode explicar, a partir disso selecionarmos. Lembrando que cada autovalor \\(\\lambda_i\\) refere-se a \\(var(y_i)\\). Para calcularmos a variação total, expressa-se pela somatória de todos os autovalores: \\[\\begin{equation} \\displaystyle \\sum_j \\lambda_j \\tag{7.3} \\end{equation}\\] Portanto, para analisar cada \\(i\\) componente, ou seja, cada autovalor (variação “explicada” por cada componente): \\[\\begin{equation} p_i=\\frac{\\lambda_j}{\\displaystyle \\sum_j \\lambda_j} \\tag{7.4} \\end{equation}\\] Sendo geralmente escolhido as componentes com seus respectivos autovalores que explicam entre 70%-90% segundo alguns pesquisadores. Outros como Kaiser (1960), propõe aceitar, observando diretamente, somente os autovalores iguais ou superiores à unidade. Importante: sobre utilizar matriz de covariância ou de correlação depende muito das fundamentações teóricas e recomendaçõesdos pesquisadores. Em geral, utiliza-se a matriz de correlação (quando padronizamos e elaboramos a matriz) ao caso de padronizar escalas distintas que podem viesar, como por exemplo, medidas de distância e de peso. Caso esteja utilizando software para a análise, dependendo do software utilizado com seu determinado modelo de formulação de componentes principais, pode ocorrer essa troca de sinal que nada mais é do que uma reflexão em relação ao eixo, uma rotação em seu espaço vetorial n-dimensional em torno da origem, poderá ocasionar uma “rotação” em torno do eixo. Tratando de algebra linear e suas combinações lineares, a combinação poderá possuir soluções diferentes que diferem apenas o sinal. 7.6.4 Exemplos Tomando como base exemplos de Mingoti (2007). Matriz de covariância amostral A Tabela apresenta dados relativos as 12 empresas no que se refere a 3 variáveis (medidas em unidades monetárias): ganho bruto (\\(X1\\)), ganho líquido (\\(X2\\)) e o patrimônio acumulado (\\(X_3\\)): Empresas Ganho Bruto (\\(X_1\\)) \\(Ganho Líquido (\\)X_2\\()** | **Patrimônio Líquido(\\)X_3$) E1 9893 564 17689 E2 8776 389 17359 E3 13572 1103 18597 E4 6455 743 8745 E5 5129 203 14397 E6 5432 215 3467 E7 3807 385 4679 E8 3423 187 6754 E9 3708 127 2275 E10 3294 297 6754 E11 5433 432 5589 E12 6287 451 8972 Após calcularmos suas covariâncias (recomendo o leitor calcular e verificar e atentar que por ser exemplificação, passível de ocorrência de arrendondamento dos valores), obtemos a matriz de covariância amostral: Ganho Bruto (\\(X_1\\)) Ganho Líquido (\\(X_2\\)) Patrimônio Líquido (\\(X_3\\)) Ganho Bruto (\\(X_1\\)) 9550608,6 706121,1 14978232,5 Ganho Líquido (\\(X_2\\)) 706121,1 76269,5 933915,1 Patrimônio Líquido (\\(X_3\\)) 14978232,5 933915,1 34408113,0 Para calcularmos os autovalores: \\[det(A_{mxn}-\\lambda l)=0: \\\\ \\begin{bmatrix} 9550608,6 -\\lambda &amp;706121,1 &amp;14978232,5\\\\ 706121,1 &amp;76269,5-\\lambda &amp; 933915,1 \\\\ 14978232,5&amp;933915,1&amp;34408113,0-\\lambda \\end{bmatrix}=0\\] Resolvendo o sistema, obtemos os seguintes autovalores das componentes principais: \\[\\lambda_1=38018192,2 \\ \\ \\lambda_2=2327881,5 \\ \\ \\lambda_3=19334,8\\] Para encontrarmos a porcentagem da variância explicada por cada auto valor: \\[\\%\\lambda_1=\\frac{38018192,2}{38018192,2+2327881,5+19334,8}.100\\%=94,2\\% \\\\ \\%\\lambda_2=\\frac{2327881,5}{38018192,2+2327881,5+19334,8}.100\\%=5,77\\% \\\\ \\%\\lambda_3=\\frac{19334,8}{38018192,2+2327881,5+19334,8}.100\\%=0,048\\%\\] Portanto, podemos descartar o segundo e o terceiro componente principal, pois o primeiro explica cerca de \\(94,2\\%\\). Por fim os autovetores podem sem calculados: \\[A_{mxn}\\vec{u}=\\lambda \\vec{u}\\] Com \\(A_{mxn}\\) a matriz de covariância amostral, \\(\\vec{u}\\) o autovetor e \\(\\lambda\\) os respectivos autovalores dos autovetores. \\[\\begin{bmatrix} \\vec{u_1}\\\\ \\vec{u_2} \\\\ \\vec{u_3} \\end{bmatrix} \\begin{bmatrix} 9550608,6 &amp;706121,1 &amp;14978232,5\\\\ 706121,1 &amp;76269,5 &amp; 933915,1 \\\\ 14978232,5&amp;933915,1&amp;34408113,0 \\end{bmatrix} = \\lambda_i \\begin{bmatrix} \\vec{u_1}\\\\ \\vec{u_2} \\\\ \\vec{u_3} \\end{bmatrix} \\] \\[\\mbox{substituindo os autovalores:}\\\\ \\begin{bmatrix} \\vec{u_1}\\\\ \\vec{u_2} \\\\ \\vec{u_3} \\end{bmatrix} \\begin{bmatrix} 9550608,6 &amp;706121,1 &amp;14978232,5\\\\ 706121,1 &amp;76269,5 &amp; 933915,1 \\\\ 14978232,5&amp;933915,1&amp;34408113,0 \\end{bmatrix} = \\begin{bmatrix}0,942&amp;0&amp;0 \\\\ 0&amp;0,0577&amp;0 \\\\0&amp;0&amp; 0,0048 \\end{bmatrix}\\begin{bmatrix} \\vec{u_1}\\\\ \\vec{u_2} \\\\ \\vec{u_3} \\end{bmatrix}\\] Teremos os autovetores: | | Autovetor Ganho Bruto (\\(\\vec{u_1}\\)) | Autovetor Ganho Líquido (\\(\\vec{u_2}\\)) | Autovetor Patrimônio Líquido (\\(\\vec{u_3}\\)) | |:-:|:-:|:-:|:-:| | Autovetor Ganho Bruto (\\(\\vec{u_1}\\)) | 0,425 | 0,900 | -0,099 | | Autovetor Ganho Líquido (\\(\\vec{u_2}\\)) | 0,028 | 0,096 | 0,995 | | Autovetor Patrimônio Líquido (\\(\\vec{u_3}\\)) | 0,905 | -0,426 | 0,016 | Com os autovetores, podemos elaborar as três componentes principais: \\[\\hat{y_1}=0,425(Ganho Bruto)+0,028(GanhoLíquido)+0,905(PatrimônioLíquido)\\\\ \\hat{y_2}=0,900(Ganho Bruto)+0,096(GanhoLíquido)-0,429(PatrimônioLíquido) \\hat{y_3}=-0,099(Ganho Bruto)+0,995(GanhoLíquido)+0,016(PatrimônioLíquido)\\] por meio da observação de seus resultados podemos analisar que: A primeira componente possui alta correlação-positiva com todas as três variáveis, podemos analisar como um índice de desempenho global da empresa. Pelo autovetor, podemos ver que o patrimônio possui o maior peso e de menor o ganho líquido. Podemos verificar que quanto maior for os valores das variáveis, maior será dessa componente, ou melhor, maior será o desempenho global da empresa. Esta ocupa, observando pelos autovalores, 94,\\20% de toda variação explicada, dependendo da pesquisa pode-se descartar as outras componentes. A segunda componente que ocupa 5,77% de toda variação explicada (autovalor), possui o ganho bruto e patrimônio de maior variância amostral (analisando o tabela de covariância amostra). Pelos autovetores, podemos verificar que o ganho bruto é a variável dominante com segunda maior variância amostral. Com a componente próximo a zero, entende-se que haverá um certo equilíbrio entre ganho bruto e patrimônio acumulado, o que na verdade o aumento do ganho bruto eleva-se esta componente e o patrimônio contrário. Note que há correlação bem menor entre elas. A terceira componente com pouca variância total explicada, referente ao ganho líquido de menor variância amostral, possui pouca importância. Apena o ganho líquido possui alta correlação, visto que às outras duas são próximas de zero. Determinada as componentes principais, podemos obter seus valores numéricos (escores) para cada elemento amostral. Basicamente substituímos os valores originais na funções encontradas de componentes principais (\\(y_1,y_2 \\ \\mbox{e}\\ y_3\\)): Empresas \\(CP_1\\) \\(CP_2\\) \\(CP_3\\) E1 8857,59 -165,27 -90,18 E2 8079,36 -1046,65 -158,93 E3 11257,93 2810,25 96,18 E4 -690,80 566,19 284,23 E5 3844,09 -3084,94 -30,40 E6 -5915,42 1841,62 -224,93 E7 -5504,97 -119,93 124,81 E8 -3796,38 -1367,83 -0,64 E9 -7729,15 789,46 -160,88 E10 -3848,18 -1473,28 121,59 E11 -3989,16 960,15 25,13 E12 -564,92 290,23 14,02 Podemos observar que a empresa E9 possui o menor desempenho, e as E1, E2 e E3 os melhores. Entenda que não necessariamente o sinal de negativo é sempre ser um pior valor, isso depende da pesquisa e da interpretação do sinal ou como em caso de autovetores, indica a rotação. Para analisarmos por gráfico não é recomendável utilizar neste caso, devido que são valores bem grandes para serem inseridos. No caso de Matriz de correlação, que serão padronizados os dados, podemos visualizar melhor. 7.7 Clusters 7.8 AOC e ROC 7.9 modelos nivel III 7.10 grad boosting -&gt; estudar boosting e bagging dentro de emseamble 7.11 Redes Neurais References "]
]
