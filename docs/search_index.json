[
["index.html", "Machine Learning Prefácio 0.1 Por que ler esse livro? 0.2 Estrutura 0.3 Informações a respeito do conteúdo 0.4 Agradecimentos", " Machine Learning Elton Massahiro Saito Loures 2020-11-23 Prefácio 0.1 Por que ler esse livro? 0.2 Estrutura 0.3 Informações a respeito do conteúdo 0.4 Agradecimentos install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Capítulo 1 Introdução 1.1 Dicas de estudo 1.2 Dicionário", " Capítulo 1 Introdução Caro leitor, se você veio até esse livro é bem provável que passou e/ou ainda passa pelas mesmas dificuldades que todo estudante interessado nessa área. Ao elevado número de pesquisas que fiz para aprender o que era a Inteligência Artificial, o que era o Machine Learning (Aprendizado de Máquina) e todos os outros temas similares, é nítido que ainda não está totalmente definido o conceito de cada um. É um ramo novo na área acadêmica, na indústria e em todo o mercado, com diversos temas, diversos modelos matemáticos, diversos modelos computacionais, diversos softwares, diversas aplicações e em diversas áreas. Diversos “diversos”… E o mais assustador é que esse campo une todos esses “diversos”, tornando o universo caótico ainda maior. Quando destaco o termo “caótico”, refiro exatamente pela ironia deste mote, todo esse universo confuso é aplicado em nosso cotidiano para organizar, analisar, diagnosticar e facilitar as coisas. Poucos instruem como devemos enxergar todo esse cosmos que ao longo da história está passando por diversas construções para estruturar seu conceito. Com uma tentativa de trazer isso com base em artigos, livros, vídeos, podcasts e cursos, disponho este simples livro com o propósito de organizar a imagem que você, leitor, tem de Aprendizado de Máquina e entender os principais modelos utilizados tanto no meio acadêmico, quanto no mercado de trabalho. 1.1 Dicas de estudo Não cabe a mim dizer como estudar, mas o que posso lhe aconselhar como principal ponto é a paciência. Temas como esse podem abranger qualquer campo, desde a filosofia até a área da saúde e portanto, do mesmo modo que se aplica a qualquer conteúdo, o mais importante é a base. Leia, releia, pesquise, veja vídeos, ouça um podcast, converse e discuta com colegas e professores a respeito. Não se cobre de que precisa aprender o mais rápido possível, mas preze a qualidade do estudo. Com intuito de explicar sobre Aprendizado de Máquina. Na seção AQUI VOU COLOCAR A REFERENCIA DA SESSAO, para faciliar o leitor dependendo de sua demanda de conteúdo, busquei separar em subseções a lógica computacional e a matemática. Tornando mais prático para o público que não tem interesse no modelo matemático e que busca o conhecimento de determinado assunto quanto ao público que demanda esse conteúdo. 1.2 Dicionário Assimetria e Curtose: Variância e Desvio padrão (Erro padrão): Distribuição normal: Distribuição binomial: Disitrbuição de Poisson: (Banzatto and Kronka 1992) Quando número de plantas daninhas por parcela, número de insetos capturados em armadilhas luminosas, número de pulgões ou ácaros por folhas, etc. Teorema de Bayes: quando tratamos de probabilidades, \\(P(A|B)\\) e \\(P(B|A)\\) podem ser parecidos, mas possuem grande diferença entre as probabilidades que representam. Por exemplo \\(P(A|B)\\) pode se referir sobre a probabilidade de uma pessoa que cometeu um furto (B) ser condenada (A) e \\(P(B|A)\\) seria a probabilidade de uma pessoa que foi condenada por furto ter efetivamente cometido um crime. A causa se torna o efeito e o efeito se torna a causa (Freund 2009). Pela regra geral de multiplicação que afirma que a probabilidade da ocorrência de dois eventos é o produto da probibilidade da ocorrência de um deles pela probabilidade condicional da ocorrência do outro evento, temos: \\[\\begin{equation} P(A \\bigcap B)= P(A). P(B|A) \\ \\mbox{ou} \\ P(A \\bigcap B)= P(B). P(A|B) \\tag{1.1} \\end{equation}\\] Igualando ambas expressões, temos: $ P(A). P(B|A) = P(B). P(A|B)$ e portanto, divindo por \\(P(B)\\), obtém-se o Teorema de Bayes que descreve a probabilidade de um evento, baseado em um conhecimento a priori que pode estar relacionado ao evento: \\[\\begin{equation} P(A|B) = \\frac{P(A).P(B|A)}{P(B)} \\tag{1.2} \\end{equation}\\] Para \\(B_n\\) e \\(A_k\\) atributos, podemos reescrever: \\[\\begin{equation} P(A_k|B_1,...,B_n) = \\frac{P(A_k).P(B_1,...,B_n|A_k)}{P(B_1,...,B_n)} \\tag{1.3} \\end{equation}\\] Exemplo: este exemplo pode ser encontrado em Freund (2009). Numa certa empresa, 4% dos homens e 1% das mulheres têm mais de 1,75m de altura, respectivamente, sendo que 60% dos trabalhadores são mulheres. Um trabalhador é escolhido ao acaso. Qual a probabilidade de que tenha mais de 1,75m? Solução: Temos de informação de que 60% dos trabalhadores são mulheres e que 1% delas possuem mais de 1,75m. Portanto 40% dos trabalhadores são homens, sendo 4% deles com mais de 1,75m. Logo temos que: \\[P(&gt; 1, 75m) = (0, 04 . 0.4) + (0, 01 . 0.6) = 0, 022 \\\\ → 2, 2\\% \\ \\mbox{ de probabilidade de que tenha mais de 1,75m.}\\] E que seja homem dado que o trabalhador escolhido tenha mais de 1,75m? Solução: pelo enunciado “que seja homem dado que o trabalhador escolhido tenha mais de 1,75m”, podemos perceber que já possuímos uma afirmação que já foi escolhido uma pessoa que tenha mais que 1,75m e queremos saber se é homem. Por meio da questão anterior sabemos a probabilidade P(&gt; 1,75m). Portanto: \\[P(H| &gt; 1, 75m) = \\frac{P(&gt; 1, 75m|H).P(H)}{P(&gt; 1, 75m)}=\\frac{0,04.0,4}{0, 022} \\\\ → 72,73\\% \\ \\mbox{de probabilidade de ser homem dado que seja maior que 1,75m.}\\] Função de verossimilhança: a verossimilhança \\(L\\) de um conjunto de parâmetros \\(\\theta\\), com dada informação \\(x\\). É igual a probabilidade da mesma observação \\(x\\) ter ocorrido dados os valores dos mesmos parâmetros \\(\\theta\\). Conhecendo um parâmetro \\(\\theta\\), a probabilidade condicional de \\(x\\) é \\(P(x|\\theta)\\), mas se o valor de \\(x\\) é conhecido, pode-se realizar inferências sobre o valor de \\(\\theta\\) (Bolfarine and Sandoval 2001). \\[\\begin{equation} L(\\theta |x)=P(x| \\theta) \\tag{1.4} \\end{equation}\\] Para “\\(n\\)” valores: \\[\\begin{equation} L(\\theta |x_1,..., x_n)=\\prod_{i=1}^{n} P(x_i| \\theta) \\tag{1.5} \\end{equation}\\] Geralmente utiliza-se o logaritmo natural em verossimilhança \\(L(\\theta |x)=ln L(\\theta|x)\\) como função suporte e facilitar em seu estudo. Para facilitar a compreensão, considere a observação de que você esteja ouvindo barulho em sua sala de estar num dia de natal (observação \\(x\\)), você parte da hipótese inicial que poderia ser o “Papai Noel” lhe entregando presentes (hipótese \\(\\theta\\)). A probabilidade de ser Noel lhe entregando presente apenas porque ouviu o barulho, isto é, \\(P(\\theta|x)\\) é baixa. No entanto o contrário, você com a afirmação de que é o Noel lhe entregando presentes, a probabilidade de haver barulho em sua sala de estar é bem alta, logo a verossimilhança \\(L(\\theta|x)=P(x|\\theta)\\). Parâmetros: podem ser vistos como características númericas de um modelo ou população. Os valores não podem ser mensurados diretamente mas que podem ser estimados através dos dados de uma amostra. Paramétrico: Correlação: Supervisionada x Não supervisionada: References "],
["i-a.html", "Capítulo 2 Inteligência Artificial (IA) 2.1 O que é IA? De onde veio esse conceito? 2.2 A arte de uma IA", " Capítulo 2 Inteligência Artificial (IA) 2.1 O que é IA? De onde veio esse conceito? Humano (taxonomicamente Homo sapiens), termo que derivado do latim “homem sábio”. Pensamos, analisamos, aprendemos , prevemos e manipulamos. Somos seres inteligentes. Já pesquisou o significado de “inteligência” no dicionário? É importante entender o conceito de inteligência, pois nem tudo que o ser humano faz pode ser classificado como inteligente. Aprender somar para calcular a soma de \\(2+2\\) é uma ação inteligente, mas copiar o resultado e colocar em sua folha de resultados que é 4 pode não ser tanto assim. Da mesma forma uma calculadora que executa um código passado por um humano, contendo dentro todos os passos a serem executados (algoritmos) para resolver esse cálculo, não é considerada. Quando tratamos da inteligência artificial não é fácil definir o que ela é. O seu próprio conceito vem sendo discutido e moldado ao longo do tempo. A idéia de construir uma máquina pensante ou um ser artificial que se assemelhasse aos humanos é muito antigo. O mito do Golem, por exemplo, um dos primeiros seres artificiais criados pelo homem. Dizia a lenda que o mito do Golem surgiu no século XIII quando uma matéria informe tornou-se num homúnculo a partir da invocação mágica de Elijah de Chelm que escreveu em sua fronta “Shemhamforash” - nome secreto de Deus (MOSER 2006). Na literatura foi publicado o famoso romance Frankensteins (Shelley 1818) que relata a história de um estudante que constrói um monstro em seu laboratório. Mas como ela realmente surgiu? O primeiro trabalho a ser reconhecido como IA foi elaborado por McCulloch and Pitts (1943) que tinha como propósito estudar como os neurônios podiam funcionar, modelando uma rede neural simples com circuitos elétricos. Os mesmos autores sugeriram que as redes neurais definidas em conformidade poderiam ser capazes de aprender. Por seguinte, Hebb (1949) escreveu The Organization of Behavior que fortalecia as teorias de que o condicionamento psicológico estava presente em qualquer parte dos animais. Teve como a premissa de que dois neurônios participantes de uma sinapse, têm ativação simultânea, então a força da conexão entre eles deve ser seletivamente aumentada, ou seja, os caminhos neurais são fortalecidos cada vez que são utilizados. Em 1950, o matemático Claude E. Shannon publicou um artigo sobre como “ensinar” seu computador a jogar xadrez (Shannon 1950); no mesmo ano Alan Turing, em “Computing Machinery and Intelligence” (TURING 1950), sugeriu que, ao invés de perguntarmos se as máquinas podem pensar, devemos perguntar se as máquinas podem passar por um teste de inteligência comportamental, o teste de Turing. Uma forma de avaliar se uma máquina consegue se passar por um humano em uma conversa por escrito com um avaliador passando no teste caso o avaliador não conseguisse identificar se estava conversado com um computador ou com outro ser humano. No ano seguinte, os estudantes Marvin Minsky e Dean Edmonds construíram o SNARC, o primeiro computador de rede neural que simulava uma rede de 40 neurônios. Em 1956 houve a conferência de verão em Dartmouth College (Hanover, New Hamphire), foi oficializada o nascimento da IA. John McCarthy, Minsky, Claude Shannon e Nathaniel Rochester elaboram uma proposta a fim de reunir pesquisadores dos Estados Unidos interessados em teoria de redes neurais, autômatos e estudo da inteligência: Propusemos que um estudo de dois meses e dez homens sobre inteligência artificial fosse realizado durante o verão de 1956 no Dartmouth College, em Hanover, New Hampshire. O estudo foi para prosseguir com a conjectura básica de que cada aspecto de aprendizado ou qualquer outra característica da inteligência pode, em princípio, ser descrita com tanta precisão a ponto de que uma máquina pode ser feita para simulá-la. Será realizada uma tentativa para descobrir como fazer com que as máquinas usem a linguagem, a partir de abstrações e conceitos, resolvam os tipos de problemas hoje reservados aos seres humanos e se aperfeiçoarem. Achamos que poderá haver avanço significativo em um ou mais desses problemas se um grupo cuidadosamente selecionado de cientistas trabalhar em conjunto durante o verão. — “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence” , McCarthy et al. (2006) , Agosto de 1955. Entre diversas ideias e apresentações, Allen Newell e Herbert Simon apresentaram o programa logic theorist, capaz de provar diversos teoremas e segundo Simon, capaz de pensar não numericamente. Apesar de muitos editores não se agradarem, esta importante proposta trouxe nos próximos anos, uma dominação nesse campo (Russel and NORVIG 2004): General Problem Solver (GPS), projetado por Newell and Shaw (1959), é um sistema que buscava imitar o homem na forma de resolver problemas. Concluíram de que a forma em como dividia um objetivo em sub objetivos e possíveis ações era similar à forma em como o homem fazia. Esta pesquisa ajudou a estabelecer os fundamentos teóricos dos sistemas de símbolos e forneceram à área da IA uma série de técnicas de programação voltadas à manipulação simbólica, por exemplo, as técnicas de busca heurística; a IBM produz alguns dos primeiros programas de IA, entre os quais, em 1959 o Geometry Theorem Prover; Arthur Samuel desenvolveu um programa capaz de jogar damas ao nível de um jogador de torneio. O programa jogava melhor do que o seu autor; John McCarthy no MIT, em 1958, define a linguagem de programação Lisp (List Processing) que se transformou na linguagem dominante da IA e publicou um artigo intitulado “Programs with common sense” (McCarthy 1968), onde descrevia um programa hipotético designado por “Advice taker”, o qual pode ser visto como o primeiro sistema completo da IA; Slagle (1963), com o programa SAINT, foi capaz de resolver problemas de cálcuo integral; Evans (1964) e Bobrow (1967), com os respectivos programas ANALOGY e STUDENT, resolviam problemas de análises geométricas semelhantes aos testes de QI e problemas clássicos de álgebra. Em base de Huffman (1971), Waltz (1975), Winograd (1972), Winston (1970) e Fahlman (1974), foi elaborado o mundo de blocos, que consiste em um conjunto de blocos sólidos colocados sobre uma mesa de modo que a mão de um robô reorganize-os. Claro que os primeiros sistema houveram dificuldades com problemas mais difíceis. Desde traduções que exigiam conhecimento profundo para solucionar ambiguidades, por exemplo, como situações de necessidade de hardwares melhores e limitações fundamentais nas estruturas simples. Com ressalva, em Perceptrons (Minsky and Papert 1969) demonstra que embora suas redes neurais simples (perceptrons) pudessem aprender, eram capazes de representar muito pouco. Mas com exigência da formalização acadêmica na década de 70, permitiu o desenvolvimento de sistemas com grande desempenho intelectual com perspectivas industriais e comerciais, surgindo novos sistemas dispostos a resolver problemas mais complexos do que antes: DENDRAL (Buchanan, Sutherland, and Feigenbaum 1969), analisa compostos orgânicos a fim de determinar sua estrutura molecular; MYCIN (Buchanan and Shortliffe 1984), Sistema pericial (expert system) foi capaz de diagnosticar infecções no sangue. E sucessivamente foi crescendo este enorme e maravilhoso campo. O japão lança o projeto “Fifth Generation” para construir em dez anos computadores inteligentes com capacidade de fazer milhões de inferências por segundo em 1981; uso de IA na guerra do Golfo em 1991; sistemas de perícia para casos médicos no mesmo ano; sistemas para condução de veículos automotores e detectores de colisões nas ruas (1993); reserva de viagens (1994); brinquedos inteligentes (2000); computador que se comunica ao nível de uma criança com 15 meses (2001). Ao longo dos anos da história da ciência da computação, a ênfase em algoritmos e tratamento de dados vem aumentando. 2.2 A arte de uma IA Atualmente, existem muitas atividades, pesquisas e aplicações em diversos temas que muitas vezes nem perbemos: Recomendações de mídia: com base em seu perfil de uso, o algoritmo compara filmes, músicas, clips, etc com base em vários usuários que possuem os gostos similares ao nosso. Recomedando aquilo que provavelmente irá nos agradar. Por exemplo Spotify, YouTube e Netflix. Reconhecimento de fala e assistentes virtuais: já refletiu sobre como funciona sua Google Assistente? Com ondas sonoras emitidas pela voz, o algoritmo reconhece palavras, frases e até mesmo o timbre, fornecendo respostas de acordo com o que recebe. Jogos: a inteligência artificial desenvolvida pela OpenAi conseguiu derrotar uma das melhores equipes do Dota 2 do mundo. Logística: a crise de 1991, por exemplo, no Golfo Pérsico. Foi utilizada a DART (Cross and Walker 1994), uma ferramenta que envolveu até 50.000 veículos, transporte de carga aérea e pessoa simultâneamente com o objetivo de realizar um planejamento logístico automatizado levando em conta rotas, pontos de partida e resolução de conflitos. Reconhecimento de imagens: identificação de objetos, pessoas, animais e qualquer figura com base em exemplos prévios, como por exemplo identificador de pessoas em uma foto do Facebook. Verificação de compras: detecção de comportamentos suspeitos a partir do histórico e perfil do usuário, como a e-commerce. Automóveis autônomos: por meio do algoritmo, visualiza a estrada, as placas, condição climática, outros veículos e diversos outros obstáculos para tomar decisões de seu trajeto sem a necessidade de uma pessoa. Poderíamos falar desde exemplos de inteligência artificial aplicados casos jurídicos, diagnósticos na área da saúde, identificadores de fake news (notíficas falsas) até a robótica. É uma extensa lista de exemplos na área que até hoje estão em desenvolvimento em busca de cada vez mais melhorar. A AGI (Artificial General Intelligence), ou Inteligência Artificial Geral, trabalha na criação de uma inteligência artificial generalista, similar a humana, capaz de ser especialista em uma área, mas também aprender com facilidade outras. Uma área que se tornou uma das principais linhas de pesquisa e nos dias de hoje gera discussões sobre até onde a IA pode alcançar. References "],
["vertentes-de-uma-ia-e-fundamentação-filosófica.html", "Capítulo 3 Vertentes de uma IA e fundamentação filosófica", " Capítulo 3 Vertentes de uma IA e fundamentação filosófica Os filósofos têm estado por aí há muito mais tempo que os computadores e vêm tentando resolver algumas questões que se relacionam à IA: como a mente funciona? É possível que as máquinas ajam com inteligência, de modo semelhante às pessoas, e, se isso acontecer, elas realmente terão mentes conscientes? Quais são as implicações éticas de máquinas inteligentes? “Inteligência Artificial”, RUSSEL and Norvig (2013). Com todo o desenvolvimento da IA, os algoritmos podem funcionar em níveis humanos em tarefas que aparentemente envolvem julgamento humano ou, como Turing acrescentou, “aprender a partir da experiência” e a capacidade de “distinguir o certo do errado”(RUSSEL and Norvig 2013). Paul Meehl (Meehl 1954) analisou os processos de tomada de decisão de especialistas treinados em tarefas subjetivas como prever o sucesso de um aluno em um programa de treinamento ou a reincidência de um criminoso e descobriu que algoritmos simples de aprendizado estatístico fizeram previsões melhores que os especialistas. A reflexão sobre “máquinas inteligentes e pensantes” é recente em nossa história e passa por longas discussões sobre o alcance dessa inteligência. Desde a classificação elaborada pelo filósofo John Searle em 1980, tomou-se na doutrina em geral a divisão do uso da inteligência artificial em “fraca” e “forte” (Searle 1980). A inteligência artificial fraca “nos permite formular e testar hipóteses de forma mais rigorosa e precisa”, no entanto, ela é dependente da inserção do conhecimento fornecido pelo ser humano que a programa. A máquina não é capaz de produzir raciocínios próprios, autônomos (Searle 1980; Guimarães 2019). Seartle também explica que a máquina adequadamente preparada é realmente uma mente, no sentido de que os computadores que recebem os programas certos poderiam estar, literalmente, preparados para compreender eter outros estados cognitivos (Searle 1980). Searle (1980) em seu naturalismo biológico, critica a inteligência artificial forte pois, segundo ele, as máquinas não possuem a complexidade de sistema nervoso, neurônios com axonios e dendritos e tudo mais. Para corroborar sua crítica, Searle descreve uma situação hipótetica simulando um programa que passa pelo teste de turing e que “não entende nada de suas entradas e saídas”, não havendo os requisitos para ser considerada uma mente. O sistema foi nomeado como “quarto chinês”. Ele se usa como exemplo com a situação de que não tem conhecimento da língua chinesa, estaria trancada e isolado num quarto recebendo uma folha de papel com ideogramas em chinês escritos. Por não conhecer a língua, não possui ideia alguma do que se trata. Em seguida, ele recebe uma seguda folha com ideiogramas chineses acompanhados de um conjunto de regras em inglês (língua nativa) que permitem a correlação da segunda folha com a primeira. Por fim, recebe uma terceira folha com ideogramas chineses, com regras em inglês que orientam a dar em respostas específicos ideogramas chineses associados a outros ideogramas da terceira folha, correlacionando os elementos da atual com as duas anteriores. As pessoas externas do quarto denominam a terceira folha como o “script”, a segunda folha de “história” e a primeira folha de “questões”. Essas pessoas consideram que os símbolos que Searle entregou em resposta à terceira folha são as “respostas às questões” e todo o conjunto de regras que lhe foi entrega são o “programa” (Guimarães 2019). Com o tempo Searle se torna melhor em dar respostas de acordo com as regras que permitem manipular os ideogramas chieses e de maneira similar ocorre com os programadores externos do quarto, que ficam bos em escrever os programas do ponto de vista externo. Qualquer pessoa que observa as resposta de Searle não contestaria de que Searle não fala chinês. Da mesma forma se o mesmo experimento fosse feito com textos em inglês, sua língua nativa, ele daria respostas em patamares semelhantes (Guimarães 2019). Searle conclui que no caso em chinês ele opera como um computador, respondendo corretamente mas sem a menor ideia do que está respondendo. Ao caso em inglês, ele irá responder como um ser humano e com consciência de suas respostas. O quarto se refere ao computador, o ser humano ao software de IA. Com isso ele assume que só seria possível produzir artificialmente uma máquina com sistema suficientemente semelhante a nós se poder duplicar exatamente as causas e seus efeitos, assim de fato seria possível produzir consciência, intencionalidade (fenômeno biológico dependente da bioquímica específica de suas origens) e tudo o mais usando princípios químicos diferentes dos usados por seres humanos (Searle 1980) . Em contestamento a Searle, Daniel Dennett defende o projeto de Turing porque agir inteligente consiste na capacidade de processamento de informação (Dennett 2009). Segundo Dennett, o problema da mente deve ser abordado com base na teoria evolutiva darwiniana pois o que entendemos por mental está relacionado ao tipo de resposta que nosso organismo dá para as demandas que estão para além daquelas que dizem respeito à manutenção da vida (Silveira 2013). Para ele, como ele denomina de intencionalidade intrínseca, Seartle errou em atribuir aos humanos a intencionalidade produzida exclusivamente pela interação das partes que constituem uma totalidade complexa, não necessitando de influências ou interferências externas. Para Dennet nossa intencionalidade não é original (Dennett 2009). Para Dennet o principal argumento criticando o argumento do quarto chinês, é a forma como investigamos os fenômenos mentais. É uma região que possibilita infinitas especulações, sendo o método das ciências empíricas o mais apropriado ao estudo da mente (Silveira 2013). A diferença entre ambos é de natureza filosófica com ontologias e epistemologias divergentes. É notável a importância das discussões filosóficas. O antagonismo dicotômicos dos dois filósofos possuem fundamentações que auxiliam na compreensão da mente. Quando teremos estas respostas? As máquinas serão capazes de raciocinar algum dia? Até onde uma IA pode chegar? References "],
["machinelearning.html", "Capítulo 4 O Aprendizado de Máquina 4.1 Como a máquina aprende?", " Capítulo 4 O Aprendizado de Máquina Agora que entendemos o conceito e a origem de uma IA, podemos entrar no tão esperado Machine Learning (ML). Alguns pensam erroneamente ser algo distinto de uma IA, mas é importante entender que ela é um campo específico da inteligência artificial que tem como base a ideia de que sistemas podem aprender com dados e iterações, identificar padrões para que aprimorem seu desempenho diante de problemas específicos e possam tomar decisões com a menor intervenção humana possível. Como modelos estatísticos, busca entender a estrutura dos dados modelos que atendam a certos pressupostos - muitas vezes não temos conhecimento de como essa estrutura se parece. Samuel (1959), engenheiro do MIT popularizou o termo “Machine Learning” (Aprendizado de Máquina), descrevendo o conceito com “um campo de estudo que dá aos computadores a habilidade de aprender sem terem sidos programados para tal” (Simon 2013). Com a expansão da internet e seu abundante armazenamento de dados na web, o Big data, foi necessário - ainda é - aprimorar sistemas de organização, classificação, análise de dados e identificação de padrões para tratá-los. Isso fez com que o Aprendizado de Máquina entrasse em destaque e passasse a ser uma das áreas mais importantes. Na seção 2 foi apresentado alguns exemplos de aplicações de IA, o mesmo se aplicam para o ML. Um aprendizado de máquina não é o mesmo que uma lista de instruções. Imagine uma criança aprendendo a andar de bicicleta, ela pode até receber algumas instruções para melhorar seu aprendizado, mas provável que ela irá aprender melhor com a tentativa e erro. Pedala, cai, levanta, pedala novamente e assim sucessivamente até ela realmente saber andar. Da forma similar ocorre com o Aprendizado de Máquina. 4.1 Como a máquina aprende? Para facilitar a compressão, imagine que você é um vendedor e está interessado em clietes “bons pagadores” e “maus pagadores”. Para cada cliente, possui um conjunto de dados como: idade, quantidade de faturas pagas antes do vencimento nos últimos 12 meses, quantidade de faturas atrasadas nos últimos 12 meses, região que reside, tempo de cadastro, etc. Você já se encontra com um banco de dados muito grande de clietes com seus respectivos dados e classificações como bons pagadores e maus pagadores e pretende utilizar um algoritmo de ML para aprender com esses dados de modo que, quando você receber o banco de dados de um novo cliente, esse algoritmo pode prever se a tendência desse cliente seria de bom pagador ou mau pagador. Primeiramente, você iria alimentar seu algoritmo de ML com os dado históricos que passaram por toda uma análise se havia dados faltantes, redundantes, etc e já classificados entre cliente bom pagador e mau pagador e suas respectivas características para treiná-lo. Com estes dados o algoritmo irá aprender por meio de com quais condições são necessárias para o cliente ser classificado como bom pagador ou mau pagador. Importante ressaltar que existem diferentes algoritmos de Aprendizado de Máquina que poderiam resolver esse problema, de acordo com modelos estatíticos e comandos computacionais que atendam a certos pressupostos. Como verificarmos se os dados já estão bons para aplicar o algoritmo? Quais modelos podemos aplicar? Como sabemos que essas previsões são confiáveis? Como evitar problemas de um modelo ruim? References "],
["preprocesso.html", "Capítulo 5 Pré-processamento 5.1 Dados faltantes e a Limpeza de dados 5.2 Transformação de dados 5.3 Features Selection - Seleção de atributos (SA)", " Capítulo 5 Pré-processamento Para o profissional que trabalha com Aprendizado de Máquina ou outras áreas, embora exigindo boa parte do tempo nesta etapa, é uma das mais importantes. O pré-processamento é um conjunto de atividades que buscar preparar, organizar e estruturar o banco de dados (dataset) para que possa trabalhar com os dados. Ela torna a informação de seus dados mais consistentes, com organização rígida e geralmente classificados de acordo com o seu formato (caracteres, binários, númericos, etc). Podemos dizer que ele é um conjunto de técnicas do campo de Mineração de dados (Data mining), uma outra área além de Inteligência Artificial (que engloba Aprendiza de Máquina) – que já é grande por si só - , que trat-se de uma outra dimensão de estudos e metodologias, isso sem falarmos de outros campos além destes dois. Neste tópico, vamos abordar algumas delas que são muito utilizadas nesta área. Note que em todos os procedimentos de Aprendizado de Máquina existe inúmeras metodologias para serem aplicadas em cada etapa e, de acordo com o interesse do pesquisador, pode ser utilizado diferentes estratégias com diferentes combinações. Não há uma receita de bolo, sabemos que precisamos extrair dados, pré-processalos (aplicar uma(s) estratégia para analisar, classificar os atributos, eliminar os redundantes, preencher ou eliminar os faltantes), desenvolver seus modelos de Aprendizado de Máquina, treiná-los e por fim, avaliar todo o seu modelo. É… Não é fácil, mas todo esse procedimento é fundamental para que se obtenha um modelo adequado. Portanto nesta seção busquei separar em alguns tópicos para facilitar a compreensão, porém entenda que TODAS as metodologias e estratégias podem ser combinadas e estão entrelaçadas. É como vários conjuntos em um Diagrama de Venn que estão dentro do Pré-processamento que está dentro de Mineração de dados e que está interseccionada com Aprendizado de Máquina (dentro de IA). Não se assuste: No último capítulo deste livro estará um diagrama e uma explicação mais “cronológica” de todo esse cosmos, com suas “gálaxias” e sistemas “solares” de conteúdo. 5.1 Dados faltantes e a Limpeza de dados Durante o desenvolvimento destes modelos é comum se deparar com dados faltantes em seu banco de dados e que podem ser ocasionadas por razões diversas como não preenchimento cadastral, problemas de armazenamento de dados ou até mesmo situações aleatórias não identificadas. A escolha da forma de tratar esses dados faltantes é fundamental para o modelo. Os valores faltantes total quando todas as informações são perdidas ou parcial quando somente uma parte delas são perdidas (Little and Rubin 2019), descrevem que os motivos de aparecimento de dados faltantes são comumente classificados em: Missing Completely at Random (MCAR): neste caso, as observações faltante surgiram de maneira aleatória, portanto as razões para as perdas não são relacionadas às respostas do sujeito. O uníco problema gerado pelos dados faltantes é a perda de poder da análise a ser realizada. Por exemplo, um jovem que deixou de responder uma questão de sua prova sem querer, sem motivo algum. Missing at Random (MAR): os dados faltantes dependem das variáveis preenchidas e, portanto, podem ser totalmente explicadas pelas variáveis presentes no conjunto de dados. É possível não viesar a análise, considerando as informações que causam estes dados faltantes. Como por exemplo uma pesquisa elaborada por uma universidade com a finalidade de analisar a renda das mulheres em sua cidade porém não possui recursos financeiros suficiente para entrevistar todas as mulheres. A pesquisa é respondida por uma parcela de mulheres na cidade e todas as envolvidas estão com os dados completamente observados, seria analisado uma amostra aleatória de mulheres. Missing Not at Random (MNAR): nesta situação os dados faltante são gerados de forma não mensurável, isto é, de eventos que o pesquisador não consegue observar e não tem controle. É o pior caso e algumas vezes, é necessário técnica mais robustas. Em geral,dados situados nos extremos da distribuição são mais propensos a serem faltantes (muito baixos ou altos em relação ao padrão da amostra). 5.1.1 Tratamento de dados faltantes Existem diversas metodologias de tratamentos em dados faltantes. Quando os dados são faltantes em um conjunto de dados, existem cinco grandes categorias de tratamento de análise que um pesquisador deve escolher. Como mencionado anteriormente e ainda reforço, a escolha do tratamento de análise de dados faltantes tem implicações importantes para a acurácia e o viés das estimativas. A Tabela 1 resume as definições e os maiores problemas nos cinco tipos de análises (Andrade et al. 2019). Tabela 1 : Metodologia de dados faltantes. Determinados termos estão na seção 1.2 e alguns outros serão apresentados ao longo do livro. Técnicas de Análise para dados faltantes Definições Maiores Problemas Listwise Deletion Exclui todos os casos para os quais alguns dados estão faltando Descarta dados de respondentes com respostas parciais. Menor amostra, menor potência. Viés em MAR e MNAR. Pairwise Delection Calcula as estimativas (médias, EP, correlações) usando todos os casos disponíveis com dados relevantes para cada estimativa. Diferentes correlações representam misturas de subpopulação. Às vezes, a matriz de covariância não é definida positiva. Viés em MAR e MNAR. Nenhuma amostra faz sentido para a matriz de correlação (EP impreciso). Imputação Simples Preenche cada valor faltante, por exemplo média, por regressão, etc. A imputação média (entre casos) e a imputação por regressão são ambas tendenciosas sob MCAR! Nenhuma amostra faz sentido para a matriz de correlação (EP impreciso). EP’s subestimados se você tratar o conjunto de dados como completo. Máxima Verossimilhança (MV) Estima diretamente os parâmetros de interesse a partir de uma matriz de dados incompleta; ou calcula estimativas como média, desvio padrão, ou correlação usando algum algoritmo. Não-viesada sob MCAR e MAR. Melhora à medida que adiciona mais variáveis ao modelo de imputação. Número de variáveis deve ser menor que 100. EP’S preciso para FIML. para o algoritmo EM, nenhuma amostra faz sentido para a matriz de correlação (EP impreciso). Imputação Múltipla (IM) Imputa valores faltantes várias vezes, cria-se m conjuntos de dados completamente imputados. Executa a análise em cada conjunto de dados imputado. Combina os m resultados para obter estimativas de parâmetros e erros padrão. Imparcial sob MCAR e MAR. Melhora à medida que adiciona mais variáveis ao modelo de imputação. O número de variáveis deve ser menor que 100. EP’s precisos. Fornece estimativas ligeiramente diferentes a cada vez que analisa os dados. Em Equações Estruturais, piora a convergência. Listwise deletion: exclui todos os casos para os quais alguns dados estão faltando . A eliminação dos casos frequentemente reduz muito o tamanho da amostra e o poder estatístico do teste de hipóteses. Importante o pesquisador se atentar que mesmo quando o poder do teste parece adequado, este método pode produzir estimativas de parâmetros tendenciosas sob dados faltantes sistemático (MAR e MNAR). O listwise deletion restringe a população-alvo do estudo, assim em geral quase nunca se utiliza esse procedimento. Uma vez que ele descarta dados que custaram tempo, disponibilidade dos participantes e até mesmo recursos financeiros, a eliminação desses participantes da pesquisa pode violar o princípio ético da pesquisa (Rosenthal 1994). Resumo geral: elimina todos os casos que possuem dados faltantes em sua pesquisa. Pairwise deletion: este método tenta minimizar a perda que ocorre em Listwise deletion. Como exemplo a matriz de correlação. Uma correlação como explicada em 1.2, mede a força da relação entre duas variáveis. Para cada par de variáveis para os quais os dados estão disponíveis, o coeficiente de correlação indicará a força. Em Listwise será o mesmo tamanho para todas as correlações excluindo toda observação faltante, em Pairwise deletion irá variar. Ela exclui apenas os casos que não tem respostas completas dentro da observação, aproveitando o maior número de casos possíveis. Resumo geral: ao invés de eliminar as observações (coluna ou linha inteira da matriz) com dados faltantes, como listwise deletion, este metodo elimina apenas os casos que não tem respostas completas nas combinações das observações, aproveitando o maior número possível. Imputação simples: envolve o preenchimento de cada dado faltante com uma suposição de qual deve ser o valor que está faltando no conjunto de dados. Os exemplos mais comuns de imputação simples são: imputação pela média - substituição de cada valor faltante pela média do grupo para a variável correspondente; imputação hot deck* - substituição de cada dado faltante por um valor “doador” que possui um escore similar em outras variáveis; e imputação por regressão – substituindo cada valor faltante por um valor predito com base em um modelo de regressão múltipla (será explicado conceito de regressão posteriormente), obtido a partir dos valores observados (Andrade et al. 2019). A maioria das técnicas de imputação simples é tendenciosa. Por exemplo, a imputação pela média insere uma média constante para cada valor faltante, as estimativas da variância e da correlação serão tendenciosas – mesmo que o mecanismo de dados faltantes seja completamente aleatório (MCAR). A imputação por regressão leva à subestimação da variância e superestimação da correlação (pois os valores imputados estarão exatamente na linha de regressão). Pode-se melhorar ao caso de regressão adicionando um termo de erro aleatório aos valores imputados (regressão estocástica), no entanto, ainda são imprecisas. Ao caso dos testes de hipóteses, não estima com precisão o erro padrão (Andrade et al. 2019). Resumo geral: envolve o preenchimento de cada dado faltante com uma “boa adivinhação” de qual deve ser o valor que está faltando no conjunto de dados, sendo essa estimação de acordo com o pesquisador e sua pesquisa (média, regressão, etc). Imputação múltipla (IM): cada valor faltante é substituído por dois ou mais valores imputados e ordenados a fim de representar a incerteza sobre qual valor imputar, permitindo que as estimativas das variâncias estimadas sejam calculadas com dados completos (Rubin 2004). Assim, \\(m\\) imputações atribuídas a cada valor faltante gera \\(n\\) conjuntos de dados completados que são analisados inerente aos valores observados da amostra. Muitos utilizam este método, visto que aumenta a eficiência de estimação, facilita o estudo direto da sensibilidade de inferências, abrange uma variedade de análises e geralmente válidas por incorporar incertezas devido à falta de dados. Tornando-os mais eficientes que a imputação simples, porém mais trabalhosa e ocupa mais espaço de armazenamento. Em desvantagem desse método,pode surgir discrepância na variância quando se admite pressupostos equivocados (modelo escolhido não consistente com os dados), com isso um \\(m\\) pequeno se torna mais adequado com menor gravidez. Uma das característica mais importantes desse método é que os valores faltantes para cada envolvido é predito a partir de seus próprios valores observados, com o ruído aleatório adicionado para preservar uma correta quantidade de variabilidade nos dados imputados (Schafer and Graham 2002). Schafer (1999) recomenda que a quantidade necessária de imputações para que a estimativa de conjunto de dados tenha relativa eficiência, com a seguinte equação: \\[\\begin{equation} RE=\\sqrt{1+\\frac{\\lambda}{m}} \\tag{5.1} \\end{equation}\\] onde, é \\(m\\) é a quantidade do conjunto de dados completados e \\(\\lambda\\) é a taxa de informação - caso fosse 50% dos dados faltantes, \\(\\lambda=0,5\\). Claro que o método para mensurar a quantidade necessária varia de acordo com o tema da pesquisa e a escolha do pesquisador. Dependendo área que o pesquisador está interessado, pode-se haver outras recomendações para mensurar a quantidade. A IM é composto basicamente por três passos (Assunção 2012): Imputação dos dados: são gerados m bancos de dados completos através de técnicas adequadas que devem levar em conta ao máximo a relação entre os dados faltantes e os observados. Existe diversos métodos que podem ser utilizadas para este primeiro passo, um dos mais utilizados atualmente é o método de regressão linear bayesiana - ao caso de não entender o que são as técnicas de Regressão linear nem de Bayes, as seções XXXXXXXXXXXX instruem. Este método tem como resposta a variável que possui dados faltantes (\\(Y\\)) e como variáveis preditoras são utilizadas as demais variáveis presentes (\\(X_1, X_2,..., X_k\\)), com \\(k\\) número de preditoras. Na abordagem Bayesiana, a regressão linear é formulada através de distribições de probabilidade ao invés da abordagem clássica. Seu modelo será: \\[Y_i \\sim N(\\beta^T X_k , \\sigma ^2 I)\\] A variável dependente \\(Y_i\\) é gerada a partir de uma Distribuição Normal (Gaussiana) 1.2 caracterizada pela média e variância (\\(\\sigma^2\\). A média é o produto entre os parâmetros \\(\\beta\\) e variáveis independentes \\(X_k\\). O objetivo deste método é determinar a distribuição posterior para os parâmetros do modelo ao invés de encontrar um único valor. A resposta e seus parâmetros são gerados por meio de uma distribuição de probabilidade. Para encontrar as distribuições dos parâmetros do modelo, a inferência bayesiana utiliza o Teorema de Bayes para combinar informações prévias ao experimento e dados de amostra com o objetivo de deduzir as propriedades sobre um parâmetro de interesse a partir dos dados de entrada \\(X_k\\) e de saída \\(Y\\). A aplicação de Bayes neste contexto seria: \\[\\begin{equation} P(\\beta|y,X)=\\frac{P(y|\\beta,X)P(\\beta|X)}{P(y|X)} \\tag{5.2} \\end{equation}\\] onde \\(P(\\beta|X)\\) reflete a incerteza de \\(\\beta\\). Qualquer informação que se tenha inicialmente sobre o parâmetro é tratado como ela (pode ser utilizada como não informativa). Em \\(P(y|\\beta,X)\\) é a verossimilhança que diz respeito a distribuição característica dos dados (interpretada como no caso clássico). O denominado \\(P(y|X)\\) é tratada como uma constante de normalização para a equação e reflete a probabilidade que pode-se obter qualquer dado. Ressalto que existe diversos métodos nesta primeira etapa e recomendo o leitor interessado, buscar outras literaturas. Análise dos bancos de dados gerados pelo passo 1: ao criar o conjunto de dados imputados, é importante fazer uma análise separadamente para cada um dos \\(m\\) banco de dados da mesma forma como tradicionalmente se faz, o modelo pode variar de acordo com o pesquisador - são apresentadas na seção SEIS AQUI COLOCAR A SEÇÃO DEPOIS. Combinar os resultados: com as análises realizadas, precisa-se combinar os resultados apropriados para obter a inferência da imputação repetida. Por meio do passo 2, obtém-se estimativas para o parâmetro de interesse \\(D\\). Estas estimativas podem ser qualquer medida escalar como médias, variâncias, correlações, coeficientes de regressão por exemplo. A estimativa \\(D\\) será a combinação será a média das estimativas individuais. \\[\\begin{equation} \\overline{D}=\\frac{1}{m}\\displaystyle \\sum^{m}_{s=}\\hat{D}_s \\tag{5.3} \\end{equation}\\] Em seguida, a variância combinada é calculada: \\[\\begin{equation} T =\\overline{E}+ (1+\\frac{1}{m})F \\tag{5.4} \\end{equation}\\] em que \\(\\overline{E}= \\frac{1}{m}\\displaystyle \\sum^{m}_{s=} E_s\\) é a média das variâncias que preserva a variabilidade natural (\\(E\\)) do parâmetro de interesse nos \\(m\\) banco de dados e \\(F=\\frac{1}{(m+1)}\\displaystyle \\sum^{m}_{s=}(\\hat{D}_s-\\overline{D})^2\\) o componentes que estima a incerteza causada pelos dados faltantes. Se \\(F\\) for muito pequeno as estimativas dos parâmetros são muito semelhantes, com menos incerteza. Do contrário as incertezas variam muito. Resumo geral: a imputação múltipla executa uma rotina de imputação simples repetidamente (múltiplas associações sobre os valores plausíveis) e consegue estimar sem víes o erro padrão. Ocorre as imputações muitas vezes contabilizando a imprecisão de cada imputação. Método de máxima verossimilhança (EM - Expecativa-maximização): proposto por Fisher (1912) , é um método paramétrico (ver 1.2) que parte do princípio de especificar como a função de verossimilhança (ver 1.2) deveria ser utilizada como um instrumento de redução de dados Casella and Berger (2010). Este método consiste na escolha do conjunto de valores para os parâmetros que torne um máximo a função de verossimilhança. A inferência de verossimilhança pode ser considerada como um processo de obtenção de informação sobre um vetor de parâmetros \\(\\theta\\), a partir do ponto \\(x\\) do conjunto amostral, por meio da função de verossimilhança. Vários vetores podem produzir a mesma verossimilhança, reduzindo a informação de \\(\\theta\\) (Cordeiro 1999). O objetivo é encontrar uma estimativa do parâmetro \\(\\theta\\), \\(\\hat{\\theta}\\), que maximize a verossimilhança. Portanto, utiliza-se o conceito de derivada (diferenciação) e igualamos a zero (Bolfarine and Sandoval 2001). \\[\\begin{equation} L&#39;(\\theta;x)=\\frac{\\delta L(\\theta;x)}{\\delta \\theta}=0 \\tag{5.5} \\end{equation}\\] Para inferir se é um ponto máximo, aplica-se a segunda derivada e verificar se o resultado é menor que zero (Bolfarine and Sandoval 2001). \\[\\begin{equation} L&#39;&#39;(\\hat{ \\theta};x)=\\frac{\\delta^2 log L(\\theta;x)}{\\delta \\theta^2}&lt;0 \\tag{5.6} \\end{equation}\\] Com algoritmo EM (Expectativa-maximização), por Dempster, Laird, and Rubin (1977) é um procedimento que realiza a estimativa dos parâmetros (vetor de médias e a matriz de covariância) por meio da máxima verossimilhança em conjuntos amostrais incompletos (dados faltantes) e pode ser utilizado como uma ferramenta para inserção de dados. Por um processo iterativo, na etapa E(Estimação/Esperança) se estima os dados faltantes para completar a matriz dos dados, no caso calcula-se a esperança condicional (média condicional) da função de log-verossimilhança; no passo M (Maximização), com os dados completados, encontra-se um \\(\\hat{\\theta}\\) que maximiza a esperança condicional da log-verossimilança e então seu resultado é usado para fazer a inferência no passo E e assim sucessivamente até que o algoritmo processado tenha convergido, ou seja, a diferença entre o valores da verossimilhança dos dados incompletos na \\(k\\)-ésima e na \\((k+1)\\)-ésima iteração seja tão pequena (Enders 2010 ; Pereira 2019). Resumo geral: o algoritmo EM, faz a etapa E com a função de verossimilhança para encontrar um valor médio e preencher os dados faltantes, faz a etapa M utilizando a máximização de verossimilhança para encontrar um valor médio com o menor erro possível e continua, a partir do resultado do segundo passo, sucessivamente até convergir no melhor valor e menor erro possível (global) para preencher os dados faltantes. Além de dados faltantes, é possível lidarmos com grande volume de dados. Por isso, o processamento computacional se torna cada vez mais complexo e para aumentarmos a eficiência e reduzir os custos usamos o processo de redução de dados ou a hierarquização para separarmos os conjuntos a serem estudados. Pode-se por meio de Agregação de cubo de dados (atividade de construção de um cubo de dados) que apesar de gerar maior necessidade de armazenamento, permite um processamento mais rápido por não necessitar varrer toda a base em busca de determinado valor. A Seleção de subconjuntos de atributos para utilizar os atributos altamente relevantes em detrimento dos menos relevantes (como por exemplo verificar pela significância). Ou também reduzir a numerosidade ou dimensionalidade que permitem que os dados seja estimados por alternativas de representação de dados menores e compactados e alguns métodos para hierarquizar as variáveis. Na seção de XXXXXXXXXXXX serão apresentados as principais estratégias. 5.1.2 Outlier Um outlier é um valor que se encontra distante da normalidade e que provavelmente causará anomalias nos resultados obtidos, pois pode viesar negativamente todo o resultado de uma análise e que seu comportamento pode ser justamente o que está sendo procurado. São basicamente dados que se diferenciam drasticamente dos outros, conhecidos como anomalias, pontos fora da curva, dados discrepantes, ruídos, e que estão fora da distribuição normal. Pode-se verificar dados incomuns apenas verificando a taebla, mas dependendo do tamanho de seu banco de dados não é uma boa recomendação. Uma das melhores maneiras de identificarmos dados outliers é utilizando gráficos. Ao plotar um gráfico o analista consegue verificar que existe algo diferente. Como exemplo, um estudo no sistema de saúde brasileiro pela AQUARELA (2017) utilizando dados da prefeitura de Vitória no Espírito Santo, analisando fatores que levam as pessoas a não comparecerem em consultas agendadas no sistema público de saúde da cidade. Padrões encontrados de que mulheres comparecerem muito mais que os homens e crianças faltam poucos às consultas, porém, uma senhora outlier, com 79 anos agendou uma consulta e com 365 dias de antecedência apareceu à consulta. Neste caso, convém ser estudado o outlier pelo comportamento trazer informações relevantes que podem ser adotadas para aumentar a taxa de assiduidade nos agendamentos. Outlier do caso indicado pela seta vermelha 5.1. Figura 5.1: “Gráfico de estudo no sistema de saúde apresentando outlier (AQUARELA 2017).” Por diversos motivos pode ocorrer de ter presença de outlier nos dados e podem viesar negativamente todo resultado de uma análise e seu comportamente pode muitas vezes ser o que justamente o pesquisador está procurando. Há possibilidade do outlier ser importante para o pesquisador entender o por que da anomalia estar acontecendo, ou para identificar algum dado extraído erroneamente, por exemplo. Uma maneira mais complexa e muito precisa, é de identificá-los através de análise dos dados. Encontrando a distribuição estatísticas que mais e aproxima à distribuição dos dados e utilizar métodos estatísticos para detectar as anomalias. Como por exemplo o uso de histograma e a distribuição normal para verificar os dados que estão dentro e fora do intervalo de confiança (ver 1.2 Distribuição normal). 5.2 Transformação de dados 5.2.1 Tipos de datasets A escolha das medidas estatísticas para sua análise ou modelo de Aprendizado de Máquina dependem muito dos tipos de dados das variáveis em observação. Estes tipos de dados podem ser numéricos (como uma sala de aula, com alunos que variam sua altura de 1,51 metros a 1,98 metros) e categórico (como uma classificação num hospital de pacientes doentes ou não doentes), embora esses dois tipos podem ser subdivididos como números inteiros e ponto flutuante para variáveis numéricas e booleano, ordinal ou nominal para variáveis categóricas. As subdivisões mais comuns são: - Variáveis Numéricas: 1. Variáveis inteiras (exemplo: \\(1,2,3,..., n\\)); 2. Variáveis de ponto flutuante (parte fracionária, por exemplo: 1,17; 0,10; 47,2). Variáveis categóricas: Variáveis booleanas (dicotômicas, binárias: Verdadeiro e Falso). Variáveis ordinais (1º, 2º, 3º, etc). Variáveis nominais (não possuem ordenação como por exemplo, cor dos olhos: azuis, castanhos, pretos e verdes). Importante ressaltar que quando trabalhamos dentro da programação, possuem mais tipos além de int (númericos inteiros) char (caracteres) e float (pontos flutuantes), como o double que armazena números com ponto flutuantes com precisão dubla com o dobro da capacidade de float, string como cadeia de caracteres. Muitos algoritmos possuem a limitação de trabalhar somente com atributos qualitativos (variáveis categóricas), com isso muitas vezes é necessário aplicar algum método capaz de transformar um atributo quantitativo em um atributo qualitativo (faixas de valores). Uma estratégia que cresce ao longo do tempo é o processo de discretização que transforma atributos contínuos em atributos discretos como por exemplo, dividir alturas entre menor que 1,70 metros e maior igual que 1,70 metros. Dependendo do estudo pode ser adequado, embora o pesquisador precisa tomar muito cuidado pois é provável que possar perder algumas informações. De mesmo modo, é possível transformar variáveis categóricas em númericas, como por exemplo classificar tamanhos como pequeno = 1, médio = 2 e grande = 3 possibilitando por meio do mapeamento manter a ordem dos valores (Batista and others (2003)). É bem comum estes tipos de tratamento de dados ao caso de datas, como trabalhos que aplicam-se séries temporais em que o pesquisador precisa estudar a sazonalidade de algum objeto de estudo. A soja por exemplo pode-se analisar sua tendência ao longo dos anos, mas quando tratamos os dados e analisamos em outro período podemos verificar que possui sazonalidades em sua produção. Em análises para investimentos também, atentar o comportamento mensal e diário das ações de uma empresa, muitas vezes está com tendência de alta num âmbito mensal, porém ao analisar diariamente é possível que esteja em baixa. Para facilitar a compreensão, considere a série temporal AirPassengers que representa o número de passageiros mensalmente em uma empresa de transporte aéreo ao período de 1949 a 1960 (Box and Jenkins 1976). Figura 5.2: “Gráfico de estudo no sistema de saúde apresentando outlier (AQUARELA 2017).” Para o campo de transformação de dados e séries temporais, ao leitor que pretende ir mais a fundo nestes outros “galhos” de estudos. Recomendo buscar outras literaturas que tem como foco este temas. Em discretizações por exemplo, Dougherty, Kohavi, and Sahami (1995) e Garcia et al. (2012) abordam diversos métodos que podem agradá-lo. 5.2.2 Normalização e padronização Muitos conjuntos de dados apresentam atributos contínuos que espalham-se em diferentes faixas de valores ou possuem distintas variações, devido às suas naturezas ou escalas em que foram medidas. Estas diferenças podem ser fundamentais e precisam ser levadas em conta (CARVALHO et al. 2011). Em situações também para validarmos a análise variância precisa-se dos requisitos de atiditividade, independência, normalidade e homogeneidade de variâncias - será apresentada em ANOVA seção XXXXXXXX. Quando alguma das características mencionadas acontece ou não verifica seus requisitos o pesquisador, antes de fazer uma análise não-paramétrica (1.2), pode-se transformar seus dados (Banzatto and Kronka 1992). Normalização por reescala: através de um valor mínimio e um máximo, gera um novo intervalo onde os valores de um atributo estão contidos. Um intervalo entre 0 e 1. \\[\\begin{equation} x_{ij}=\\frac{x_{ij}-min_j}{max_j-min_j} \\tag{5.7} \\end{equation}\\] sendo \\(x_i\\) a observação de ordem \\(i\\), \\(min_j\\) e \\(max\\) os valores mínimos e máximos do atributo \\(j\\) respectivamente. Transformação de raiz quadrada: frequentemente utilizada para dados de contagens que geralmente segue uma distribuição de Poisson (1.2), onde a média é igual à variância (Banzatto and Kronka 1992). \\[\\begin{equation} \\sqrt{x_i} \\tag{5.8} \\end{equation}\\] sendo \\(x_i\\) representando as observações do banco de dados. Quando ocorrem zeros ou valores baixos (menores que 10 ou 15), recomenda-se \\(\\sqrt{x+0,5} \\ \\mbox{ou} \\sqrt{x+1,0}\\) (Banzatto and Kronka 1992). Transformação angular: recomenda-se para dados expressos em porcentagens, que geralmente seguem a distribuição binomial (1.2). Atualmente existe tabelas apropriadas para essa transformação (Banzatto and Kronka 1992). Segundo Banzatto and Kronka (1992) porcentagens entre 30% e 70% ou as porcentagens são resultantes da divisão dos valores observados nas parcelas por um valor constante tornam-se desnecessárias e pode-se analisar diretamente os dados originais, mas atente-se pois algumas vezes variar essas exceções de acordo com sua área e pesquisador que a propõe. \\[\\begin{equation} arc \\ sen \\sqrt{\\frac{x}{100}} \\tag{5.9} \\end{equation}\\] Transformação logaritmica: quando verificada determinada proporcionalidade entre as médias e desvios padrões dos diversos tratamentos. É geralmente utilizada para problemas de assimetria (1.2). Em casos, por exemplo, tratamentos com amplitude alta como uma população numerosa que varia de 1.000 a 10.000 indivíduos ou tratamentos de baixa amplitude de 10 a 100 indivíduos. Esta trasformação pode ser útil. \\[\\begin{equation} log(x) \\ \\mbox{ou} ln(x) \\tag{5.10} \\end{equation}\\] Uma vez transformados os dados em logaritmos, a soma de dados logarítmicos não tem o mesmo valor que a soma de seus antilogaritmos, mas representa o produto destes. Padronização: é um método muito utilizado por diversas áreas de pesquisa. Neste caso diferentes atributos podem abranger diferentes intervalos, porém possuir os mesmos valores para alguma medida de posição e de variação (CARVALHO et al. 2011). Imagine você como economista interessado em avaliar o desempenho da produção de soja com as variáveis econômicas e monetárias o Brasil e possui as seguintes variáveis: produção de soja anual medida em milhares de toneladas, taxa básica de juros SELIC medida em porcentagem, receita média anual em milhares de reais, área plantada de soja medida em hectares. Já podemos perceber que todos possuem medidas e grandezas bem diferente uma das outras. Este o propósito da padronização, deixar com que todas as variáveis tenham uma medida em comum. \\[\\begin{equation} x_{ij}=\\frac{x_{ij}-\\overline{X}}{S_j} \\tag{5.11} \\end{equation}\\] em que \\(\\overline{X_j}\\) e \\(S_j\\) representam a média e o desvio padrão do atributo \\(j\\) respectivamente. Após a transformação todos os atributos terão a média zero e desvio-padrão unitário. Caso transformado seu banco de dados e seu banco de dados apresentarem uma distribuição contínua não-normal, ou não-homogênea ou não-aditiva, não há outra alternativa senão utilizar a estatística não-paramétrica. Resumo geral: Muitos conjuntos de dados apresentam atributos contínuos que espalham-se em diferentes faixas de valores ou possuem variações diferentes, por motivo de suas naturezas ou escalas medidas. Estas diferenças podem ser muito importantes e precisam ser levadas em conta para não causar erros em sua pesquisa. Para isso usam-se alguns métodos para transformar seus dados para que possam ser trabalhados, apresentados os principais neste livro. Em situações para fazermos análise variância precisa-se também ser transformado seus dados caso não cumpra seus requisitos. Caso o problema ainda persistir, precisa-se utilizar estatística não-paramétrica. 5.3 Features Selection - Seleção de atributos (SA) Uma literatura que achei bastante interessante foi Parmezan et al. (2012). Seguindo sua estrutura a respeito de Seleção de atributos. Podemos definir SA como a determinação de um subconjunto ótimo de atributos, partindo de algum critério ou medida de importância, que representa a informação importante dos dados (Parmezan et al. 2012). Extraímos um subconjunto de \\(P\\) atributos a partir de um conjunto original de \\(N\\) atributos, sendo \\(P\\leq M\\) (Parmezan et al. 2012; Liu and Motoda 1998; Lee 2005). A cada conjunto de dados com \\(M\\) atributos, existem \\(2^M\\) subconjuntos de atributos candidatos (Langley and others 1994). Existem diversas metodologias para selecionarmos os atributos que podem variar em sentido de buscas e estratégias para a seleção. Repare que os tópicos mencionados anteriormente também são utilizados para remoção e seleção, foi fragmentado apenas para facilitar a compreensão. O “sentido de busca” influencia na determinação do(S) ponto(s) de partida no espaço de busca, ou seja, na direção em que a busca será realizada e os operadores que serão utilizados. Elas são categorizadas, seguindo Parmezan et al. (2012) e Liu and Motoda (2008), em: • Forward Selection - Seleção para Frente: o estado inicial é estabelecido como vazio (subconjunto vazio de atributos), e os atributos são incluídos um por vez; • Backward Elimination - Eliminação por Trás: o ponto de partida é iniciado com o conjunto de todos os atributos (completo), tais quais são removidos sucessivamente; • Bidirectional Search - Pesquisa Bidirecional: como o próprio nome diz, duas buscas são processadas simultâneamente. Ambas terminam quando atingem o centro do espaço de busca, ou quando uma das buscas encontra os melhores atributos antes de alcançar o centro do espaço de busca; • Random Search - Pesquisa Aleatória: com o propósito de evitar que a busca fique restrita a ótimos locais. Não tem uma direção específica para buscar, pois o ponto de partida da busca e o modo de adicionar ou remover atributos são decididos aleatoriamente. Além dos sentidos de busca, existem diversas abordagens que avaliam subconjuntos de atributos e que podem remover tanto atributos irrelevantes quanto redundantes (Parmezan et al. 2012; Liu and Motoda 2008). A seguir, as principais abordagens: • Filter - Filtro: Com a finalidade de filtrar atributos não importantes, essa abordagem é feita antes da construção dos modelos. A ideia é simplesmente receber como entrada o conjunto de exemplos descrito utilizando somente o subconjunto de atributos importantes identificados. Ela ocorre antes do aprendizado de máquina (John, Kohavi, and Pfleger 1994) e utiliza-se métodos estatísticos diversos para esta seleção, como por exemplo árvores de decisão ou as “medidas de importância” que são apresentadas na próxima seção. • Wrapper- Empacotar: ocorre também externamente ao algoritmo de aprendizado. Este método gera um subconjuto candidato de atributos, executa o algoritmo de aprendizado considerado somente esse subcojunto selecionado de treinamento e avalia a precisão desse classificador. Repete-se esse processo para cada subconjunto de atributos até buscar um bom modelo. Como exemplo temos a análise por arvores de decisão e florestas aleatórias (serão apresentadas mais a frente). Tem como desvantagem o custo operacional desta abordagem. Exemplo de aplicações: Naive Bayes e Máquina de vetores de suporte para classificação. • Embedded - Embutida: é realizada internamente pelo próprio algoritmo de extração de padrões. Esta estratégia seleciona o subconjunto de atributos no processo de construção do modelo de classificação, durante a fase de treinamento, e geralmente são específicos para um dado algoritmo de aprendizado. A principal diferença dos métodos do tipo embedded e wrapper, é que em embedded depende em relação a um modelo preditivo específico, assim não permite a sua implementação em combinação com outros modelos (Souza 2014). Observação e resumo geral: Note que o que muitas vezes confunde o leitor é o excesso de categorias - que ironicamente tem o propósito de organizar e facilitar. Basicamente são estratégias diferentes com sentidos diferentes de se iniciar a busca de atributos que podem ser irrelevantes ou relevantes: antes de criar um modelo de Aprendizado de maquina; usa-se um modelo de aprendizado para selecionar os atributos antes de iniciar uma etapa de análise [pode-se até mesmo realizar outro algoritmo de aprendizado após este algoritmo de seleção] ou a própria seleção com a análise [mesmo algoritmo para selecionar e concluir]). Quando misturamos esta estratégia, denominamos de híbridos. Figura 5.3: “Diferença de Filter, Wrapper e Embedded respectivamente (modificado de Covões (2010)).” References "],
["algoritmos-de-aprendizagem.html", "Capítulo 6 Algoritmos de Aprendizagem 6.1 Medidas de importância 6.2 Teste de hipóteses e Análise de Variância 6.3 Naive Bayes 6.4 Regressão 6.5 Gradiente Descendente 6.6 SVM 6.7 Arvores de Decisão 6.8 Elastic Net 6.9 KNN 6.10 K-means 6.11 PCA 6.12 Clusters 6.13 AOC E ROC", " Capítulo 6 Algoritmos de Aprendizagem Existe uma infinidade de algoritmos utilizados em machine learning, cada um com uma finalidade específica. Há também características que podem inviabilizar a escolha do modelo mais preciso para determinado problema, como a utilização alto poder computacional. 6.1 Medidas de importância Um atributo é dito importante se quando removido a medida de importância considerada em relação aos atributos restantes é deteriorada , seja a precisão da medida, consistência, informação, distância ou dependência Tradução de Liu and Motoda (2012). É fundamental estimarmos a importância de um atributo, tanto uma avaliação individual quanto à avaliação de subconjuntos de atributos. É uma questão complexa e multidimensional (Liu and Motoda 2012). Podemos avaliar se os atributos selecionados pela etapa do pré-processamento auxiliam a melhorar a precisão do classificador ou a simplifcar algum modelo construído. A seguir, apresenta-se algumas medidas utilizadas (Lee 2005). 6.1.1 Medidas de Informação As medidas de informação determinam o ganho de informação a partir de um atributo. O ganho de informação é definido como a diferença entre a incerteza a priori e a incerteza a posteriori considerando-se o atributo \\(X_i\\). \\(X_i\\) é preferido ao atributo \\(X_j\\) se seu ganho de informação for maior que de \\(X_j\\). Uma das mais utilizadas é a entropia que normalmente é usada na teoria da informação para medir a pureza ou impureza de um determinado conjunto. Shannon (1948), tomou como “ponto de partida” encontrar uma forma matemática de medir o quanto de informação existe na transmissão de uma mensagem de um ponto a outro, denominando-a entropia. Sua proposta baseava-se na ideia de que o aumento da probabilidade do próximo símbolo diminuiria o tamanho da informação. Com isso, a entropia pode ser definida como a quantidade de incerteza que há em uma mensagem e que diminui à medida que os símbolos são transmitidos (vai se conhecendo a mensagem), tendo-se então a informação, que pode ser vista como redução da incerteza (Shannon 1948; Paviotti and Magossi 2019). Por exemplo: ao utilizarmos como idioma a nossa língua portuguesa e ao transmitir como símbolo a letra “q”, a probabilidade do próximo símbolo ser a letra “u” é maior que a de ser qualquer outro símbolo, enquanto que a probabilidade de ser novamente a letra “q” é praticamente nula (Paviotti and Magossi 2019). Shannon define que a entropia pode ser calculada por meio da soma das probabilidades de ocorrência de cada símbolo pela expressão \\(∑ p_i = 1 = 100\\%\\), em que \\(p_i\\) representa a probabilidade do i-ésimo símbolo que compõe a mensagem. Segundo ele, estes símbolos devem ser representados através de sequências binárias, utilizando das propostas de Nyquist (1924) e Hartley (1928). Sua proposta consistia em representar símbolos de um alfabeto através de um logaritmo de acordo com suas respectivas unidades de informação. A entropia proposta por ele é obtida pela média das medidas de Hartley (Moser and Chen 2012). Se A é discreto com distribuição de probabilidade \\(p(A)\\), a entropia será: \\[\\begin{equation} H(A)=- \\sum p(A)log_2(p(A)) \\tag{6.1} \\end{equation}\\] Para facilitar a compreensão, vamos supor um exemplo de um questionário com resposta binária entre “Sim” e “não”: quanto mais distribuído as probabilidades das respostas, mais desorganizada é, logo maior suaa entropia, do contrário caso for uma probabilidade de ser zero “sim”/“não” ou de ser 1 (100%), ou seja, ter apenas uma opção de resposta, será menos distribuído e portanto menor usa entropia. Figura 6.1: Gráfico de Probabilidade x Entropia. O ganho de informação portanto mede a redução da entropia (nesse caso) causada pela partição dos exemplos de acordo com os valores do atributo. \\[\\begin{equation} \\mbox{Ganho de Informação}(D,T)=\\mbox{entropia}(D)-\\displaystyle \\sum_{i=1}^k \\frac{|D_i|}{|D|}. \\mbox{entropia}(D_i) \\tag{6.2} \\end{equation}\\] É muito utilizado em algoritmo de Árvore de decisão que será apresentado nesta mesma seção com um exemplo de seu uso. 6.1.2 Medidas de Distância Também conhecidas com medidas de separabilidade, discriminação e divergência. Em caso de duas classes, um atributo \\(X_i\\) é preferido ao atributo \\(X_j\\) se fornece uma diferença maior que \\(X_j\\) entre as probabilidades condicionais das duas classes. Uma das mais utilizadas é a distância Euclidiana. 6.1.3 Medidas de Dependência Figura 6.2: Padrões de correlação. Elaborado por Gujarati and Porter (2011) e adaptado Henri (1978). 6.1.4 Medidas de Precisão 6.1.5 Medidas de consistência 6.2 Teste de hipóteses e Análise de Variância 6.3 Naive Bayes Antes de falarmos sobre este algoritmo, vamos para o conceito matemático. Em (1.2) tratamos do Teorema de Bayes para \\(n\\) atributos. Colocando-o como probabilidade condicional: \\[\\begin{equation} p(A|B_{1},...,B_{n}) = \\\\ p(A)p(B_{1}|A)p(B_{2}|A,B_{1}),p(B_{3}|A,B_{1},B_{2})...p(B_{n}|A,B_{1},B_{2},...,B_{n−1}) \\tag{6.3} \\end{equation}\\] Assumindo que cada atributo \\(B_i\\) é condicionalmente independente de todos os outros \\(B_j\\) para \\(j\\neq i\\) e \\(p(B_i|A,B_j)=p(B_i|A)\\) o modelo poderá ser expresso como: \\[\\begin{equation} p(A_k|B_1,...,B_n)=p(A_k)p(B_1|A_k)p(B_2|A_k),...=p(A_k)\\prod_i^n p(B_i|A_k) \\ k ∈{1,...,k} \\tag{6.4} \\end{equation}\\] Por fim para podermos classificarmos, aplicamos argumentos de máxima para otimizarmos a função, obtém-se o classificador de Naive Bayes: \\[\\begin{equation} \\mbox{classificador} \\ \\hat{y}=argmax \\ p(A_k)\\displaystyle \\prod_{i=1}^n p(B_i|A_k) \\ \\ k ∈{1,...,k} \\tag{6.5} \\end{equation}\\] Lembrando que para cada atributo, a sua distribuição de probabilidades é assumida como normal. O Naive Bayes é uma técnica de classificação baseado no teorema de Bayes com uma suposição de independência entre os preditores, ou seja, este classificador assume que a presença de uma característica particular em uma classe não está relacionada com a presença de qualquer outro fator. Por exemplo, uma fruta verde, redonda e com um tamanho de diâmetro X pode ser uma melancia, porém mesmo que estas variáveis dependam uns dos outros e de outras características, todas estas propriedades contribuem de forma independente para a probabilidade de que seja uma melancia. Este modelo é muito utilizado devido que é fácil de construir e particularmente útil para grandes volumes de dados. Porém a própria independência entre os preditores a torna desvantajosa para apliação prática e que variáveis categóricas num conjunto de dados de teste que não foram treinadas, não irá estimar essa nova variável. Exemplo: para facilitar, podemos supor que estamos trabalhando no diagnóstico de uma nova doença e que foi feito testes em 100 pessoas aleatórias (exemplo de Orgânica Digital (2019)). Após coletarmos a análise, descobrimos que das 100 pessoas, 20 possuíam a doença (20%) e 80 pessoas estavam saudáveis (80%), sendo que das pessoas que possuíam a doença, 90% receberam o resultado positivo no teste da doença, e 30% das pessoas que não possuíam a doença também receberam o teste positivo. Caso uma nova pessoa realizar o teste e receber um resultado positivo, qual a probabilidade de ela realmente possuir a doença? Figura 6.3: Dados coletados de uma amostra de 100 pessoas aleatórias. Com o algoritmo de Naive Bayes, buscamos encontrar uma probabilidade da pessoa possuir a doença, dado que recebeu um resultado positivo, multiplicando a probabilidade de possuir a doença pela probabilidade de “receber um resultado positivo, dado que tem a doença”. De mesmo modo verificar a probabilidade de não possuir a doença dado que recebeu um resultado positivo. Ou seja, ao caso de ter a doença dado que o resultado deu positivo: \\[P(doença|positivo) = 20% * 90% \\] \\[P(doença|positivo) = 0,2 * 0,9 \\] \\[P(doença|positivo) = 0,18\\] Para o caso de não ter a doença, dado que deu positivo: \\[P(não \\ doença|positivo) = 80% * 30%\\] \\[P(não \\ doença|positivo) = 0,8 * 0,3\\] \\[P(não\\ doença|positivo) = 0,24\\] Após isso precisamos normalizar os dados, para que a soma das duas probabilidades resulte 1 (100%). Como vimos em pré-processamento 5, a Normalização por reescala por meio de um valor mínimio e um máximo, gera um novo intervalo onde os valores de um atributo estão contidos. Um intervalo entre 0 e 1. Portanto, dividimos o resultado pela soma das duas probabilidades. \\[P(doença|positivo) = 0,18/(0,18+0,24) = 0,4285\\] \\[P(não doença|positivo) = 0,24/(0,18+0,24) = 0,5714\\] Logo, podemos concluir que se o resultado do teste da nova pessoa for positivo, ela possui aproximadamente 43% (0,4285) de chance de estar doente. Observação e resumo geral: Naive Bayes é uma técnica de classificação baseado no teorema de Bayes com uma suposição de independência entre os preditores diferentemente do caso em 1.2 (Teorema de Bayes), ou seja, O Naive Bayes assume que a presença de uma característica particular em uma classe não está relacionada com a presença de qualquer outro fator. Ao caso da melancia, uma fruta verde, redonda e com um tamanho de diâmetro X é possível ser ela, porém mesmo que estas variáveis dependam uma das outras e de outras características, elas contribuem de forma independente para a probabilidade de que seja uma melancia. É um modelo simples de construir e útil para grandes volumes de dados. Porém a própria independência entre os preditores a torna desvantajosa para apliação prática e que variáveis categóricas num conjunto de dados de teste que não foram treinadas, não irá estimar essa nova variável. Por isso Naive vem do significado “ingênuo”, pois como a Figura 6.4 demonstra, os atributos contribuem de forma independente para a probabilidade de A. Figura 6.4: Gráfico de Probabilidade x Entropia. 6.4 Regressão 6.4.1 Análise de Regressão Linear A análise de variância, pressupõe a independência dos efeitos dos diversos tratamentos utilizados no experimento. Quando a hipótese não é verificada, necessitamos refletir a dependência entre os efeitos dos tratamentos. No caso de experimentos quantitativos, frequentemente justifica a existência da equação de regressão, que une os valores dos tratamentos aos analisados. Em grande parte, trata de estimação e/ou previsão do valor médio (para população) da variável dependente com base nos valores conhecidos da variável explanatória, ela é supervisionada. Como na prática não conseguimos análisar uma população, trabalhamos em cima de amostras e estimamos para o todo, para que possamos fazer uma aproximação. Partimos da ideia de estimarmos uma função com dados amostrais com o menor erro possível. Portanto, o \\(Y_i\\) (população) observado pode ser expresso como: \\[\\begin{equation} Y_i=\\hat{Y_i}+\\hat{\\mu_i} \\tag{6.6} \\end{equation}\\] E o modelo para função de regressão amostral: \\[\\begin{equation} Y_i=\\hat{\\beta_0}+\\hat{\\beta_1 X_i}+\\hat{\\mu_i} \\tag{6.7} \\end{equation}\\] em que: \\(\\hat{Y_i}\\) é o valor observado com \\(i\\) níveis de \\(X\\) (estimador da esperança \\(E(Y|Xi)\\)), \\(\\hat{\\beta_0}\\) a constante de regressão estimado e intercepto de \\(\\hat{Y}\\), \\(\\hat{\\beta_1}\\) o coeficiente de regressão estimado que seria a variação de \\(\\hat{Y}\\) em função da variação de cada unidade de \\(X\\), \\(X_i\\) com \\(i\\) níveis da variável independente e \\(\\hat{\\mu_i}\\) é o erro associado à distância entre o valor observado e o correspondente ponto na curva. Note que os “chapéis” em cima das variáveis é utilizado quando referimos a estimações, ou seja, são variáveis de dados amostrais e não a população. Mas como estimâmetros os parâmetros da função de forma que fique mais próxima possível e com o menor erro? Com o Método dos Mínimos Quadrados (MMQ) atribuído ao Carl Friedrich Gauss - matemático alemão - torna-se possível estimar os melhores \\(\\beta_0\\) e \\(\\beta_1\\) que minimizam os erros. Como não podemos observar a função de regressão populacional (FRP), precisamos estimálo por meio da função de regressão amostral: \\[Y_i=\\hat{\\beta_0}+\\hat{\\beta_1}X_i+\\hat{\\mu_i} \\\\ Y_i=\\hat{Y_i}+\\hat{\\mu_i} \\\\ \\mbox{Logo temos que} \\rightarrow \\ \\hat{\\mu_i}=Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i\\] Podemos ver que os erros \\(\\hat{\\mu_i}\\) (resíduos) são basicamente as diferenças entre os valores observados e estimados de \\(Y\\). Ao caso de dados com \\(n\\) pares de observações de \\(Y\\) e \\(X\\), queremos encontrar a FRA que se encontra o mais próximo possível do \\(Y\\) observado, ou seja, escolher a FRA de modo que a soma dos resíduos \\(\\sum \\hat{\\mu}_i=\\sum(Y_i-\\hat{Y_i})\\) seja a menor possível. Porém, como se pode ver pelo diagrama de dispersão na Figura 6.5, os erros possuem a mesma importância com variações entre sinais positivos e negativos e sua somatória será zero. Isso dificultari a possibilidade de minimizarmos. Figura 6.5: Critério do minímos quadrados Gujarati and Porter (2011). Para evitarmos isso, utilizamos o critério dos mínimos quadrados, de modo que elevamos os resíduos ao quadrado. Fazendo isso, o método dá mais peso aos resíduos (não irão mais se anular), podendo visualizar melhor o “tamanho” do erro total e obter propriedades estatísticas mais desejáveis. \\[\\begin{equation} \\sum \\hat{\\mu}^2_i=\\sum(Y_i-\\hat{Y_i})^2 \\\\ = \\sum (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1 X_i})^2 \\tag{6.8} \\end{equation}\\] O método dos mínimos quadrados nos oferece estimativas únicas de \\(\\beta_0\\) e \\(\\beta_1\\) que proporcionam o menor valor possível (encontrando \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\)) de \\(\\sum \\hat{\\mu}_i\\). Por meio de cálculo diferenciável (recomendo o leitor interessado em se aprofundar na definição matemática buscar literaturas em foco estatísticoler, como por exemplo a seção 3A de Gujarati and Porter (2011)) encontra-se: \\[\\begin{equation} \\sum Y_i=n\\hat{\\beta_0} + \\hat{\\beta_1} \\sum X_i \\tag{6.9} \\end{equation}\\] \\[\\begin{equation} \\sum Y_i X_i=\\hat{\\beta_0} \\sum X_i + \\hat{\\beta_1} \\sum X_i^2 \\tag{6.10} \\end{equation}\\] AQUI VOU COLOCAR DO JEITO Q FIZ EM ECONOMETRIA COM AS DEFINCOES E AS DERIVADAS Para que seja feito o modelo de regressão, ela depende das premissas: independência das variáveis erro, homogeneidade das variâncias, normalidade e relação linear entre as variáveis. Coeficiente de determinação \\(r^2\\): medir a qualidade de seu ajuste Estimamos os parâmetros e o erro da função, agora precisamos considerar a qualidade do ajuste da linha de regressão ajustada a um conjunto de dados, ou seja, vamos descobrir quão “bom” o ajuste dessa linha de regressão amostral é adequada aos dados. Se todas as observações estivessem exatamente em cima da linha de regressão, seria “perfeito”, o que raramente acontece e provávelmente seria um problema de Overfitting (será apresentado no próximo capítulo para verificarmos a validade do modelo). O coeficiente de terminação \\(r^2\\) é um medida que diz quanto a linha de regressão amostral ajusta-se aos dados. Para entendermos melhor, vamos visualizar por Diagrama de Venn (Kennedy 1981). O círculo \\(Y\\) representa a variação da variável dependente \\(Y\\) e o círculo \\(X\\), a variação da variável explanatória \\(X\\) como vimos em regressão linear. A área sombreada indica o quanto em que a variação de \\(Y\\) é explicada pela variação de \\(X\\). Quanto maior a área sobreposta, maior a parte da variação de \\(Y\\) é explicada por \\(X\\). O coefiente de determinação \\(r^2\\) é apenas a medida numérica dessa sobreposição. Na Figura 6.6, conforme move-se da esquerda para a direita, a sobreposição aumenta, ou seja, uma proporção cada vez maior da variação de \\(Y\\) é explicada por \\(X\\) (o \\(r^2\\) aumenta). Sem sobreposição, \\(r^2=0\\) e com total sobreposição, \\(r^2=1\\), pois 100% da variação de \\(Y\\) é explicada por \\(X\\). Portanto o coefienciente situa-se no intervalo entre 0 e 1. Figura 6.6: Critério do minímos quadrados Gujarati and Porter (2011). Podemos chegar ao coeficiente de determinação apenas por manipulação algébrica: \\[\\mbox{sabemos que:} \\ y_i=\\hat{y}_i+\\hat{\\mu}_i \\\\ \\mbox{elevando ao quadrado e somando a amostra:} \\ \\sum y^2_i=\\sum \\hat{y}^2_i+\\sum \\hat{\\mu}^2_i+2\\sum \\hat{y}_i \\hat{\\mu}_i \\\\ \\mbox{como} \\ \\sum \\hat{\\mu}_i=0, \\ \\mbox{temos que:}\\ \\sum y^2_i= \\hat{y}^2_i+\\sum \\hat{\\mu}^2_i \\\\ \\sum y^2_i=\\hat{\\beta}^2_1 \\sum x_i^2+\\sum \\hat{\\mu}^2_i \\] \\[\\begin{equation} \\mbox{podemos dizer} \\ SQT=SQE+SQR \\tag{6.11} \\end{equation}\\] sendo SQT a soma total dos quadrados, SQE a soma do quadrados explicados e SQR soma dos quadrados dos resíduos. \\[\\mbox{dividindo a equação anterior por SQT:}\\\\ 1=\\frac{SQE}{SQT}+\\frac{SQR}{SQT} \\\\ \\mbox{definindo}\\ r^2 \\ \\mbox{como:} \\ \\frac{SQE}{SQT} \\] \\[\\begin{equation} \\mbox{obtemos:} \\ r^2=1-\\frac{SQR}{SQT} \\rightarrow 1 - \\frac{\\sum \\hat{\\mu}_i}{\\sum (Y_i - \\overline{Y}_i)^2} \\tag{6.12} \\end{equation}\\] Por manipulação algébrica, podemos verificar também que \\(r^2=\\hat{\\beta}^2_1(\\frac{S^2_x}{S^2_y})\\), sendo \\(S^2_x\\ \\mbox{e} \\ S^2_y\\) as respectivas variâncias amostrais de \\(X\\) e \\(Y\\). Note que ao aplicarmos a raiz quadrada no coeficiente de determinação obtemos o coeficiente de correlação visto em 6.1.3, que mede o grau de associação entre duas variáveis. \\[r=\\pm \\sqrt{r^2}\\] AQUI VOU COLOCAR UM EXEMPLO DE REGRESSOA PARA ENTENDER E PARTE MATEMATICA e falar de ANOVA Não esqueça: dependendo das variáveis em estudo é possível que haja comportamento polinomial ao observarmos no gráfico, podendo ser quadrática, cúbica, etc. Os procedimentos são os mesmos de que linear, mas basicamente incluímos a variável e seu respectivo grau. Dependendo do comportamento muitas vezes é mais fácil ao invés e manter em exponencial (não linear), linearizarmos a função por meio dos logaritmos, semi-logaritmicos entre outros. Isso faz com que temos menos trabalho para tratarmos e estimarmos os parâmetros da função exponencial. Figura 6.7: Em (a) curva de função exponencial e (b) após aplicarmos o logaritmo (Gujarati and Porter 2011). Atualmente é bem comum utilizarmos o modelo log-log, pois seu coeficiente angular \\(\\beta_i\\) mede a elasticidade de \\(Y\\) em relação a \\(X\\), ou seja, a variação percentual de \\(Y\\) correspondente a uma variação percentual em \\(X\\). Por exemplo: na Figura 6.7 se \\(Y\\) representa a quantidade demandada de camisetas e \\(X\\) seu preço unitário. Em (a) temos a relação da quantidade de demanda por camisetas e o preço, mas com a transformação logaritmica teremos a estimação de \\(-\\beta_2\\) (pois é uma reta descendente) que indica a elasticidade preço (variação em \\(ln(Y)\\) por unidade de variação em \\(ln(X)\\)). Portanto teríamos a variação percentual da quantidade demandada de camisetas dada uma variação percentual do preço. Atente-se: porcentagem (Gujarati and Porter 2011). Resumo geral: Em palavras, r2 mede a proporção ou percentual da variação total de Y explicada pelo modelo de regressão. 6.4.2 Regressão Linear Múltipla Na prática deparamos com muitas outros fatores que podem influenciar em sua variável dependente \\(Y\\). Portanto são acrescentadas dentro de seu modelo de regressão mais variáveis, o que é conhecido como Regressão Linear Múltipla, nada mais do que uma ampliação da regressão linear simples. Num modelo, por exemplo, com três variáveis (caso mais simples) pode ser expressa para a amostra como: \\[\\begin{equation} Y_i=\\hat{\\beta_0}+\\hat{\\beta_{1}}X_{1i}+\\hat{\\beta_{2}}X_{2i}+\\mu_i \\tag{6.13} \\end{equation}\\] Da mesma forma, \\(Y_i\\) a variável dependente, \\(X_{2}\\) e \\(X_{3}\\) as independentes explanatórias (explicativa), \\(\\mu_i\\) o erro estocático e \\(i\\) para indicar \\(i\\)-ésima observação. Ao caso dos parâmetros, \\(\\beta_0\\) como intercepto, \\(\\beta_1\\) e \\(\\beta_2\\) os coeficientes parciais de regressão/angulares. \\(\\beta_2\\) mede a variação no valor médio de \\(Y\\) (esperança de \\(Y\\)), por unidade de variação em \\(X_2\\), mantendo \\(X_3\\) constante, ou seja, traz o efeito “direto” de uma unidade de variação em \\(X_2\\) sobre o valor médio de \\(Y\\), excluindo o efeito de \\(X_3\\) na média de \\(Y\\). De mesmo modo, \\(X_3\\) com \\(X_2\\) constante. A regressão múltipla pressupõe as mesma hipóteses de que a regressão linear simples, porém como acréscimo - e muito importante- que as variáveis independentes devem estar ausentes de multicolinearidade, ou seja, não devem haver relação linear entre si. Se essa relação linear existir entre \\(X_2\\) e \\(X_3\\) são colineares ou linearmente dependentes, do contrário linearmente independentes. Caso a multicolinearidade for perfeita, os coeficientes de regressão das variáveis \\(X\\) serão indeterminados e seus erros padrão, infinitos. Se a multicolinearidade for menos que perfeita, serão determinado mas com grandes erros padrão (em relação aos próprios coeficientes), o que trará um modelo ruim para sua estimação. Para medirmos a multicolinearidade é comum a análise de correlação de pearson entre todas as variáveis, como mencionada em Medidas de Dependência 6.1.3, ou analisar a ocorrência de intervalo de confiança mais amplo, verificação de razões “t” insignificantes mesmo que seu \\(R^2\\) esteja alto, parâmetros estimados muitos sensíveis a qualquer alteração de dados e comumente utilizado para verificar o fator de inflação de variância (FIV) (Montgomery, Peck, and Vining 2012), que pode ser expressa como: \\[\\begin{equation} VIF_j=\\frac{1}{1-r^2_j} \\ \\ j=1,2,...,p \\tag{6.14} \\end{equation}\\] sendo \\(r^2\\) o coeficiente de correlação ao quadrado e \\(j\\) para referir as variáveis. Por exemplo, se \\(r^2_{23}\\), refe-se ao coeficiente de correlação entre as variáveis \\(X_2\\) e \\(X_3\\). Segundo ,quando este indicador apresenta o valor acima de cinco, é possível a existência de multicolinearidade (Maroco 2014). De mesmo modo que em regressão linear simples, são estimados os MQO, Máxima verossimilhança e o coeficiente de determinação múltiplo \\(R^2\\) (mesma interpratação para regressão linear simples \\(r^2\\)) para que se obtenha a melhor aproximação possível. 6.5 Gradiente Descendente 6.6 SVM 6.7 Arvores de Decisão 6.8 Elastic Net 6.9 KNN 6.10 K-means 6.11 PCA 6.12 Clusters 6.13 AOC E ROC References "],
["modelos-nível-ii.html", "Capítulo 7 Modelos nível II 7.1 Gradiente Boosting -&gt; estudar boosting e bagging dentro de Emsemnble 7.2 Random Forest 7.3 Redes Neurais", " Capítulo 7 Modelos nível II 7.1 Gradiente Boosting -&gt; estudar boosting e bagging dentro de Emsemnble 7.2 Random Forest 7.3 Redes Neurais "],
["validação-de-um-modelo.html", "Capítulo 8 Validação de um modelo 8.1 Overfitting, Underfitting 8.2 Validação Cruzada 8.3 Como escolher um bom modelo?", " Capítulo 8 Validação de um modelo 8.1 Overfitting, Underfitting Sendo muito importantes nesta área, o Underfitting (sub-ajustado) e Overfitting (sobre-ajustado) são dois termos que temos que estar sempre atentos. Um bom modelo não deve sofrer de nenhum deles (Silver 2013). Overfitting: Um cenário de overfitting ocorre quando, nos dados de treino, o seu modelo ML tem um desempenho excelente, porém quando utilizamos os dados em novos bancos de dados, seu resultado é ruim. Nesta situação, seu modelo aprendeu tão bem as relações existentes dos conjuntos de dados para treino que acabou apenas decorando esses dados. Portanto ao receber as informações das variáveis preditoras aos novos dados, o modelo tenta aplicar as mesmas regras decoradas, porém com estes novos dados (diferentes do treino) esta regra não tem validade e seu desempenho é afetado. As principais causas e soluções de um Overfitting são: Algoritmo muito complexo para os dados: caso for possível, pode-se simplificar o modelo utilizado por um algoritmo mais simples, com menos parâmetros. Permitindo reduzir as chances do modelo sofrer overfitting. Poucos dados para treinar: dependendo da quantidade de dados utilizados para treinar, pode ser que seja uma amostra pequena, com isso recomenda-se aumentar seu tamanho coletando mais dados. Ruídos nos dados de treinamento: é comum dentro do banco de dados existir algum tipo de ruído, isto é, outlier (valores extremos ou até mesmo valores incorretos nos dados). Esses ruídos podem fazer com que o modelo aprenda sobre ele, levando ao overfitting. Seria recomendado pré-processamento adequado para tratar essa interferência. 8.1.1 Underfitting: No cenário underfitting, o desempenho já é ruim no próprio treinamento de seu algoritmo. As principais causas e soluções de um Underfitting são: Algoritmo inadequado: bem provável que o modelo estatístico proposto pelo pesquisa pode não ter sido adequado ao comportamento dos dados. Por exemplo aplicar um algoritmo para funções de primeiro grau (linear) em um conjunto de dados com comportamento exponencial (função de segundo grau). Recomendável o pesquisador substituir o algoritmo escolhendo outro com outros parâmetros para solucionar o underfitting. Características não representativas: há possibilidade de que as características que estamos utilizando para treinar o modelo não sejam representativas, ou seja, não possuem relação entre si ou não sejam importantes para o modelo aplicado. Modelo com muitos parâmetros de restrição: o modelo torna-se inflexível, restrito, e não consegue se ajustar de forma adequada aos dados. Segue abaixo a Figura 8.1 demonstrando os dois casos anteriores e um modelo adequado. Figura 8.1: “Gráfico representando um Underfitting, um Modelo bem ajustado e um Overfitting respectivamente.” 8.2 Validação Cruzada A fim de que não haja previsões desastrosas geradas pelo modelo, para medirmos o desempenho real do modelo criado, é necessário que realizemos testes com ele, utilizando dados diferentes dos que foram apresentados no início. Portanto uma das técnicas mais utilizadas é a Cross-validation (Validação Cruzada). Após a realização do pré-processamento (analisar), iremos separar a totalidade dos dados históricos existentes em dois grupos, sendo o primeiro responsável pelo aprendizado do modelo, e o segundo por realizar os testes. Seguindo o mesmo exemplo de bons ou mau pagadores, usualmente separamos o conjunto de dados dos clientes em duas amostras. Uma com 8.3 Como escolher um bom modelo? References "],
["references.html", "References", " References "]
]
