<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 5 Um pouco de revisão | Machine Learning</title>
  <meta name="description" content="Tutorial de Machine Learning." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 5 Um pouco de revisão | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Tutorial de Machine Learning." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 5 Um pouco de revisão | Machine Learning" />
  
  <meta name="twitter:description" content="Tutorial de Machine Learning." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2021-01-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machinelearning.html"/>
<link rel="next" href="preprocesso.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#dicas-de-estudo"><i class="fa fa-check"></i><b>1.1</b> Dicas de estudo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>2</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="2.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>2.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="2.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>2.2</b> A arte de uma IA</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="vertentes-de-uma-ia-e-fundamentação-filosófica.html"><a href="vertentes-de-uma-ia-e-fundamentação-filosófica.html"><i class="fa fa-check"></i><b>3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
<li class="chapter" data-level="4" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>4</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="4.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>4.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dicio.html"><a href="dicio.html"><i class="fa fa-check"></i><b>5</b> Um pouco de revisão</a><ul>
<li class="chapter" data-level="5.1" data-path="dicio.html"><a href="dicio.html#um-pouco-de-álgebra-linear"><i class="fa fa-check"></i><b>5.1</b> Um pouco de Álgebra Linear</a></li>
<li class="chapter" data-level="5.2" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística.-parte-i"><i class="fa fa-check"></i><b>5.2</b> Um pouco de Estatística. Parte I</a></li>
<li class="chapter" data-level="5.3" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística.-parte-ii"><i class="fa fa-check"></i><b>5.3</b> Um pouco de Estatística. Parte II</a></li>
<li class="chapter" data-level="5.4" data-path="dicio.html"><a href="dicio.html#multlagrange"><i class="fa fa-check"></i><b>5.4</b> Multiplicadores de Lagrange</a></li>
<li class="chapter" data-level="5.5" data-path="dicio.html"><a href="dicio.html#kkt"><i class="fa fa-check"></i><b>5.5</b> Karush-Kuhn-Tucker (KKT)</a></li>
<li class="chapter" data-level="5.6" data-path="dicio.html"><a href="dicio.html#bias"><i class="fa fa-check"></i><b>5.6</b> Bias</a></li>
<li class="chapter" data-level="5.7" data-path="dicio.html"><a href="dicio.html#medidasimport"><i class="fa fa-check"></i><b>5.7</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="5.7.1" data-path="dicio.html"><a href="dicio.html#medidasdep"><i class="fa fa-check"></i><b>5.7.1</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="5.7.2" data-path="dicio.html"><a href="dicio.html#medinfo"><i class="fa fa-check"></i><b>5.7.2</b> Medidas de Informação</a></li>
<li class="chapter" data-level="5.7.3" data-path="dicio.html"><a href="dicio.html#meddist"><i class="fa fa-check"></i><b>5.7.3</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="5.7.4" data-path="dicio.html"><a href="dicio.html#medidas-de-precisão"><i class="fa fa-check"></i><b>5.7.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="5.7.5" data-path="dicio.html"><a href="dicio.html#medidas-de-consistência"><i class="fa fa-check"></i><b>5.7.5</b> Medidas de consistência</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>6</b> Pré-processamento</a><ul>
<li class="chapter" data-level="6.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>6.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="6.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>6.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="6.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>6.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>6.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="6.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>6.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="6.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>6.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>6.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>7</b> Algoritmos de Aprendizagem - Parte I</a><ul>
<li class="chapter" data-level="7.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>7.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="7.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>7.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>7.2</b> Regressão</a><ul>
<li class="chapter" data-level="7.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>7.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="7.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>7.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="7.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>7.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="7.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>7.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>7.3</b> Gradiente Descendente (GD)</a><ul>
<li class="chapter" data-level="7.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>7.3.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regularizacao"><i class="fa fa-check"></i><b>7.4</b> Regularização</a><ul>
<li class="chapter" data-level="7.4.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#penalizacoes"><i class="fa fa-check"></i><b>7.4.1</b> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></a></li>
<li class="chapter" data-level="7.4.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#elasticnet"><i class="fa fa-check"></i><b>7.4.2</b> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>8</b> Algoritmos de Aprendizagem - Parte II</a><ul>
<li class="chapter" data-level="8.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>8.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="8.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>8.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="8.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>8.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>8.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>8.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>8.3</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>8.3.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="8.3.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>8.3.2</b> Estatísticas</a></li>
<li class="chapter" data-level="8.3.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>8.3.3</b> A ACP</a></li>
<li class="chapter" data-level="8.3.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>8.3.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>8.4</b> Análise de Agrupamentos - <em>Clusters</em></a><ul>
<li class="chapter" data-level="8.4.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>8.4.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="8.4.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>8.4.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="8.4.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>8.4.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ptII.html"><a href="ptII.html#knn-k-vizinhos-mais-próximos-k-nearest-neighbors"><i class="fa fa-check"></i><b>8.5</b> KNN: K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</a><ul>
<li class="chapter" data-level="8.5.1" data-path="ptII.html"><a href="ptII.html#exknn"><i class="fa fa-check"></i><b>8.5.1</b> Exemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ptIII.html"><a href="ptIII.html"><i class="fa fa-check"></i><b>9</b> Algoritmos de Aprendizagem - Parte III</a><ul>
<li class="chapter" data-level="9.1" data-path="ptIII.html"><a href="ptIII.html#rf"><i class="fa fa-check"></i><b>9.1</b> Floresta Aleatória - <em>Random Forest</em></a></li>
<li class="chapter" data-level="9.2" data-path="ptIII.html"><a href="ptIII.html#grad-boosting---estudar-boosting-e-bagging-dentro-de-emseamble"><i class="fa fa-check"></i><b>9.2</b> grad boosting -&gt; estudar boosting e bagging dentro de emseamble</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="redesneurais.html"><a href="redesneurais.html"><i class="fa fa-check"></i><b>10</b> Redes Neurais</a></li>
<li class="chapter" data-level="11" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>11</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="11.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>11.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="11.1.1" data-path="valid.html"><a href="valid.html#underfitting-no-cenário-underfitting-o-desempenho-já-é-ruim-no-próprio-treinamento-de-seu-algoritmo."><i class="fa fa-check"></i><b>11.1.1</b> <strong>Underfitting</strong>: No cenário underfitting, o desempenho já é ruim no próprio treinamento de seu algoritmo.</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="valid.html"><a href="valid.html#validação-cruzada"><i class="fa fa-check"></i><b>11.2</b> Validação Cruzada</a></li>
<li class="chapter" data-level="11.3" data-path="valid.html"><a href="valid.html#como-escolher-um-bom-modelo"><i class="fa fa-check"></i><b>11.3</b> Como escolher um bom modelo?</a></li>
<li class="chapter" data-level="11.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>11.4</b> AOC e ROC</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dicio" class="section level1">
<h1><span class="header-section-number">Capítulo 5</span> Um pouco de revisão</h1>
<p>Com os <em>softwares</em> atuais é possível de que o pesquisador consiga fazer uma análise dos dados sem compreender totalmente a matemática por trás. Busco sempre que puder anexar um exemplo de acordo com cada tema apresentado para facilitar a compreensão, porém suponho de que o leitor esteja familiarizado com conceitos fundamentais de estatística, probabilidade e álgebra liner, portanto conceitos fundamentais como tipos de amostragem, probabilidades e suas distribuições, teste de hipóteses e significância, potência dos testes estatísticos, intervalos de confiança, escalares e vetores, espaço vetorial e transformação linear, produto interno, assimetria e curtose, entre outos..</p>
<p>Nesta seção são apresentadas brevemente um pouco desses conteúdos. É provável de que o leitor já saiba. Porém acredito de que sejam fundamentais para o Aprendizado de Máquina e seria bom para revisá-lo. Sinta-se livre em pular este capítulo. Ao caso de ser algo totalmente novo, reforço-o de introduzir com outras literaturas a respeito, pois são imprescindíveis aos conteúdos dos próximos capítulos.</p>
<p><strong>Bons estudos</strong>.</p>
<div id="um-pouco-de-álgebra-linear" class="section level2">
<h2><span class="header-section-number">5.1</span> Um pouco de Álgebra Linear</h2>
<ul>
<li><p><strong>Escalares e Vetores:</strong></p></li>
<li><p><strong>Espaço Vetorial e Transformação Linear:</strong></p></li>
<li><p><strong>Produto Interno:</strong></p></li>
</ul>
</div>
<div id="um-pouco-de-estatística.-parte-i" class="section level2">
<h2><span class="header-section-number">5.2</span> Um pouco de Estatística. Parte I</h2>
<ul>
<li><p><strong>Assimetria e Curtose:</strong></p></li>
<li><p><strong>Variância e Desvio padrão (Erro padrão)</strong></p></li>
<li><p><strong>Covariância:</strong> a covariância mede a relação linear entre duas variáveis. É possível utilizar a covariância para compreender a direção da relação entre as variáveis. Valores de covariância positivos indicam que valores acima da média de uma variável estão associados a valores médios acima da outra variável e abaixo dos valores médios são igualmente associado. Valores de covariância negativos indicam que valores acima da média de uma variável estão associados com valores médios abaixo da outra variável.</p></li>
<li><p><strong>Distribuição normal:</strong></p></li>
<li><p><strong>Distribuição binomial:</strong></p></li>
<li><p><strong>Distribuição de Poisson:</strong> <span class="citation">(Banzatto and Kronka <a href="#ref-banzatto1992experimentaccao">1992</a>)</span> Quando número de plantas daninhas por parcela, número de insetos capturados em armadilhas luminosas, número de pulgões ou ácaros por folhas, etc.</p></li>
<li><p><strong>Teorema de Bayes:</strong> quando tratamos de probabilidades, <span class="math inline">\(P(A|B)\)</span> e <span class="math inline">\(P(B|A)\)</span> podem ser parecidos, mas possuem grande diferença entre as probabilidades que representam. Por exemplo <span class="math inline">\(P(A|B)\)</span> pode se referir sobre a probabilidade de uma pessoa que cometeu um furto (B) ser condenada (A) e <span class="math inline">\(P(B|A)\)</span> seria a probabilidade de uma pessoa que foi condenada por furto ter efetivamente cometido um crime. A causa se torna o efeito e o efeito se torna a causa <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.</p></li>
</ul>
<p>Pela regra geral de multiplicação que afirma que a probabilidade da ocorrência de dois eventos é o produto da probibilidade da ocorrência de um deles pela probabilidade condicional da ocorrência do outro evento, temos:</p>
<p><span class="math display" id="eq:multprob">\[\begin{equation} 
 P(A \bigcap B)= P(A). P(B|A) \  \mbox{ou} \ P(A \bigcap B)= P(B). P(A|B)
  \tag{5.1}
\end{equation}\]</span></p>
<p>Igualando ambas expressões, temos: $ P(A). P(B|A) = P(B). P(A|B)$ e portanto, divindo por <span class="math inline">\(P(B)\)</span>, obtém-se o Teorema de Bayes que descreve a probabilidade de um evento, baseado em um conhecimento a <em>priori</em> que pode estar relacionado ao evento:</p>
<p><span class="math display" id="eq:bayes">\[\begin{equation} 
 P(A|B) = \frac{P(A).P(B|A)}{P(B)}
  \tag{5.2}
\end{equation}\]</span></p>
<p>Para <span class="math inline">\(B_n\)</span> e <span class="math inline">\(A_k\)</span> atributos, podemos reescrever:</p>
<p><span class="math display" id="eq:bayesn">\[\begin{equation} 
 P(A_k|B_1,...,B_n) = \frac{P(A_k).P(B_1,...,B_n|A_k)}{P(B_1,...,B_n)}
  \tag{5.3}
\end{equation}\]</span></p>
<p><strong>Exemplo:</strong> este exemplo pode ser encontrado em <span class="citation">Freund (<a href="#ref-freund2009estatistica">2009</a>)</span>. Numa certa empresa, 4% dos homens e 1% das mulheres têm mais de 1,75m
de altura, respectivamente, sendo que 60% dos trabalhadores são mulheres. Um trabalhador é escolhido ao acaso.</p>
<ol style="list-style-type: decimal">
<li>Qual a probabilidade de que tenha mais de 1,75m?</li>
</ol>
<p>Temos de informação de que 60% dos trabalhadores são mulheres e que 1% delas possuem mais de 1,75m. Portanto 40% dos trabalhadores são homens, sendo 4% deles com mais de 1,75m. Logo temos que:
<span class="math display">\[P(&gt; 1, 75m) = (0, 04 . 0.4) + (0, 01 . 0.6) = 0, 022 \\ → 2, 2\% \ \mbox{ de probabilidade de que tenha mais de 1,75m.}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>E que seja homem dado que o trabalhador escolhido tenha mais de 1,75m?</li>
</ol>
<p>Pelo enunciado “que seja homem dado que o trabalhador escolhido tenha mais de 1,75m”, podemos perceber que já possuímos uma afirmação que já foi escolhido uma pessoa que tenha mais que 1,75m e queremos saber se é homem. Por meio da questão anterior sabemos a probabilidade P(&gt; 1,75m). Portanto:
<span class="math display">\[P(H| &gt; 1, 75m) = \frac{P(&gt; 1, 75m|H).P(H)}{P(&gt; 1, 75m)}=\frac{0,04.0,4}{0, 022} \]</span></p>
<p><span class="math display">\[→ 72,73\% \ \mbox{de probabilidade de ser homem dado que seja maior que 1,75m.}\]</span></p>
<ul>
<li><strong>Função de verossimilhança:</strong> a verossimilhança <span class="math inline">\(L\)</span> de um conjunto de parâmetros <span class="math inline">\(\theta\)</span>, com dada informação <span class="math inline">\(x\)</span>. É igual a probabilidade da mesma observação <span class="math inline">\(x\)</span> ter ocorrido dados os valores dos mesmos parâmetros <span class="math inline">\(\theta\)</span>. Conhecendo um parâmetro <span class="math inline">\(\theta\)</span>, a probabilidade condicional de <span class="math inline">\(x\)</span> é <span class="math inline">\(P(x|\theta)\)</span>, mas se o valor de <span class="math inline">\(x\)</span> é conhecido, pode-se realizar inferências sobre o valor de <span class="math inline">\(\theta\)</span> <span class="citation">(Bolfarine and Sandoval <a href="#ref-bolfarine2001introduccao">2001</a>)</span>.</li>
</ul>
<p><span class="math display" id="eq:fverossimilhanca">\[\begin{equation} 
 L(\theta |x)=P(x| \theta)
  \tag{5.4}
\end{equation}\]</span></p>
<p>Para “<span class="math inline">\(n\)</span>” valores:</p>
<p><span class="math display" id="eq:fsumverossimilhanca">\[\begin{equation} 
 L(\theta |x_1,..., x_n)=\prod_{i=1}^{n} P(x_i| \theta)
  \tag{5.5}
\end{equation}\]</span></p>
<p>Geralmente utiliza-se o logaritmo natural em verossimilhança <span class="math inline">\(L(\theta |x)=ln L(\theta|x)\)</span> como função suporte e facilitar em seu estudo.</p>
<p>Para facilitar a compreensão, considere a observação de que você esteja ouvindo barulho em sua sala de estar num dia de natal (observação <span class="math inline">\(x\)</span>), você parte da hipótese inicial que poderia ser o “Papai Noel” lhe entregando presentes (hipótese <span class="math inline">\(\theta\)</span>). A probabilidade de ser Noel lhe entregando presente apenas porque ouviu o barulho, isto é, <span class="math inline">\(P(\theta|x)\)</span> é baixa. No entanto o contrário, você com a afirmação de que é o Noel lhe entregando presentes, a probabilidade de haver barulho em sua sala de estar é bem alta, logo a verossimilhança <span class="math inline">\(L(\theta|x)=P(x|\theta)\)</span>.</p>
<ul>
<li><p><strong>Parâmetros:</strong> podem ser vistos como características númericas de um modelo ou população. Os valores não podem ser mensurados diretamente mas que podem ser estimados através dos dados de uma amostra.</p></li>
<li><p><strong>Paramétrico x Não Paramétrico:</strong></p></li>
</ul>

</div>
<div id="um-pouco-de-estatística.-parte-ii" class="section level2">
<h2><span class="header-section-number">5.3</span> Um pouco de Estatística. Parte II</h2>
<ul>
<li><strong>Teorema do Limite Central:</strong> quando é utilizado a média amostral para estimar a média de uma população, ocorre-se incertezas em relação ao erro. O Teorema do Limite Central é um teorema fundamental para a estatísticas e faz com que possa ser aplicado independente da forma da distribuição da população. Ele diz que se <span class="math inline">\(\overline{x}\)</span> é a média de uma amostra aleatória de tamanho <span class="math inline">\(n\)</span> de uma população infinita com a média <span class="math inline">\(\mu\)</span> e o desvio-padrão <span class="math inline">\(\sigma\)</span> e se <span class="math inline">\(n\)</span> é grande o suficiente (em geral <span class="math inline">\(n=30\)</span>), então possui próximo a distribuição normal padrão:</li>
</ul>
<p><span class="math display" id="eq:teoremacentralimite">\[\begin{equation} 
 z=\frac{\overline{x}-\mu}{\sigma / \sqrt{n}}
  \tag{5.6}
\end{equation}\]</span></p>
<p>Este teorema também pode ser utilizado para populações finitas, mas não é comum e são poucas situações de que haja esta possibilidade. A utilização mais comum é quando <span class="math inline">\(n\)</span> é grande enquanto <span class="math inline">\(\frac{n}{N}\)</span> pequeno.</p>
<p><strong>Exemplo <span class="citation">(Farias <a href="#ref-fariaestatistic">2010</a>)</span>:</strong> O fabricante de uma lâmpada especial afirma que o seu produto tem vida média de 1.600 hors, com desvio padrão de 250 horas. O dono de uma empresa compra 100 lâmpadas desse fabricante. Qual é a probabilidade de que a vida média dessas lâmpadas ultrapasse 1.650?</p>
<p>Podemos aceitar que as 250 lâmpadas compradas sejam uma amostra aleatória simples da população referente às lâmpadas produzidas por esse fabricante. Como <span class="math inline">\(n=100\)</span> é um tamanho suficientemente grande de amostra, é possível utilizarmos o Teorema Central do Limite e entender que <span class="math inline">\(X=\)</span>vida útil de uma lâmpada se aproxima da distribuição normal <span class="math inline">\(\overline{X}\approx N(\mu;\frac{\sigma^2}{n})\)</span>. Logo:
<span class="math display">\[\overline{X}\approx N(1600;\frac{250^2}{100})\]</span>
<span class="math display">\[Pr(\overline{X}&gt;1650)=Pr\bigg( \frac{\overline{X}-1600}{\sqrt{\frac{250^2}{100}}}&gt;\frac{1650-1600}{\sqrt{\frac{250^2}{100}}} \bigg)\]</span>
<span class="math display">\[=Pr(Z&gt;2,0)\]</span>
<span class="math display">\[=0,5-tab(2,0)\]</span>
<span class="math display">\[=0,5-0,47725=0,02275\]</span>
A probabilidade de que a vida média dessas lâmpadas ultrapasse 1.650 é de 2,275%.</p>
<ul>
<li><strong>Testes de hipóteses:</strong></li>
</ul>
<blockquote>
<p>Uma hipótese estatística é uma afirmação ou conjectura sobre um parâmetro, ou parâmetros, de uma população (ou populações); pode também se referir ao tipo, ou natureza, da população (ou populações).</p>
<p>— <span class="citation">Freund (<a href="#ref-freund2009estatistica">2009</a>)</span>.</p>
</blockquote>
<p>O Teste de Hipóteses é um procedimento estatístico que os permitem rejeitar ou não rejeitar uma hipótese estatística por meio dos dados observados de uma amostra. Para desenvolver os processos de testes de hipóteses estatísticas precisamos saber precisamente o que esperar quando uma hipótese é verdadeira, por isso em geral formula-se a hipótese contrária àquela que queremos provar. Supondo que estamos desconfiados em um jogo que seus dados não são honestos, ao formularmos a hipótese de que esses dados são viciados, dependeria do quão viciados eles são. Porém, ao supor que eles são perfeitamente equilibrados, poderíamos calcular todas as probabilidades necessárias para concluirmos a hipótese. Se pretendermos verificar que a análise de um analista de dados é mais eficiente do que o outro, iremos formular a hipótese de que ambos são igualmente eficientes. Se a durabilidade de uma camisa feita por algodão é maior que uma camisa feita por políester, formularemos a hipótese de que ambas possuem as durabilidades iguais. A hipótese de não haver diferença (hipóteses iguais) denominamos como <strong>hipóteses nulas (<span class="math inline">\(H_0\)</span>)</strong>, utilizada para qualquer hipótese estabelecida prioritariamente para ver se ela pode ser rejeitada. A hipótese ue aceitamos quando rejeitamos a nula, é chamada de <strong>hipótese alternativa (<span class="math inline">\(H_A\)</span>)</strong> <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>. Vamos seguir alguns exemplos de <span class="citation">Freund (<a href="#ref-freund2009estatistica">2009</a>)</span>.</p>
<p><strong>Exemplo:</strong> um psicólogo pretende determinar se o tempo médio de reação de um adulto a um estímulo visual é realmente de 0,38 segundos. Sua hipótese nula
<span class="math display">\[H_0: \mu=0,38\ \mbox{segundos}\]</span>
contra a hipótese alternativa
<span class="math display">\[H_A: \mu \neq 0,38\ \mbox{segundos}\]</span>
em que <span class="math inline">\(\mu\)</span> é o tempo médio de reação de um adulto ao estímulo visual. Para realizar o teste, o psicólogo decide tomar uma amostra aleatória de <span class="math inline">\(n=40\)</span> adultos com objetivo de aceitar a hipótese nula se a média da amostra cair em algum ponto entre 0,36 e 0,40 segundos; do contrário a hipótese será rejeitada. Como a decisão se baseia em uma amostra, existe a possibilidade de a média amostral ser menor do que 0,36 segundos ou maior que 0,40 segundos mesmo se a verdadeira média amostral ser 0,38 segundos. Da mesma forma é possível que a média amostral esteja entre os intervalos de 0,36 e 0,40 segundos mesmo que a verdadeira média possua, por exemplo, 0,41 segundos. Portanto, é importante investigar a probabilidade de que o teste nos leve a uma decisão errada.</p>
<p>Vamos supor que o desvio padrão seja <span class="math inline">\(\sigma=0,08\)</span> segundos para estes dados e investiguemos a possibilidade de rejeitar falsamente a hipótese nula. Iremos supor que o verdadeiro tempo médio de reação seja 0,38 segundos, então encontramos a probabilidade de que a média amostral vá ser menor ou igual a 36 segundos ou maior igual a 40. A probabilidade de que isso ocorra é dada pela somas das áreas das duas regiões coloridas apresentadas na Figura <a href="dicio.html#fig:testehip1">5.1</a> a seguir, e pode ser determinada pela distribuição amostral da média por uma distribuição normal.</p>
<p>Supondo que a população amostrada possa ser tratada como sendo infinita, a média da distribuição amostral é dado por:</p>
<p><span class="math display">\[\sigma_{\overline{x}}=\frac{\sigma}{\sqrt{n}}=\frac{0,08}{\sqrt{40}}\approx 0,0126\]</span>
A linha de divisória em unidades padronizadas, são:
<span class="math display">\[z=\frac{0,36-0,38}{0,0126}\approx -1,59 \ \mbox{e} \ z=\frac{0,40-0,38}{0,0126}\approx 1,59\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:testehip1"></span>
<img src="Figuras/testehip1.png" alt="Critério de teste e distribuição amostral de \(\overline{x}\) com \(\mu =0,38\) segundos (Freund 2009)." width="70%" />
<p class="caption">
Figura 5.1: Critério de teste e distribuição amostral de <span class="math inline">\(\overline{x}\)</span> com <span class="math inline">\(\mu =0,38\)</span> segundos <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.
</p>
</div>

<p>Por meio da Tabela Z (tabela I do livro estatistica que vou por) observamos que a área da cauda da distribuição amostral da será <span class="math inline">\(0,50000-0,4441=0,0559\)</span>. Portanto a probabilidade de obter um valor em uma ou em outra cauda da distribuição será de <span class="math inline">\(2(0,0559)=0,1118\)</span>.</p>
<p>Vamos agora com a possibilidade de que o teste deixa de detectar que a hipótese nula é falsa, ou seja <span class="math inline">\(\mu \neq 0,38\)</span> segundos. Portanto iremos supor que o verdadeiro tempo médio de reação seja de 0,41 segundos. Obtendo uma média amostral no intervalo de 36 a 40 segundos levaria à aceitação errônea da hipótese nula de <span class="math inline">\(\mu=0,38\)</span> segundos. Portanto a média da distribuição amostral será:</p>
<p><span class="math display">\[\sigma_{\overline{x}}=\frac{\sigma}{\sqrt{n}}=\frac{0,08}{\sqrt{40}}\approx 0,0126\]</span>
As linhas divisórias em unidades padronizadas, são:
<span class="math display">\[z=\frac{0,36-0,41}{0,0126}\approx -3,77 \ \mbox{e} \ z=\frac{0,40-0,41}{0,0126}\approx -0,79\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:testehip2"></span>
<img src="Figuras/testehip2.png" alt="Critério de teste e distribuição amostral de \(\overline{x}\) com \(\mu =0,41\) segundos (Freund 2009)." width="70%" />
<p class="caption">
Figura 5.2: Critério de teste e distribuição amostral de <span class="math inline">\(\overline{x}\)</span> com <span class="math inline">\(\mu =0,41\)</span> segundos <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.
</p>
</div>

<p>Por fim cabe ao psicólogo decidir qual risco é aceitável: a probabildiade de 0,11 de rejeitar erroneamente a hipótese nula de <span class="math inline">\(\mu = 0,38\)</span> ou a probabilidade 0,21 de erroneamente aceitá-la quando na realidade é <span class="math inline">\(0,41\)</span>.</p>
<p>Portanto resume-se em:</p>
<table>
<caption><span id="tab:tabelahipotese">Tabela 5.1: </span> Resumo de uma situação típica dos testes de hipóteses.</caption>
<thead>
<tr class="header">
<th></th>
<th><strong>Aceitar <span class="math inline">\(H_0\)</span></strong></th>
<th><strong>Rejeitar <span class="math inline">\(H_0\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong><span class="math inline">\(H_0\)</span> é verdadeiro</strong></td>
<td>Decisão Correta</td>
<td>Erro tipo I</td>
</tr>
<tr class="even">
<td><strong><span class="math inline">\(H_0\)</span> é falso</strong></td>
<td>Erro tipo II</td>
<td>Decisão Correta</td>
</tr>
</tbody>
</table>
<p>Se a hipótese nula <span class="math inline">\(H_0\)</span> é verdadeira e aceita ou falsa e rejeitada, a decisão é correta em ambos casos; se é verdadeira e rejeitada ou falsa aceita. O erro tipo I e a probabilidade de obtê-lo é ocorrida pela letra grega <span class="math inline">\(\alpha\)</span> e o erro tipo II pelo <span class="math inline">\(\beta\)</span>. Portanto pelo exemplo, temos que <span class="math inline">\(\alpha=0,11\)</span> e <span class="math inline">\(\beta=0,21\)</span> quando <span class="math inline">\(\mu=0,41\)</span> e o psicólogo deve decidir se aceita ou rejeita a hipótese nula de <span class="math inline">\(\mu=0,38\)</span></p>
<ul>
<li><strong>Região Crítica e nível de significância:</strong> no geral definimos uma região crítica (RC) como o conjunto de valores no qual a probabilidade de ocorrência é pequena sob a hipótese de ser verdade o <span class="math inline">\(H_0\)</span>. Por exemplo: lançada 30 vezes uma moeda, sendo obtida num total de 28 caras. Claramente iremos desconfiar que é uma moeda honesta, visto que a probabilidade de ser obtida 28 caras em 30 lançamentos de uma moeda honesta é de 0,000000433996. Mesmo que haja essa mínima possibilidade de que a moeda honesta acerte este evento, pela perspectiva do teste de hipóteses, a obtenção de tal evento será uma evidência de que a nossa hipótese nula de honestidade da moeda não é muito plausível. Assim não dizemos que a moeda não é honesta, concluímos que não há evidência suficiente para apoiar a hipótese nula <span class="citation">(Farias <a href="#ref-fariaestatistic">2010</a>)</span>.</li>
</ul>
<p>A definição dessa pequena probabilidade se faz por meio da escolha do <strong>nível de significância <span class="math inline">\(\alpha\)</span></strong> do teste, expressa como:</p>
<p><span class="math display" id="eq:nivelsignificancia">\[\begin{equation} 
 \alpha=Pr(\mbox{Erro tipo I})=Pr(\mbox{rejeitar} \ H_0|H_0 \ \mbox{é verdadeira})
  \tag{5.7}
\end{equation}\]</span></p>
<p>Geralmente é utilizado em <span class="math inline">\(\alpha=0,05\)</span>, <span class="math inline">\(\alpha=0,01\)</span> ou <span class="math inline">\(\alpha=0,10\)</span> como nível de significância, com isso torna-se possível estabelecer a região crítica usando a distribuição amostral da estatística de teste <span class="citation">(Farias <a href="#ref-fariaestatistic">2010</a>)</span>.</p>
<p>De <span class="citation">Farias (<a href="#ref-fariaestatistic">2010</a>)</span>, segue:</p>
<p><strong>Exemplo 1:</strong> Considere uma população representada por uma variável aleatória normal com média <span class="math inline">\(\mu\)</span> e variância 400. Queremos testar:</p>
<p><span class="math display">\[H_0:\mu = 100\]</span>
<span class="math display">\[H_A:\mu \neq 100\]</span>
Com base em uma amostra aleatória simples de tamanho <span class="math inline">\(n=16\)</span>. Para tal define-se a região crítica como RC: <span class="math inline">\(\overline{X}&lt;85\)</span> ou <span class="math inline">\(\overline{X}&gt;115\)</span>. Qual é a probabilidade do erro tipo I?</p>
<p><span class="math display">\[\alpha=Pr(\mbox{Erro tipo I})=Pr(\mbox{rejeitar} \ H_0|H_0 \ \mbox{é verdadeira})\]</span>
<span class="math display">\[=Pr[\{\overline{X}&lt;85\} \cup \{\overline{X}&gt;115\} \ | \ \overline{X}\sim N(100;\frac{400}{16}=25)]\]</span>
<span class="math display">\[=Pr[\overline{X}&lt;85 \ | \  \overline{X}\sim N(100;25)]+Pr[\overline{X}&gt;115 \ | \ \overline{X}\sim N(100;25)]\]</span>
<span class="math display">\[=Pr\bigg( Z&lt;\frac{85-100}{5} \bigg)+Pr\bigg( z&gt;\frac{115-100}{5} \bigg)\]</span>
<span class="math display">\[=Pr(Z&gt;-3)+Pr(Z&gt;3) \rightarrow 2.Pr(Z&gt;3)\]</span>
<span class="math display">\[\alpha=0,0027\]</span></p>
<ul>
<li><strong>Aplicações do Teste de Hipótese - Média com a variância conhecida:</strong> ao caso de interessarmos na média de uma população normal e supondo que a variância seja conhecida. Pelo teste temos que:</li>
</ul>
<p><span class="math display" id="eq:testehipmedia">\[\begin{equation} 
z=\frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}
  \tag{5.8}
\end{equation}\]</span></p>
<p>onde <span class="math inline">\(\mu_o\)</span> é o valor da média que ocorre sobre a hipótese nula. Trabalhar com unidades padronizadas <span class="math inline">\(z\)</span> nos permitem formular vários critérios que se aplicam a muitos problemas diferentes. Lembrando que são amostras suficientemente grandes para que a distribuição amostral da média possa ser próxima por uma distribuição normal padrão.</p>
<p><strong>Exemplo <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>:</strong> Uma oceanógrafa com base numa amostra aleatória de tamanho <span class="math inline">\(n=35\)</span> e ao nível 0,05 de significância, quer testar se a profundidade média do oceano numa determinada área é de 72,4 metros conforme registrado. O que ela decidirá se obtiver <span class="math inline">\(\overline{x}=73,2\)</span> metros e se puder supor, usando informações de estudos anteriores análogos, que <span class="math inline">\(\sigma=2,1\)</span> metros?</p>
<p><span class="math display">\[H_0: \mu=72,4 \mbox{metros}\]</span>
<span class="math display">\[H_A: \mu \neq 72,4 \mbox{metros}\]</span>
Temos que <span class="math inline">\(\alpha=0,05\)</span>, ou seja, pela tabela de distribuição normal bilateral AQUI PONHO A TABELA ANEXADA teríamos 0,025 para cada lado e portanto, ciente de que cada lado da curva equivale a 0,5 (ou 50%) e subtraindo os 0,025, obtemos <span class="math inline">\(0,475\)</span>. Pela tabela verificamos então que iremos rejeitar a hipótese nula se <span class="math inline">\(Z \leq -1,96\)</span> ou <span class="math inline">\(z \geq 1,96\)</span>. Logo:
<span class="math display">\[z=\frac{73,2-72,4}{2,1/\sqrt{35}}\approx 2,25\]</span>
Como <span class="math inline">\(z-2,25\)</span> pertence a região critíca, então a hipótese nula deve ser rejeitada, a diferença entre <span class="math inline">\(\overline{x}=73,2\)</span> e <span class="math inline">\(\mu=72,4\)</span> é significante. Note que se a oceanógrafa tivesse utilizado o nível de 0,01 de significância nesse exemplo, ela não poderia ter rejeitado a hipótese nula. Pelo <strong>valor <span class="math inline">\(p\)</span> (probabilidade de cauda)</strong> que é muito utilizado atualmente como medida de significância e é dado pela área total sob a curvada esquerda de <span class="math inline">\(Z=-2,25\)</span> e da direita <span class="math inline">\(z=2,25\)</span>, observando a tabela temos que <span class="math inline">\(2(0,5000-0,4878)=0,0244\)</span>, poderíamos rejeitar a hipótese nula ao nível de 0,0244 de significância.</p>
<ul>
<li><strong>Aplicações do Teste de Hipótese - Média com a variância desconhecida:</strong> vamos supor que não sabemos o valor da variância e agora a um caso de <span class="math inline">\(n\)</span> não suficientemente grande, ou seja, utilizamos o teste <span class="math inline">\(t\)</span>:</li>
</ul>
<p><span class="math display" id="eq:testehipvardesc">\[\begin{equation} 
t=\frac{\overline{X}-\mu_0}{S/\sqrt{n}}
  \tag{5.9}
\end{equation}\]</span></p>
<p><strong>Exemplo <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>:</strong> A safra de alfafa de uma amostra aleatória de seis lotes de teste é dada por 1,4; 1,6; 0,9; 1,9; 2,2; e 1,2 tonelada por acre. Ao nível de 0,05 de significância, teste se isso corrobora a alegação de que a safra média para este tipo de alfafa é de 1,5 tonelada por acre.</p>
<p><span class="math display">\[H_0: \mu=1,5\]</span>
<span class="math display">\[H_A: \mu\neq1,5\]</span>
Temos que <span class="math inline">\(\alpha=0,05\)</span>, temos que <span class="math inline">\(n=6\)</span> observações e portanto 6-1=5 graus de liberdade. Portanto pela tabela XXXXXXXXX de distribuição <span class="math inline">\(t\)</span> de <em>student</em> , em área na cauda superior de 0,025 e 5 G.L, rejeitaremos a hipótese nula se <span class="math inline">\(t\leq -2,75\)</span> ou <span class="math inline">\(t\geq 2,75\)</span>. Calculando a médio e o desvio-padrão dos dados, substituindo na expressão anterior, temos:
<span class="math display">\[t=\frac{1,533-1,5}{0,472/\sqrt{6}}\approx 0,171\]</span>
Portanto não podemos rejeitar a hipótese nula, ou seja, os dados tendem a apoiar a alegação de que a safra média para esse tipo de alfafa é de 1,5 tonelada por acre.</p>
<p>Existe diversos outros como o teste para duas médias com amostras independentes ou dependentes, com proporções, etc. Ao leitor que pretende se aprofundar nesse conteúdo sugiro buscar literaturas complementares no campo de estatística.</p>
<ul>
<li><p><strong>Análise de Variância:</strong></p></li>
<li><p><strong>Supervisionada x Não supervisionada:</strong></p></li>
</ul>
</div>
<div id="multlagrange" class="section level2">
<h2><span class="header-section-number">5.4</span> Multiplicadores de Lagrange</h2>
</div>
<div id="kkt" class="section level2">
<h2><span class="header-section-number">5.5</span> Karush-Kuhn-Tucker (KKT)</h2>
</div>
<div id="bias" class="section level2">
<h2><span class="header-section-number">5.6</span> Bias</h2>
<p><a href="https://iaexpert.academy/2020/09/28/importancia-do-bias-nas-redes-neurais/" class="uri">https://iaexpert.academy/2020/09/28/importancia-do-bias-nas-redes-neurais/</a></p>

</div>
<div id="medidasimport" class="section level2">
<h2><span class="header-section-number">5.7</span> Medidas de Importância</h2>
<blockquote>
<p>Um atributo é dito importante se quando removido a medida de importância considerada em relação aos atributos restantes é deteriorada , seja a precisão da medida, consistência, informação, distância ou dependência</p>
<p>Tradução de <span class="citation">Liu and Motoda (<a href="#ref-liu2012feature">2012</a>)</span>.</p>
</blockquote>
<p>É fundamental estimarmos a importância de um atributo, tanto uma avaliação individual quanto à avaliação de subconjuntos de atributos. É uma questão complexa e multidimensional <span class="citation">(Liu and Motoda <a href="#ref-liu2012feature">2012</a>)</span>. Podemos avaliar se os atributos selecionados pela etapa do pré-processamento auxiliam a melhorar a precisão do classificador ou a simplifcar algum modelo construído. A seguir, apresenta-se algumas medidas utilizadas <span class="citation">(Lee <a href="#ref-lee2005seleccao">2005</a>)</span>.</p>
<div id="medidasdep" class="section level3">
<h3><span class="header-section-number">5.7.1</span> Medidas de Dependência</h3>
<p>Conhecidas como medidas de <strong>correlação</strong> ou <strong>associação</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:correlacao"></span>
<img src="Figuras/correlacao.png" alt="Padrões de correlação. Elaborado por Gujarati and Porter (2011) e adaptado Henri (1978)." width="70%" />
<p class="caption">
Figura 5.3: Padrões de correlação. Elaborado por <span class="citation">Gujarati and Porter (<a href="#ref-gujarati2011econometria">2011</a>)</span> e adaptado <span class="citation">Henri (<a href="#ref-theil1978">1978</a>)</span>.
</p>
</div>

</div>
<div id="medinfo" class="section level3">
<h3><span class="header-section-number">5.7.2</span> Medidas de Informação</h3>
<p>As medidas de informação determinam o ganho de informação a partir de um atributo. O ganho de informação é definido como a diferença entre a incerteza a <em>priori</em> e a incerteza a <em>posteriori</em> considerando-se o atributo <span class="math inline">\(X_i\)</span>. <span class="math inline">\(X_i\)</span> é preferido ao atributo <span class="math inline">\(X_j\)</span> se seu ganho de informação for maior que de <span class="math inline">\(X_j\)</span>. Uma das mais utilizadas é a entropia que normalmente é usada na teoria da informação para medir a pureza ou impureza de um determinado conjunto.</p>
<p><span class="citation">Shannon (<a href="#ref-shannon1948mathematical">1948</a>)</span>, tomou como “ponto de partida” encontrar uma forma matemática de medir o quanto de informação existe na transmissão de uma mensagem de um ponto a outro, denominando-a entropia. Sua proposta baseava-se na ideia de que o aumento da probabilidade do próximo símbolo diminuiria o tamanho da informação. Com isso, a entropia pode ser definida como a quantidade de incerteza que há em uma mensagem e que diminui à medida que os símbolos são transmitidos (vai se conhecendo a mensagem), tendo-se então a informação, que pode ser vista como redução da incerteza <span class="citation">(Shannon <a href="#ref-shannon1948mathematical">1948</a>; Paviotti and Magossi <a href="#ref-paviotti2019consideraccoes">2019</a>)</span>. Por exemplo: ao utilizarmos como idioma a nossa língua portuguesa e ao transmitir como símbolo a letra “q”, a probabilidade do próximo símbolo ser a letra “u” é maior que a de ser qualquer outro símbolo, enquanto que a probabilidade de ser novamente a letra “q” é praticamente nula <span class="citation">(Paviotti and Magossi <a href="#ref-paviotti2019consideraccoes">2019</a>)</span>.</p>
<p>Shannon define que a entropia pode ser calculada por meio da soma das probabilidades de ocorrência de cada símbolo pela expressão <span class="math inline">\(∑ p_i = 1 = 100\%\)</span>, em que <span class="math inline">\(p_i\)</span> representa a probabilidade do i-ésimo símbolo que compõe a mensagem. Segundo ele, estes símbolos devem ser representados através de sequências binárias, utilizando das propostas de <span class="citation">Nyquist (<a href="#ref-nyquist1924certain">1924</a>)</span> e <span class="citation">Hartley (<a href="#ref-hartley1928transmission">1928</a>)</span>. Sua proposta consistia em representar símbolos de um alfabeto através de um logaritmo de acordo com suas respectivas unidades de informação. A entropia proposta por ele é obtida pela média das medidas de Hartley <span class="citation">(Moser and Chen <a href="#ref-moser2012student">2012</a>)</span>.</p>
<p>Se A é discreto com distribuição de probabilidade <span class="math inline">\(p(A)\)</span>, a entropia será:</p>
<p><span class="math display" id="eq:entropia">\[\begin{equation} 
  H(A)=- \sum p(A)log_2(p(A)) 
  \tag{5.10}
\end{equation}\]</span></p>
<p>Para facilitar a compreensão, vamos supor um exemplo de um questionário com resposta binária entre “sim” e “não”: quanto mais distribuído as probabilidades das respostas, mais desorganizada é, logo maior suaa entropia, do contrário caso for uma probabilidade de ser zero “sim”/“não” ou de ser 1 (100%), ou seja, ter apenas uma opção de resposta, será menos distribuído e portanto menor usa entropia.</p>
<div class="figure" style="text-align: center"><span id="fig:entropia"></span>
<img src="Figuras/entropia.jpg" alt="Gráfico de Probabilidade x Entropia." width="70%" />
<p class="caption">
Figura 5.4: Gráfico de Probabilidade x Entropia.
</p>
</div>

<p>O ganho de informação portanto mede a redução da entropia (nesse caso) causada pela partição dos exemplos de acordo com os valores do atributo.</p>
<p><span class="math display" id="eq:ganhodeinf">\[\begin{equation} 
  \mbox{Ganho de Informação}(D,T)=\mbox{entropia}(D)-\displaystyle \sum_{i=1}^k \frac{|D_i|}{|D|}. \mbox{entropia}(D_i) 
  \tag{5.11}
\end{equation}\]</span></p>
<p>É muito utilizado em algoritmo de <strong>Árvore de decisão</strong> que será apresentado na seção <a href="ptII.html#ptII">8</a> com um exemplo de seu uso.</p>
</div>
<div id="meddist" class="section level3">
<h3><span class="header-section-number">5.7.3</span> Medidas de Similaridade e Dissimilaridade</h3>
<p>É provável que já tenha ouvido falar em algo como medidas de distância, separabilidade, discriminação, divergência, similaridade ou dissimilaridade. É uma questão importante decidir até que ponto dois elementos de um conjunto de dados podem ser considerados com caractéristicas semelhantes ou não.</p>
<p>Supondo que possuímos um conjunto de dados constituído por <span class="math inline">\(n\)</span> elementos amostrais. O objetivo é agrupar esses elementos em <span class="math inline">\(g\)</span> grupos de acordo com um vetor <span class="math inline">\(X_j=[X_{1j}X_{2j}...X_{pj}]&#39;, j=1,2,...,n\)</span> que representa o valor observado da variável <span class="math inline">\(i\)</span> medida no elemento <span class="math inline">\(j\)</span>. Lembrando que existem diversas medidas diferentes e que produzem um determinado tipo de agrupamento de acordo com sua metodologia. A seguir será apresentado algumas comuns no ramo.</p>
<div id="disteuclidana" class="section level4">
<h4><span class="header-section-number">5.7.3.1</span> Distância Euclidiana</h4>
<p>A distância Euclidiana entre dois elementos <span class="math inline">\(X_l\)</span> e <span class="math inline">\(X_k\)</span>, com <span class="math inline">\(l \neq k\)</span>, é definida por:</p>
<p><span class="math display" id="eq:euclidiana">\[\begin{equation} 
  d(X_l,X_k)=[(X_l-X_k)&#39;(X_l - X_k)]^{1/2}=[\displaystyle \sum^p_{i=1}(X_{il}-X_{ik})^2]^{1/2}
  \tag{5.12}
\end{equation}\]</span></p>
<p>sendo comparado os dois elementos amostrais em cada variável pertencente ao vetor. Por exemplo, a tabela a seguir apresenta a renda mensal (em salários mínimos) e a idade de seis indivíduos de uma localidade <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</p>
<table>
<caption><span id="tab:dadossrendaa">Tabela 5.2: </span> Renda e Idade de 6 indíviduos <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Indivíduo</strong></th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
<th align="center">D</th>
<th align="center">E</th>
<th align="center">F</th>
<th align="center">Média</th>
<th align="center">Desvio Padrão</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Renda</strong></td>
<td align="center">9,6</td>
<td align="center">8,4</td>
<td align="center">2,4</td>
<td align="center">18,2</td>
<td align="center">3,9</td>
<td align="center">6,4</td>
<td align="center">8,15</td>
<td align="center">5,61</td>
</tr>
<tr class="even">
<td align="center"><strong>Idade</strong></td>
<td align="center">28</td>
<td align="center">31</td>
<td align="center">42</td>
<td align="center">38</td>
<td align="center">25</td>
<td align="center">41</td>
<td align="center">34,17</td>
<td align="center">7,14</td>
</tr>
</tbody>
</table>
<p>A distância Euclidiana entre os indíviduos A e B nas variáveis Renda e Idade será:</p>
<p><span class="math display">\[d(X_A,X_B)=[(9,60-8,40)^2+(28-31)^2]^{1/2}=3,23\]</span>
Assim sucessivamente para cada uma das observações, obtemos a matriz:</p>
<p><span class="math display">\[D_{6x6}=\begin{bmatrix}\\
 &amp;A&amp;B&amp;C&amp;D&amp;E&amp;F \\
 A&amp;0&amp;&amp;&amp;&amp;&amp;\\
 B&amp;3,23&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 15,74&amp; 12,53&amp;0&amp;&amp;&amp;\\
 D&amp; 13,19&amp; 12,04&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,44&amp; 7,50&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 13,39&amp; 10,19&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Podendo agora analisar quais observações estão mais próximas entre si ou mais distantes de acordo com as características da Renda e da Idade.</p>
</div>
<div id="distponderada" class="section level4">
<h4><span class="header-section-number">5.7.3.2</span> Distância Ponderada</h4>
<p>Também conhecido como <strong>distância generalzada</strong>, esta metologia tem como base uma matriz <span class="math inline">\(A_{pxp}\)</span> de ponderação. Quando <span class="math inline">\(A_{pxp}\)</span> for uma matriz identidade, esta distância generalizada será a <strong>distância Euclidiana</strong>; se <span class="math inline">\(A_{pxp}\)</span> for a matriz inversa da matriz de covariâncias amostrais <span class="math inline">\(S^{-1}_{pxp}\)</span>, será a <strong>distância de <span class="citation">Mahalanobis (<a href="#ref-mahalanobis1936generalized">1936</a>)</span></strong> e se <span class="math inline">\(A_{pxp}=diag(\frac{1}{p})\)</span>, teremos a distância <strong>Euclidiana Média</strong>. A escolha dessa matriz <span class="math inline">\(A_{pxp}\)</span> reflete o tipo de informação que o pesquisador pretende utilizar na ponderação das diferenças das coordernadas dos vetores em estudo <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>, a distância de Mahalanobis, por exemplo, leva em consideração as possíveis diferenças de variâncias e as relações lineares entre as variáveis, em termos de variância, na ponderação.</p>
<p><span class="math display" id="eq:distpond">\[\begin{equation} 
  d(X_l,X_k)=[(X_l-X_k)&#39;A_{pxp}(X_l - X_k)]^{1/2}
  \tag{5.13}
\end{equation}\]</span></p>
<p>Continuando com dados da <a href="dicio.html#tab:dadossrendaa">5.2</a> e tomando como exemplo de ponderada pelo método de Mahalanobis. Ao calcular a matriz de covariância e sua respectiva inversa, obtemos:</p>
<p><span class="math display">\[S=
\begin{bmatrix}
31.471&amp; 2.15000 \\
2.150&amp; 50.96667 
\end{bmatrix}\]</span>
<span class="math display">\[S^{-1}=
\begin{bmatrix}
0,0032&amp; -0,0013 \\
-0,0013&amp; 0,019 
\end{bmatrix}\]</span></p>
<p>Portanto pelo cálculo da distância Ponderada por Mahalanobis:</p>
<p><span class="math display">\[d(X_A,X_B)=\bigg[(1,2 \ \ -3) S^{-1} \begin{pmatrix} 1,2 \\ -3  \end{pmatrix} \bigg]^{1/2}=0,46\]</span>
E sucessivamente calcula-se para às outras observações.</p>
</div>
</div>
<div id="medidas-de-precisão" class="section level3">
<h3><span class="header-section-number">5.7.4</span> Medidas de Precisão</h3>
<p>Referente a tarefas de precisão, dado um algoritmo de aprendizado com sua amostra de dados. o algoritmo de maior desempenho preditivo ao modelo será selecionado <span class="citation">(Kohavi, John, and others <a href="#ref-kohavi1997wrappers">1997</a>)</span>. Não necessariamente precisa de um único subconjunto ótimo de atributos, pois é possível alcançar a mesma precisão com diferentes subconjuntos de atributos.</p>
<p>A <strong>Utilidade Incremental</strong> por exemplo <span class="citation">(Caruana and Freitag <a href="#ref-caruana1994useful">1994</a>)</span>, onde uma dada amostra de dados <span class="math inline">\(S\)</span>, com um determinado algoritmo de aprendizado e um subconjunto de atributos. Um atributo <span class="math inline">\(X_i\)</span> é incrementalmente útil para o modelo em relação ao subconjunto de dados <span class="math inline">\(F\)</span> se a precisão da hipótese produzida pelo modelo considerando o conjunto de atributos <span class="math inline">\(X_i \cup F\)</span> é melhor que a precisão alcançanda utilizando-se apenas o subconjunto <span class="math inline">\(F\)</span> <span class="citation">(Lee <a href="#ref-lee2005seleccao">2005</a>)</span>.</p>
<p>É muito comum seu uso em algoritmos de seleção de atributos que realizam a busca no espaço de subconjuntos de atributos, removendo e adicionando-os com abordagens como <em>wrapper</em> e <em>embedded</em> (apresentadas na seção seguinte). Importante lembrar que um atributo considerado importante não implica que o mesmo estará no subconjunto ótimo de atributos.</p>
</div>
<div id="medidas-de-consistência" class="section level3">
<h3><span class="header-section-number">5.7.5</span> Medidas de consistência</h3>
<p>São medidas dependentes do conjunto de treinamento que permitem encontrar um subconjunto mínimo de atributos que satisfaz a proporção de inconsistência aceita (definida geralmente pelo pesquisador e com base alguma fundamentação teórica). O objetivo da análise por consistência é proporcionar a construção de hipóteses lógicas consistentes em um conjunto de treinamento. Note que elas não detectam a ocorrência de atributos redundantes, pois não possibilitam a distinção entre atributos igualmente adequados <span class="citation">(Parmezan et al. <a href="#ref-parmezan2012avaliaccao">2012</a>)</span>.</p>
<p>Um atributo <span class="math inline">\(X_i\)</span> é importante se aparece em toda fórmula <em>booleana</em> e do contrário não importante <span class="citation">(Almuallim and Dietterich <a href="#ref-almuallim1994learning">1994</a>; Lee <a href="#ref-lee2005seleccao">2005</a>)</span>, por exemplo:</p>
<p><span class="math display">\[X_1=1 \ \mbox{e} \ X_2=0 \ \mbox{então classe}=1\]</span>
<span class="math display">\[X_1=1 \ \mbox{e} \ X_3=0 \ \mbox{então classe}=1\]</span>
<span class="math display">\[X_1=0 \ \mbox{e} \ X_2=1 \ \mbox{então classe}=0\]</span>
Partindo dessa definição, <span class="math inline">\(X_1\)</span> é considerado importante pois é encontrado em todas as regras delimitadas, e portanto, <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_3\)</span> não são importantes.</p>
<p><span class="citation">Dash and Liu (<a href="#ref-dash2003consistency">2003</a>)</span> e <span class="citation">Liu, Setiono, and others (<a href="#ref-liu1996probabilistic">1996</a>)</span> definem como critério de avaliação que um subconjunto de atributos importantes é definido por meio de uma <strong>taxa de inconsistência</strong>.</p>
<ol style="list-style-type: decimal">
<li><p>Um exemplo será considerado <strong>inconsistente</strong> se existirem pelo menos dois exemplos exatamente iguais exceto pelo valor da classe;</p></li>
<li><p>A <strong>contagem de inconsistência</strong> é dada pelo número de vezes que este exemplo ocorre nos dados subtraído o maior número entre as diferentes classes;</p></li>
<li><p>a <strong>taxa de inconsistência</strong> de um subconjunto de atributos é a soma de todas as contagens de inconsistência de todos os exemplos do subconjunto nos dados dividido pelo número total de exemplos.</p></li>
</ol>
<p>Por exemplo: um exemplo <span class="math inline">\(E_i\)</span> inconsistente aparece <span class="math inline">\(N_{Ei}\)</span> vezes dos quais <span class="math inline">\(N_{C1}\)</span> pertencem à classe <span class="math inline">\(C_1\)</span>, <span class="math inline">\(N_{C2}\)</span> pertencem à classe <span class="math inline">\(C_2\)</span> e <span class="math inline">\(N_{C3}\)</span> pertencem à classe <span class="math inline">\(C_3\)</span>. Portanto <span class="math inline">\(N_{Ei}=N_{C1}+N_{C2}N_{C3}\)</span>. Supondo que <span class="math inline">\(N_{C3}\)</span> é o maior valor entre todos, a contagem de inconsistência é <span class="math inline">\(N_{Ei}-N_{C3}\)</span>. Com dado o subconjunto e um valor mínim da taxa delimitado pelo pesquisador, se a taxa de inconsistência for menor que o definido, poderá ser dito consistente <span class="citation">(Lee <a href="#ref-lee2005seleccao">2005</a>)</span>.</p>
<p>Existe muitos critérios de importância de atributos em muitas literaturas e torna dificultoso em identificar quais algoritmos e metodologias são mais apropriados para o conjunto de dados. Com as medidas de importância, torna-se possível avaliar se os atributos selecionados auxiliam no modelo proposto pelo pesquisador ou o oposto. Cabe ao pesquisador com base em literaturas verificar qual utilizador de acordo com suas preferências e conjunto de dados em análise.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-almuallim1994learning">
<p>Almuallim, Hussein, and Thomas G Dietterich. 1994. “Learning Boolean Concepts in the Presence of Many Irrelevant Features.” <em>Artificial Intelligence</em> 69 (1-2): 279–305.</p>
</div>
<div id="ref-banzatto1992experimentaccao">
<p>Banzatto, David Ariovaldo, and S do N Kronka. 1992. “Experimentação Agrı́cola.” <em>Jaboticabal: Funep</em> 2.</p>
</div>
<div id="ref-bolfarine2001introduccao">
<p>Bolfarine, Heleno, and Mônica Carneiro Sandoval. 2001. <em>Introdução à Inferência Estatı́stica</em>. Vol. 2. SBM.</p>
</div>
<div id="ref-caruana1994useful">
<p>Caruana, Rich, and Dayne Freitag. 1994. “How Useful Is Relevance?” <em>FOCUS</em> 14 (8): 2.</p>
</div>
<div id="ref-dash2003consistency">
<p>Dash, Manoranjan, and Huan Liu. 2003. “Consistency-Based Search in Feature Selection.” <em>Artificial Intelligence</em> 151 (1-2): 155–76.</p>
</div>
<div id="ref-fariaestatistic">
<p>Farias, Ana Maria Lima de. 2010. <em>Métodos Estatísticos Ii</em>. V. único. Rio de Janeiro, RJ: Fundação CECIERJ.</p>
</div>
<div id="ref-freund2009estatistica">
<p>Freund, John E. 2009. <em>Estatı́stica Aplicada-: Economia, Administração E Contabilidade</em>. Bookman Editora.</p>
</div>
<div id="ref-gujarati2011econometria">
<p>Gujarati, Damodar N, and Dawn C Porter. 2011. <em>Econometria Básica-5</em>. Amgh Editora.</p>
</div>
<div id="ref-hartley1928transmission">
<p>Hartley, Ralph VL. 1928. “Transmission of Information 1.” <em>Bell System Technical Journal</em> 7 (3): 535–63.</p>
</div>
<div id="ref-theil1978">
<p>Henri, Theil. 1978. <em>Introduction to Econometrics</em>. Englewood Cliffs, New Jersey: Prentice Hall.</p>
</div>
<div id="ref-kohavi1997wrappers">
<p>Kohavi, Ron, George H John, and others. 1997. “Wrappers for Feature Subset Selection.” <em>Artificial Intelligence</em> 97 (1-2): 273–324.</p>
</div>
<div id="ref-lee2005seleccao">
<p>Lee, Huei Diana. 2005. “Seleção de Atributos Importantes Para a Extração de Conhecimento de Bases de Dados.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div id="ref-liu2012feature">
<p>Liu, Huan, and Hiroshi Motoda. 2012. <em>Feature Selection for Knowledge Discovery and Data Mining</em>. Vol. 454. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-liu1996probabilistic">
<p>Liu, Huan, Rudy Setiono, and others. 1996. “A Probabilistic Approach to Feature Selection-a Filter Solution.” In <em>ICML</em>, 96:319–27. Citeseer.</p>
</div>
<div id="ref-mahalanobis1936generalized">
<p>Mahalanobis, Prasanta Chandra. 1936. “On the Generalized Distance in Statistics.” In. National Institute of Science of India.</p>
</div>
<div id="ref-mingoti2007analise">
<p>Mingoti, Sueli Aparecida. 2007. “Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada.” In <em>Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada</em>, 295–95.</p>
</div>
<div id="ref-moser2012student">
<p>Moser, Stefan M, and Po-Ning Chen. 2012. <em>A Student’s Guide to Coding and Information Theory</em>. Cambridge University Press.</p>
</div>
<div id="ref-nyquist1924certain">
<p>Nyquist, Harry. 1924. “Certain Factors Affecting Telegraph Speed.” <em>Transactions of the American Institute of Electrical Engineers</em> 43: 412–22.</p>
</div>
<div id="ref-parmezan2012avaliaccao">
<p>Parmezan, Antonio Rafael Sabino, Huei Diana Lee, Newton Spolaôr, and Wu Feng Chung. 2012. “Avaliação de Métodos Para Seleção de Atributos Importantes Para Aprendizado de Máquina Supervisionado No Processo de Mineração de Dados.”</p>
</div>
<div id="ref-paviotti2019consideraccoes">
<p>Paviotti, José Renato, and Carlos J Magossi. 2019. “Considerações Sobre O Conceito de Entropia Na Teoria Da Informação.”</p>
</div>
<div id="ref-shannon1948mathematical">
<p>Shannon, Claude E. 1948. “A Mathematical Theory of Communication.” <em>The Bell System Technical Journal</em> 27 (3): 379–423.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machinelearning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="preprocesso.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05.1-fundamentosIIanova.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
