<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 3 Uma breve revisão | Fundamentos de Machine Learning</title>
  <meta name="description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 3 Uma breve revisão | Fundamentos de Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 3 Uma breve revisão | Fundamentos de Machine Learning" />
  
  <meta name="twitter:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2021-04-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machinelearning.html"/>
<link rel="next" href="preprocesso.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>1</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="1.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>1.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="1.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>1.2</b> A arte de uma IA</a></li>
<li class="chapter" data-level="1.3" data-path="i-a.html"><a href="i-a.html#vertentes-de-uma-ia-e-fundamentação-filosófica"><i class="fa fa-check"></i><b>1.3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>2</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="2.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>2.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dicio.html"><a href="dicio.html"><i class="fa fa-check"></i><b>3</b> Uma breve revisão</a><ul>
<li class="chapter" data-level="3.1" data-path="dicio.html"><a href="dicio.html#um-pouco-de-álgebra-linear"><i class="fa fa-check"></i><b>3.1</b> Um pouco de Álgebra Linear</a></li>
<li class="chapter" data-level="3.2" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística"><i class="fa fa-check"></i><b>3.2</b> Um pouco de Estatística</a></li>
<li class="chapter" data-level="3.3" data-path="dicio.html"><a href="dicio.html#medidasimport"><i class="fa fa-check"></i><b>3.3</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="3.3.1" data-path="dicio.html"><a href="dicio.html#medidasdep"><i class="fa fa-check"></i><b>3.3.1</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="3.3.2" data-path="dicio.html"><a href="dicio.html#medinfo"><i class="fa fa-check"></i><b>3.3.2</b> Medidas de Informação</a></li>
<li class="chapter" data-level="3.3.3" data-path="dicio.html"><a href="dicio.html#meddist"><i class="fa fa-check"></i><b>3.3.3</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="3.3.4" data-path="dicio.html"><a href="dicio.html#medidas-de-precisão"><i class="fa fa-check"></i><b>3.3.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="3.3.5" data-path="dicio.html"><a href="dicio.html#medidas-de-consistência"><i class="fa fa-check"></i><b>3.3.5</b> Medidas de consistência</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>4</b> Pré-processamento</a><ul>
<li class="chapter" data-level="4.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>4.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="4.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>4.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="4.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>4.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>4.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="4.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="4.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>4.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>4.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>5</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="5.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>5.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="5.1.1" data-path="valid.html"><a href="valid.html#overfitting"><i class="fa fa-check"></i><b>5.1.1</b> <strong>Overfitting</strong></a></li>
<li class="chapter" data-level="5.1.2" data-path="valid.html"><a href="valid.html#underfitting"><i class="fa fa-check"></i><b>5.1.2</b> <strong>Underfitting</strong></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="valid.html"><a href="valid.html#holdout"><i class="fa fa-check"></i><b>5.2</b> Validação cruzada Hold-out</a></li>
<li class="chapter" data-level="5.3" data-path="valid.html"><a href="valid.html#kfold"><i class="fa fa-check"></i><b>5.3</b> Validação Cruzada <em>K-fold</em></a></li>
<li class="chapter" data-level="5.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>5.4</b> ROC e AUC</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>6</b> Modelos de Aprendizagem I</a><ul>
<li class="chapter" data-level="6.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="6.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>6.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>6.2</b> Regressão</a><ul>
<li class="chapter" data-level="6.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>6.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="6.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>6.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="6.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>6.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="6.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>6.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>6.3</b> Gradiente Descendente</a><ul>
<li class="chapter" data-level="6.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>6.3.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regularizacao"><i class="fa fa-check"></i><b>6.4</b> Regularização</a><ul>
<li class="chapter" data-level="6.4.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#penalizacoes"><i class="fa fa-check"></i><b>6.4.1</b> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></a></li>
<li class="chapter" data-level="6.4.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#elasticnet"><i class="fa fa-check"></i><b>6.4.2</b> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#k-vizinhos-mais-próximos-k-nearest-neighbors"><i class="fa fa-check"></i><b>6.5</b> K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</a><ul>
<li class="chapter" data-level="6.5.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exknn"><i class="fa fa-check"></i><b>6.5.1</b> Exemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>7</b> Modelos de Aprendizagem II</a><ul>
<li class="chapter" data-level="7.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>7.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="7.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>7.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="7.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>7.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>7.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>7.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>7.3</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>7.3.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="7.3.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>7.3.2</b> Estatísticas</a></li>
<li class="chapter" data-level="7.3.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>7.3.3</b> A ACP</a></li>
<li class="chapter" data-level="7.3.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>7.3.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>7.4</b> Análise de Agrupamentos - <em>Clusters</em></a><ul>
<li class="chapter" data-level="7.4.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>7.4.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="7.4.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>7.4.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="7.4.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>7.4.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ptII.html"><a href="ptII.html#redesneurais"><i class="fa fa-check"></i><b>7.5</b> Redes Neurais Artificiais</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptIII.html"><a href="ptIII.html"><i class="fa fa-check"></i><b>8</b> Os métodos <em>Ensemble</em></a><ul>
<li class="chapter" data-level="8.1" data-path="ptIII.html"><a href="ptIII.html#bagging"><i class="fa fa-check"></i><b>8.1</b> <em>Bagging</em></a></li>
<li class="chapter" data-level="8.2" data-path="ptIII.html"><a href="ptIII.html#boost"><i class="fa fa-check"></i><b>8.2</b> <em>Boosting</em></a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptIII.html"><a href="ptIII.html#adaboost"><i class="fa fa-check"></i><b>8.2.1</b> <em>AdaBoost</em></a></li>
<li class="chapter" data-level="8.2.2" data-path="ptIII.html"><a href="ptIII.html#gradientboost"><i class="fa fa-check"></i><b>8.2.2</b> <em>Gradient Boosting</em></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptIII.html"><a href="ptIII.html#bagboost"><i class="fa fa-check"></i><b>8.3</b> <em>Bagging x Boosting</em></a></li>
<li class="chapter" data-level="8.4" data-path="ptIII.html"><a href="ptIII.html#stacking"><i class="fa fa-check"></i><b>8.4</b> <em>Stacking</em></a></li>
<li class="chapter" data-level="8.5" data-path="ptIII.html"><a href="ptIII.html#rf"><i class="fa fa-check"></i><b>8.5</b> Floresta Aleatória - <em>Random Forest</em></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>9</b> <em>Deep Learning</em></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de <em>Machine Learning</em></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dicio" class="section level1">
<h1><span class="header-section-number">Capítulo 3</span> Uma breve revisão</h1>
<p>Com os <em>softwares</em> atuais é possível de que o pesquisador consiga fazer uma análise dos dados sem compreender totalmente a matemática por trás. Busco sempre que puder anexar um exemplo de acordo com cada tema apresentado para facilitar a compreensão, porém suponho de que o leitor esteja familiarizado com conceitos fundamentais de estatística, probabilidade e álgebra linear, portanto conceitos fundamentais como: tipos de amostragem, probabilidades e suas distribuições, teste de hipóteses e significância, potência dos testes estatísticos e intervalos de confiança, escalares e vetores, espaço vetorial e transformação linear, produto interno, assimetria e curtose, limites, derivadas e integrais, entre outros..</p>
<p>Nesta seção são apresentadas brevemente um pouco desses conteúdos. É provável de que o leitor já saiba. Porém acredito de que sejam fundamentais para o Aprendizado de Máquina e seria bom para revisá-lo. Sinta-se livre em pular este capítulo. Ao caso de ser algo totalmente novo, reforço-o de introduzir com outras literaturas, além dessa, pois são imprescindíveis aos conteúdos dos próximos capítulos.</p>
<p><strong>Bons estudos</strong>.</p>
<div id="um-pouco-de-álgebra-linear" class="section level2">
<h2><span class="header-section-number">3.1</span> Um pouco de Álgebra Linear</h2>
<p>Primeiramente é importante lembrar que grandezas escalares necessitam apenas do valor numérico (módulo) e para compreender grandezas vetoriais necessitam da direção e do sentido, além do módulo.</p>
<p>Para representação gráfica de uma grandeza vetorial, utiliza-se um segmento orientado <strong>AB</strong>, como uma “flecha”. Quanto maior for o tamanho deste segmento, maior o módulo deste vetor, ou seja, é proporcional ao comprimento.</p>
<div class="figure" style="text-align: center"><span id="fig:vetor"></span>
<img src="Figuras/vetor.png" alt="Demonstração de um vetor." width="80%" />
<p class="caption">
Figura 3.1: Demonstração de um vetor.
</p>
</div>

<ol style="list-style-type: decimal">
<li><strong>Espaço Vetorial</strong></li>
</ol>
<p>Estrutura que generaliza as propriedades de vetores em <span class="math inline">\(\mathbb{R}^3\)</span>, podendo somar elementos e realizar multiplicação por escalares. Um espaço vetorial sobre um corpo <span class="math inline">\(K\)</span> é um conjunto <span class="math inline">\(V\)</span> com as operações adição de vetores (<span class="math inline">\(+\)</span>) e multiplicação por escalar (concatenação). A soma opera em pares de vetores e retorna um vetor (<span class="math inline">\(+:V\ \mbox{x} \ V \rightarrow V\)</span>), e a multiplicação por escalar opera em pares de escalar e vetor, retornando (<span class="math inline">\(.:K \ \mbox{x} \ V \rightarrow V\)</span>). Para que <span class="math inline">\(V\)</span> e <span class="math inline">\(K\)</span> com as duas operações forme um espaço vetorial as operações devem ser <span class="citation">(Pellegrini <a href="#ref-algebrajeronimo">2015</a>)</span>:</p>
<ul>
<li>Associativas: <span class="math inline">\(c(dv)=(cd)v\)</span>;</li>
<li>A soma de vetores é comutativa: <span class="math inline">\(u+w=w+u\)</span>;</li>
<li>A multiplicação por escalar (.) é distributiva, tanto em adição de vetores quanto em adição de escalares: <span class="math inline">\(c(v+w)=cv+cw\)</span>;</li>
<li>Existe um vetor <span class="math inline">\(0\)</span>, neutro para adição: <span class="math inline">\(v+0=v\)</span>;</li>
<li>Para todo vetor <span class="math inline">\(v\)</span> existe um <span class="math inline">\(-v\)</span>, tal que <span class="math inline">\(v+(-v)=0\)</span>;</li>
<li><p>Mutiplicação pela identidade do corpo não modificar um vetor <span class="math inline">\(lv=v\)</span>.</p></li>
<li><p><strong>Exemplo 1:</strong> o conjunto composto pelos números reais <span class="math inline">\(\mathbb{R}\)</span>, com as operações de multiplicação e adição entre números reais usuais é um espaço vetorial real pois todas as propriedades são verificadas.</p></li>
<li><p><strong>Exemplo 2:</strong> o conjunto <span class="math inline">\(M_{mn}(K)\)</span> das matrizes com <span class="math inline">\(m\)</span> linhas e <span class="math inline">\(n\)</span> colunas com elementos de um corpo <span class="math inline">\(K\)</span> é um espaço vetorial de <span class="math inline">\(K\)</span>.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Transformação Linear</strong></li>
</ol>
<p>Sejam <span class="math inline">\(U\)</span> e <span class="math inline">\(V\)</span> dois espaços vetoriais sobre um mesmo corpo. Uma transformação linear é uma função <span class="math inline">\(T:V\rightarrow U\)</span> tal que para todo escalar <span class="math inline">\(c\)</span> e todos vetores <span class="math inline">\(v,w \ \epsilon \ V\)</span> <span class="citation">(Pellegrini <a href="#ref-algebrajeronimo">2015</a>)</span>,</p>
<ul>
<li><span class="math inline">\(T(v+w)=T(v)+T(w)\)</span>;</li>
<li><span class="math inline">\(T(cv)=cT(v)\)</span>.</li>
</ul>
<p>Um operador linear é uma transformação linear de um espaço nele mesmo (<span class="math inline">\(T:U\rightarrow U\)</span>).</p>
<ul>
<li><p><strong>Exemplo 1:</strong> A função que dá a transposta de uma matriz é uma transformação linear de <span class="math inline">\(M_{mn}\)</span> em <span class="math inline">\(M_{mn}\)</span>: claramente, <span class="math inline">\(c(A^T)=(cA)^T\)</span>, e <span class="math inline">\(A^T+B^T=(A+B)^T\)</span>.</p></li>
<li><p><strong>Exemplo 2:</strong> A função <span class="math inline">\(T:\mathbb{R}\rightarrow \mathbb{R}\)</span>, com <span class="math inline">\(T(x)=x+4\)</span>. Esta função não é transformação linear, pois:</p></li>
</ul>
<p><span class="math display">\[T(x+y)=T(x+y)+2\]</span>
<span class="math display">\[T(x+y)\neq T(x)+T(y)=(x+2)+(y+2)=(x+y)+4\]</span>
<span class="math display">\[\mbox{logo:} \ T(x+y)\neq T(x)+T(y)\]</span></p>
<ul>
<li><strong>Exemplo 3:</strong> a função <span class="math inline">\(f(x_1,x_2)=x_1+x_2\)</span> é uma transformação linear de <span class="math inline">\(\mathbb{R}^2\)</span> em <span class="math inline">\(\mathbb{R}\)</span>, pois para dois vetores <span class="math inline">\((x_1,x_2)\)</span> e <span class="math inline">\((y_1,y_2)\)</span>,</li>
</ul>
<p><span class="math display">\[f[(x_1,x_2)+(y_1,y_2)]=f(x_1+y_1,x_2+y_2)=x_1+y_1+x_2+y_2=f(x_1,x_2)+f(y_1+y_2)\]</span></p>
<p>ou para qualquer constante <span class="math inline">\(k\)</span> e qualquer vetor (<span class="math inline">\(x_1,x_2\)</span>),</p>
<p><span class="math display">\[f(k(x_1,x_2))=f(kx_1,kx_2)=kx_1+kx_2=k(x_1+x_2)=kf(x_1,x_2)\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Produto Interno</strong></li>
</ol>
<p>Um produto interno em um espaço vetorial <span class="math inline">\(V\)</span> sobre <span class="math inline">\(\mathbb{R}\)</span> é uma função de <span class="math inline">\(V \mbox{x} V\)</span> em <span class="math inline">\(\mathbb{R}\)</span>, denotado por <span class="math inline">\(\langle u,v \rangle\)</span>, possui as seguintes propriedades:</p>
<ul>
<li>comutatividade: <span class="math inline">\(\langle u,v \rangle=\langle v,u \rangle\)</span>;</li>
<li>positividade: <span class="math inline">\(\langle v,v \rangle \geq 0,\)</span> e <span class="math inline">\(\langle 0,0 \rangle =0\)</span>;</li>
<li>bilinearidade: o produto interno é linear nos dois argumentos (pois são comutativos) - para todo escalar e vetor.</li>
</ul>
<p>O produto interno (ou escalar) em <span class="math inline">\(\mathbb{R}^2\)</span> ou <span class="math inline">\(\mathbb{R}^3\)</span> pode ser expresso por:
<span class="math display">\[\langle u,v \rangle =\sum_i u_i \ . \  v_i = u^Tv\]</span>
em que <span class="math inline">\(T\)</span> indica a transposta de <span class="math inline">\(u\)</span>.</p>
<p>Se o produto interno de dois vetores for igual a zero, podemos dizer que são ortogonais.</p>
<ul>
<li><strong>Exemplo:</strong> Calcule o produto interno de <span class="math inline">\(u=(2,3,1)\)</span> e <span class="math inline">\(v=(1,2,2)\)</span>:</li>
</ul>
<p><span class="math display">\[\mbox{temos que: }\langle u,v \rangle =u \ . \  v\]</span>
<span class="math display">\[=(2,3,1).(1,2,2)=2.1+3.2+1.2=10\]</span></p>
</div>
<div id="um-pouco-de-estatística" class="section level2">
<h2><span class="header-section-number">3.2</span> Um pouco de Estatística</h2>
<ol style="list-style-type: decimal">
<li><strong>Parâmetros</strong></li>
</ol>
<p>Podem ser vistos como características numéricas de um modelo ou população. Os valores não podem ser mensurados diretamente mas que podem ser estimados através dos dados de uma amostra.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Paramétrico x Não Paramétrico</strong></li>
</ol>
<p>Os testes paramétricos assumem que a distribuição de probabilidade da população seja conhecida nos dados extraídos e que somente os valores de certos parâmetros, tais quais média e o desvio padrão, sejam desconhecidos. Ao caso dos dados não satisfazerem as suposições assumidas pelas técnicas tradicionais, utiliza-se métodos não paramétricos de inferência estatística. As técnicas não paramétricas assumem poucas ou até mesmo nenhuma hipótese sobre a distribuição de probabilidade da população, podemos dizer que não possuem dados com estruturas ou parâmetros característicos.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Variância e Desvio Padrão (Erro Padrão)</strong></li>
</ol>
<p>A dispersão de um conjunto de dados será pequena se os valores estão concentrados em torno da média e grande no oposto, denota-se <strong>desvios da média</strong> medir a variação de um conjunto de dados em termos das quantidades pelas quais os valores desviam de sua média:</p>
<p><span class="math display" id="eq:desviomedia">\[\begin{equation}
    x_1-\overline{x},x_2-\overline{x},...,x_n-\overline{x} \rightarrow \sum(x-\overline{x})
        \tag{3.1}
\end{equation}\]</span></p>
<p>Não estamos interessados se são negativos ou positivos, e sim em sua magnitude dos desvios. Um meio é trabalharmos com os quadrados dos desvio da média e extrairmos a raiz quadrada do resultado (compensar o uso do quadrado dos desvios):</p>
<p><span class="math display" id="eq:desviomedquad">\[\begin{equation}
 \sqrt{\frac{\sum(x-\overline{x})^2}{n}}
     \tag{3.2}
\end{equation}\]</span></p>
<p>Como na prática não possuímos a média verdadeira (populacional), somente a estimada, calcula-se então a nomeada <strong>desvio-padrão amostral</strong> denotada por <span class="math inline">\(s\)</span>:</p>
<p><span class="math display" id="eq:desviopadrao">\[\begin{equation}
s=\sqrt{\frac{\sum(x-\overline{x})^2}{n-1}}
    \tag{3.3}
\end{equation}\]</span></p>
<p>O quadrado do desvio-padrão, ou seja, <span class="math inline">\(s^2\)</span> é denominado <strong>variância amostral</strong> e tanto o desvio-padrão quanto a variância são medidas de dispersão.</p>
<p>Importante ressaltar que costumamos tratar em população como <strong>desvio-padrão populacional</strong> (<span class="math inline">\(\sigma\)</span> quando dividimos por <span class="math inline">\(N\)</span> e <span class="math inline">\(S\)</span> quando dividimos por <span class="math inline">\(N-1\)</span>). Sendo <span class="math inline">\(\sigma^2\)</span> sua <strong>variância populacional</strong>.</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Covariância</strong></li>
</ol>
<p>A covariância mede a relação linear entre duas variáveis. É possível utilizar a covariância para compreender a direção da relação entre as variáveis. Valores de covariância positivos indicam que valores acima da média de uma variável estão associados a valores médios acima da outra variável e abaixo dos valores médios são igualmente associado. Valores de covariância negativos indicam que valores acima da média de uma variável estão associados com valores médios abaixo da outra variável.</p>
<p><span class="math display" id="eq:covariancia">\[\begin{equation}
    Cov(x,y)=s^2_{xy}=E(xy)-E(x)E(y)
    \tag{3.4}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(E\)</span> é a esperança (média).</p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Distribuição Normal</strong></li>
</ol>
<p>Em estatística, uma distribuição de probabilidade descreve o comportamento aleatório de um fenômeno dependente do acaso. Há muitas distribuições de probabilidade diferentes, sendo de muita importância, a distribuição simétrica perfeitamente denominada como <strong>distribuição em forma de sino</strong> ou <strong>distribuição normal</strong>. Quando a média, mediana e a moda coincidem. Dizemos que <span class="math inline">\(X\)</span> é uma variável aleatória normal, ou normalmente distribuída, com parâmetros <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma^2\)</span>, se a função densidade de <span class="math inline">\(X\)</span> é dada por:</p>
<p><span class="math display" id="eq:fnormal">\[\begin{equation}
    f(x)=\frac{1}{\sqrt{2\pi \sigma}}e^{-(x-\mu)^2/2\sigma^2} \ \ -\infty&lt;x&lt;\infty
    \tag{3.5}
\end{equation}\]</span></p>
<p>A função de densidade é uma curva em forma de sino simétrica em relação a <span class="math inline">\(\mu\)</span>, como demonstra a seguinte figura:</p>
<div class="figure" style="text-align: center"><span id="fig:distnormal"></span>
<img src="Figuras/distnormal.png" alt="Função de densidade de probabilidade normal: (a) \(\mu\), \(\sigma=1\); (b) \(\mu\) e \(\sigma^2\) arbitrários (S.M 2010)." width="80%" />
<p class="caption">
Figura 3.2: Função de densidade de probabilidade normal: (a) <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma=1\)</span>; (b) <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma^2\)</span> arbitrários <span class="citation">(S.M <a href="#ref-rossprobability">2010</a>)</span>.
</p>
</div>

<ol start="6" style="list-style-type: decimal">
<li><strong>Distribuição Binomial</strong></li>
</ol>
<p>A distribuição de probabilidade discreta do número de sucessos com <span class="math inline">\(n\)</span> tentativas, supondo <span class="math inline">\(X\)</span> binomial com parâmetros <span class="math inline">\((n,p)\)</span>. Calcula-se sua distribuição como:</p>
<p><span class="math display" id="eq:fbinominal">\[\begin{equation}
    P[X\leq i]=\displaystyle \sum_{k=0}^i \begin{pmatrix}n\\k \end{pmatrix}p^k(1-p)^{n-k} \ \ \ i=0,1,...,n
    \tag{3.6}
\end{equation}\]</span></p>
<p>onde probabilidade de um ponto amostral com sucessos nos <span class="math inline">\(k\)</span> primeiros ensaios e falhas nos <span class="math inline">\(n−k\)</span> ensaios seguintes é <span class="math inline">\(p^k(1−p)^{n−k}\)</span>.</p>
<ol start="7" style="list-style-type: decimal">
<li><strong>Distribuição de <em>Poisson</em></strong></li>
</ol>
<p>A variável aleatória <span class="math inline">\(X\)</span> que pode assumir um valores <span class="math inline">\(0,1,2,...\)</span> é chamada de variável aleatória de <em>Poisson</em> com parâmetro <span class="math inline">\(\lambda&gt;0\)</span> <span class="citation">(S.M <a href="#ref-rossprobability">2010</a>)</span>:</p>
<p><span class="math display" id="eq:poisson">\[\begin{equation}
    p(i)=P\{X=i\}=e^{-\lambda}\frac{\lambda^i}{i!} \ \ \ i=0,1,2...
    \tag{3.7}
\end{equation}\]</span></p>
<p>A equação <a href="dicio.html#eq:poisson">(3.7)</a> define uma função de proababilidade, pois:</p>
<p><span class="math display">\[\displaystyle \sum^\infty_{i=0}p(i)=e^{-\lambda}\sum^\infty_{i=0} \frac{\lambda^i}{i!} = e^{-\lambda}e^\lambda=1\]</span>
que pode ser aplicada como aproximação de uma variável aleatória binomial com parâmetros <span class="math inline">\((n,p)\)</span> com <span class="math inline">\(n\)</span> grande e <span class="math inline">\(p\)</span> suficientemente pequeno para que <span class="math inline">\(np\)</span> tenha tamanho moderado e muito utilizado para dados de contagem, na qual a média é igual à variância <span class="citation">(Banzatto and Kronka <a href="#ref-banzatto1992experimentaccao">1992</a>)</span>.</p>
<ol start="8" style="list-style-type: decimal">
<li><strong>Teorema de Bayes</strong></li>
</ol>
<p>Quando tratamos de probabilidades, <span class="math inline">\(P(A|B)\)</span> e <span class="math inline">\(P(B|A)\)</span> podem ser parecidos, mas possuem grande diferença entre as probabilidades que representam. Por exemplo <span class="math inline">\(P(A|B)\)</span> pode se referir sobre a probabilidade de uma pessoa que cometeu um furto (B) ser condenada (A) e <span class="math inline">\(P(B|A)\)</span> seria a probabilidade de uma pessoa que foi condenada por furto ter efetivamente cometido um crime. A causa se torna o efeito e o efeito se torna a causa <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.</p>
<p>Pela regra geral de multiplicação que afirma que a probabilidade da ocorrência de dois eventos é o produto da probibilidade da ocorrência de um deles pela probabilidade condicional da ocorrência do outro evento, temos:</p>
<p><span class="math display" id="eq:multprob">\[\begin{equation} 
 P(A \cap B)= P(A). P(B|A) \  \mbox{ou} \ P(A \cap B)= P(B). P(A|B)
  \tag{3.8}
\end{equation}\]</span></p>
<p>Igualando ambas expressões, temos: <span class="math inline">\(P(A). P(B|A) = P(B). P(A|B)\)</span> e portanto, divindo por <span class="math inline">\(P(B)\)</span>, obtém-se o Teorema de Bayes que descreve a probabilidade de um evento, baseado em um conhecimento a <em>priori</em> que pode estar relacionado ao evento:</p>
<p><span class="math display" id="eq:bayes">\[\begin{equation} 
 P(A|B) = \frac{P(A).P(B|A)}{P(B)}
  \tag{3.9}
\end{equation}\]</span></p>
<p>Para <span class="math inline">\(B_n\)</span> e <span class="math inline">\(A_k\)</span> atributos, podemos reescrever:</p>
<p><span class="math display" id="eq:bayesn">\[\begin{equation} 
 P(A_k|B_1,...,B_n) = \frac{P(A_k).P(B_1,...,B_n|A_k)}{P(B_1,...,B_n)}
  \tag{3.10}
\end{equation}\]</span></p>
<ul>
<li><strong>Exemplo</strong> <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>:. Numa certa empresa, 4% dos homens e 1% das mulheres têm mais de 1,75m
de altura, respectivamente, sendo que 60% dos trabalhadores são mulheres. Um trabalhador é escolhido ao acaso.</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li>Qual a probabilidade de que tenha mais de 1,75m?</li>
</ol>
<p>Temos de informação de que 60% dos trabalhadores são mulheres e que 1% delas possuem mais de 1,75m. Portanto 40% dos trabalhadores são homens, sendo 4% deles com mais de 1,75m. Logo temos que:
<span class="math display">\[P(&gt; 1, 75m) = (0, 04 . 0.4) + (0, 01 . 0.6) = 0, 022\]</span>
<span class="math display">\[\rightarrow 2, 2\% \ \mbox{ de probabilidade de que tenha mais de 1,75m.}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>E que seja homem dado que o trabalhador escolhido tenha mais de 1,75m?</li>
</ol>
<p>Pelo enunciado “que seja homem dado que o trabalhador escolhido tenha mais de 1,75m”, podemos perceber que já possuímos uma afirmação que já foi escolhido uma pessoa que tenha mais que 1,75m e queremos saber se é homem. Por meio da questão anterior sabemos a probabilidade P(&gt; 1,75m). Portanto:
<span class="math display">\[P(H| &gt; 1, 75m) = \frac{P(&gt; 1, 75m|H).P(H)}{P(&gt; 1, 75m)}=\frac{0,04.0,4}{0, 022} \]</span></p>
<p><span class="math display">\[→ 72,73\% \ \mbox{de prob. de ser homem dado que seja maior que 1,75m.}\]</span></p>
<ol start="9" style="list-style-type: decimal">
<li><strong>Assimetria e Curtose</strong></li>
</ol>
<p>Como visto anteriormente, uma distribuição simétrica perfeitamente denominada como <strong>distribuição normal</strong> possui a seguinte forma:</p>
<div class="figure" style="text-align: center"><span id="fig:simetria"></span>
<img src="Figuras/simetria.PNG" alt="Distribuição perfeitamente simétrica (Freund 2009)." width="80%" />
<p class="caption">
Figura 3.3: Distribuição perfeitamente simétrica <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.
</p>
</div>

<p>Quando as distribuições apresentam uma ‘’cauda’’ em uma das extremidades, são denominadas <strong>assimétricas</strong>, quando apresenta na esquerda, dizemos <strong>negativamente assimétricas</strong> , na direita são <strong>positivamente assimétricas</strong>.</p>
<p>Estes conceitos aplicam-se a qualquer tipo de dados. Para conjuntos de dados grandes, pode-se agrupar e esboçar um histograma, mas caso não for suficiente, usa-se por meio de <strong>medidas de assimetria</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:assimetria"></span>
<img src="Figuras/assimetria.png" alt="Distribuição positivamente assimétrica (a) e negativamente assimétrica (b) respectivamente (Freund 2009)." width="80%" />
<p class="caption">
Figura 3.4: Distribuição positivamente assimétrica (a) e negativamente assimétrica (b) respectivamente <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.
</p>
</div>

<p>Dentre os métodos para o cálculo do coeficiente de assimetria, destaca-se o coeficiente de Pearson, que será apresentado mais a frente.</p>
<p><span class="math display" id="eq:coefassimetria">\[\begin{equation}
    SK=\frac{3(\overline{m}-q_2)}{s} \quad \quad SK=\frac{\overline{m}-Mo}{s}
     \tag{3.11}
\end{equation}\]</span></p>
<p>em <span class="math inline">\(SK\)</span> é o coeficiente de assimetria, <span class="math inline">\(\overline{m}\)</span> a média, <span class="math inline">\(q_2\)</span> a mediana, <span class="math inline">\(Mo\)</span> a moda e <span class="math inline">\(s\)</span> seu desvio padrão. A média e a mediana coincidem quando <span class="math inline">\(SK=0\)</span>.</p>
<p>Quanto ao grau de achatamento da curva são denominadas <strong>platicúrticas, mesocúrticas e leptocúrticas</strong> para as caudas curtas, neutras e longas respectivamente.</p>
<div class="figure" style="text-align: center"><span id="fig:curtose"></span>
<img src="Figuras/curtose.PNG" alt="Distribuições com formatos leptocúrtica (a), mesocúrtica (b) e platicúrtica (c) respectivamente (Freund 2009)." width="80%" />
<p class="caption">
Figura 3.5: Distribuições com formatos leptocúrtica (a), mesocúrtica (b) e platicúrtica (c) respectivamente <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.
</p>
</div>

<p>E, para medir o grau de curtose, pode-se utilizar o coeficiente de curtose:</p>
<p><span class="math display" id="eq:curtose">\[\begin{equation}
    K=\frac{q_3-q_1}{2(P_{90}-P_{10})}
    \tag{3.12}
\end{equation}\]</span></p>
<p>onde <span class="math inline">\(K\)</span> é o coeficiente de curtose, <span class="math inline">\(q_3\)</span> o terceiro quartil, <span class="math inline">\(q_1\)</span> o primeiro quartil, <span class="math inline">\(P_{90}\)</span> e <span class="math inline">\(P_{10}\)</span> são os percentis 90 e 10 respectivamente.</p>
<ol start="10" style="list-style-type: decimal">
<li><strong>Função de Verossimilhança</strong></li>
</ol>
<p>A verossimilhança <span class="math inline">\(L\)</span> de um conjunto de parâmetros <span class="math inline">\(\theta\)</span>, com dada informação <span class="math inline">\(x\)</span>. É igual a probabilidade da mesma observação <span class="math inline">\(x\)</span> ter ocorrido dados os valores dos mesmos parâmetros <span class="math inline">\(\theta\)</span>. Conhecendo um parâmetro <span class="math inline">\(\theta\)</span>, a probabilidade condicional de <span class="math inline">\(x\)</span> é <span class="math inline">\(P(x|\theta)\)</span>, mas se o valor de <span class="math inline">\(x\)</span> é conhecido, pode-se realizar inferências sobre o valor de <span class="math inline">\(\theta\)</span> <span class="citation">(Bolfarine and Sandoval <a href="#ref-bolfarine2001introduccao">2001</a>)</span>.</p>
<p><span class="math display" id="eq:fverossimilhanca">\[\begin{equation} 
 L(\theta |x)=P(x| \theta)
  \tag{3.13}
\end{equation}\]</span></p>
<p>Para “<span class="math inline">\(n\)</span>” valores:</p>
<p><span class="math display" id="eq:fsumverossimilhanca">\[\begin{equation} 
 L(\theta |x_1,..., x_n)=\prod_{i=1}^{n} P(x_i| \theta)
  \tag{3.14}
\end{equation}\]</span></p>
<p>Geralmente utiliza-se o logaritmo natural em verossimilhança <span class="math inline">\(L(\theta |x)=ln L(\theta|x)\)</span> como função suporte e facilitar em seu estudo.</p>
<p>Para facilitar a compreensão, considere a observação de que você esteja ouvindo barulho em sua sala de estar num dia de natal (observação <span class="math inline">\(x\)</span>), você parte da hipótese inicial que poderia ser o “Papai Noel” lhe entregando presentes (hipótese <span class="math inline">\(\theta\)</span>). A probabilidade de ser Noel lhe entregando presente apenas porque ouviu o barulho, isto é, <span class="math inline">\(P(\theta|x)\)</span> é baixa. No entanto o contrário, você com a afirmação de que é o Noel lhe entregando presentes, a probabilidade de haver barulho em sua sala de estar é bem alta, logo a verossimilhança <span class="math inline">\(L(\theta|x)=P(x|\theta)\)</span>.</p>

<ol start="11" style="list-style-type: decimal">
<li><strong>Teorema do Limite Central</strong></li>
</ol>
<p>Quando é utilizado a média amostral para estimar a média de uma população, ocorre-se incertezas em relação ao erro. O Teorema do Limite Central é um teorema fundamental para a estatísticas e faz com que possa ser aplicado independente da forma da distribuição da população. Ele diz que se <span class="math inline">\(\overline{x}\)</span> é a média de uma amostra aleatória de tamanho <span class="math inline">\(n\)</span> de uma população infinita com a média <span class="math inline">\(\mu\)</span> e o desvio-padrão <span class="math inline">\(\sigma\)</span> e se <span class="math inline">\(n\)</span> é grande o suficiente (em geral <span class="math inline">\(n=30\)</span>), então possui próximo a distribuição normal padrão:</p>
<p><span class="math display" id="eq:teoremacentralimite">\[\begin{equation} 
 z=\frac{\overline{x}-\mu}{\sigma / \sqrt{n}}
  \tag{3.15}
\end{equation}\]</span></p>
<p>Este teorema também pode ser utilizado para populações finitas, mas não é comum e são poucas situações de que haja esta possibilidade. A utilização mais comum é quando <span class="math inline">\(n\)</span> é grande enquanto <span class="math inline">\(\frac{n}{N}\)</span> pequeno.</p>
<ul>
<li><strong>Exemplo</strong> <span class="citation">(Farias <a href="#ref-fariaestatistic">2010</a>)</span>: O fabricante de uma lâmpada especial afirma que o seu produto tem vida média de 1.600 hors, com desvio padrão de 250 horas. O dono de uma empresa compra 100 lâmpadas desse fabricante. Qual é a probabilidade de que a vida média dessas lâmpadas ultrapasse 1.650?</li>
</ul>
<p>Podemos aceitar que as 250 lâmpadas compradas sejam uma amostra aleatória simples da população referente às lâmpadas produzidas por esse fabricante. Como <span class="math inline">\(n=100\)</span> é um tamanho suficientemente grande de amostra, é possível utilizarmos o Teorema Central do Limite e entender que <span class="math inline">\(X=\)</span>vida útil de uma lâmpada se aproxima da distribuição normal <span class="math inline">\(\overline{X}\approx N(\mu;\frac{\sigma^2}{n})\)</span>. Logo:
<span class="math display">\[\overline{X}\approx N(1600;\frac{250^2}{100})\]</span>
<span class="math display">\[Pr(\overline{X}&gt;1650)=Pr\bigg( \frac{\overline{X}-1600}{\sqrt{\frac{250^2}{100}}}&gt;\frac{1650-1600}{\sqrt{\frac{250^2}{100}}} \bigg)\]</span>
<span class="math display">\[=Pr(Z&gt;2,0)\]</span>
<span class="math display">\[=0,5-tab(2,0)\]</span>
<span class="math display">\[=0,5-0,47725=0,02275\]</span>
A probabilidade de que a vida média dessas lâmpadas ultrapasse 1.650 é de 2,275%.</p>
<ol start="12" style="list-style-type: decimal">
<li><strong>Testes de Hipóteses</strong></li>
</ol>
<blockquote>
<p>Uma hipótese estatística é uma afirmação ou conjectura sobre um parâmetro, ou parâmetros, de uma população (ou populações); pode também se referir ao tipo, ou natureza, da população (ou populações).</p>
<p>— <span class="citation">Freund (<a href="#ref-freund2009estatistica">2009</a>)</span>.</p>
</blockquote>
<p>O Teste de Hipóteses é um procedimento estatístico que nos permite rejeitar ou não rejeitar uma hipótese estatística por meio dos dados observados de uma amostra. Para desenvolver os processos de testes de hipóteses estatísticas precisamos saber precisamente o que esperar quando uma hipótese é verdadeira, por isso em geral formula-se a hipótese contrária àquela que queremos provar. Supondo que estamos desconfiados em um jogo que seus dados não são honestos, ao formularmos a hipótese de que esses dados são viciados, dependeria do quão viciados eles são. Porém, ao supor que eles são perfeitamente equilibrados, poderíamos calcular todas as probabilidades necessárias para concluirmos a hipótese. Se pretendermos verificar que a análise de um analista de dados é mais eficiente do que o outro, iremos formular a hipótese de que ambos são igualmente eficientes. Se a durabilidade de uma camisa feita por algodão é maior que uma camisa feita por políester, formularemos a hipótese de que ambas possuem as durabilidades iguais. A hipótese de não haver diferença (hipóteses iguais) denominamos como <strong>hipóteses nulas (<span class="math inline">\(H_0\)</span>)</strong>, utilizada para qualquer hipótese estabelecida prioritariamente para ver se ela pode ser rejeitada. A hipótese que aceitamos quando rejeitamos a nula, é chamada de <strong>hipótese alternativa (<span class="math inline">\(H_A\)</span>)</strong>. Vamos a um exemplo.</p>
<ul>
<li><strong>Exemplo</strong> <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>: um psicólogo pretende determinar se o tempo médio de reação de um adulto a um estímulo visual é realmente de 0,38 segundos. Sua hipótese nula
<span class="math display">\[H_0: \mu=0,38\ \mbox{segundos}\]</span>
contra a hipótese alternativa
<span class="math display">\[H_A: \mu \neq 0,38\ \mbox{segundos}\]</span>
em que <span class="math inline">\(\mu\)</span> é o tempo médio de reação de um adulto ao estímulo visual. Para realizar o teste, o psicólogo decide tomar uma amostra aleatória de <span class="math inline">\(n=40\)</span> adultos com objetivo de aceitar a hipótese nula se a média da amostra cair em algum ponto entre 0,36 e 0,40 segundos; do contrário a hipótese será rejeitada. Como a decisão se baseia em uma amostra, existe a possibilidade de a média amostral ser menor do que 0,36 segundos ou maior que 0,40 segundos mesmo se a verdadeira média amostral ser 0,38 segundos. Da mesma forma é possível que a média amostral esteja entre os intervalos de 0,36 e 0,40 segundos mesmo que a verdadeira média possua, por exemplo, 0,41 segundos. Portanto, é importante investigar a probabilidade de que o teste nos leve a uma decisão errada.</li>
</ul>
<p>Vamos supor que o desvio padrão seja <span class="math inline">\(\sigma=0,08\)</span> segundos para estes dados e investiguemos a possibilidade de rejeitar falsamente a hipótese nula. Iremos supor que o verdadeiro tempo médio de reação seja 0,38 segundos, então encontramos a probabilidade de que a média amostral vá ser menor ou igual a 36 segundos ou maior igual a 40. A probabilidade de que isso ocorra é dada pela somas das áreas das duas regiões coloridas apresentadas na Figura <a href="dicio.html#fig:testehip1">3.6</a> a seguir, e pode ser determinada pela distribuição amostral da média por uma distribuição normal.</p>
<p>Supondo que a população amostrada possa ser tratada como sendo infinita, a média da distribuição amostral é dado por:</p>
<p><span class="math display">\[\sigma_{\overline{x}}=\frac{\sigma}{\sqrt{n}}=\frac{0,08}{\sqrt{40}}\approx 0,0126\]</span>
A linha de divisória em unidades padronizadas, são:
<span class="math display">\[z=\frac{0,36-0,38}{0,0126}\approx -1,59 \ \mbox{e} \ z=\frac{0,40-0,38}{0,0126}\approx 1,59\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:testehip1"></span>
<img src="Figuras/testehip1.png" alt="Critério de teste e distribuição amostral de \(\overline{x}\) com \(\mu =0,38\) segundos (Freund 2009)." width="70%" />
<p class="caption">
Figura 3.6: Critério de teste e distribuição amostral de <span class="math inline">\(\overline{x}\)</span> com <span class="math inline">\(\mu =0,38\)</span> segundos <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.
</p>
</div>

<p>Por meio da Tabela Z (XXXXXXXXXXX) observamos que a área da cauda da distribuição amostral da será <span class="math inline">\(0,50000-0,4441=0,0559\)</span>. Portanto a probabilidade de obter um valor em uma ou em outra cauda da distribuição será de <span class="math inline">\(2(0,0559)=0,1118\)</span>.</p>
<p>Vamos agora com a possibilidade de que o teste deixa de detectar que a hipótese nula é falsa, ou seja <span class="math inline">\(\mu \neq 0,38\)</span> segundos. Portanto iremos supor que o verdadeiro tempo médio de reação seja de 0,41 segundos. Obtendo uma média amostral no intervalo de 36 a 40 segundos levaria à aceitação errônea da hipótese nula de <span class="math inline">\(\mu=0,38\)</span> segundos. Portanto a média da distribuição amostral será:</p>
<p><span class="math display">\[\sigma_{\overline{x}}=\frac{\sigma}{\sqrt{n}}=\frac{0,08}{\sqrt{40}}\approx 0,0126\]</span>
As linhas divisórias em unidades padronizadas, são:
<span class="math display">\[z=\frac{0,36-0,41}{0,0126}\approx -3,77 \ \mbox{e} \ z=\frac{0,40-0,41}{0,0126}\approx -0,79\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:testehip2"></span>
<img src="Figuras/testehip2.png" alt="Critério de teste e distribuição amostral de \(\overline{x}\) com \(\mu =0,41\) segundos (Freund 2009)." width="80%" />
<p class="caption">
Figura 3.7: Critério de teste e distribuição amostral de <span class="math inline">\(\overline{x}\)</span> com <span class="math inline">\(\mu =0,41\)</span> segundos <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.
</p>
</div>

<p>Por fim cabe ao psicólogo decidir qual risco é aceitável: a probabildiade de 0,11 de rejeitar erroneamente a hipótese nula de <span class="math inline">\(\mu = 0,38\)</span> ou a probabilidade 0,21 de erroneamente aceitá-la quando na realidade é <span class="math inline">\(0,41\)</span>.</p>
<p>Portanto resume-se em:</p>
<table>
<caption><span id="tab:tabelahipotese">Tabela 3.1: </span> Resumo de uma situação típica dos testes de hipóteses.</caption>
<thead>
<tr class="header">
<th></th>
<th><strong>Aceitar <span class="math inline">\(H_0\)</span></strong></th>
<th><strong>Rejeitar <span class="math inline">\(H_0\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong><span class="math inline">\(H_0\)</span> é verdadeiro</strong></td>
<td>Decisão Correta</td>
<td>Erro tipo I</td>
</tr>
<tr class="even">
<td><strong><span class="math inline">\(H_0\)</span> é falso</strong></td>
<td>Erro tipo II</td>
<td>Decisão Correta</td>
</tr>
</tbody>
</table>
<p>Se a hipótese nula <span class="math inline">\(H_0\)</span> é verdadeira e aceita ou falsa e rejeitada, a decisão é correta em ambos casos; se é verdadeira e rejeitada ou falsa aceita. O erro tipo I e a probabilidade de obtê-lo é ocorrida pela letra grega <span class="math inline">\(\alpha\)</span> e o erro tipo II pelo <span class="math inline">\(\beta\)</span>. Portanto pelo exemplo, temos que <span class="math inline">\(\alpha=0,11\)</span> e <span class="math inline">\(\beta=0,21\)</span> quando <span class="math inline">\(\mu=0,41\)</span> e o psicólogo deve decidir se aceita ou rejeita a hipótese nula de <span class="math inline">\(\mu=0,38\)</span></p>
<ol start="13" style="list-style-type: decimal">
<li><strong>Região Crítica e Nível de Significância</strong></li>
</ol>
<p>No geral definimos uma região crítica (RC) como o conjunto de valores no qual a probabilidade de ocorrência é pequena sob a hipótese de ser verdade o <span class="math inline">\(H_0\)</span>. Por exemplo: lançada 30 vezes uma moeda, sendo obtida num total de 28 caras. Claramente iremos desconfiar que é uma moeda honesta, visto que a probabilidade de ser obtida 28 caras em 30 lançamentos de uma moeda honesta é de 0,000000433996. Mesmo que haja essa mínima possibilidade de que a moeda honesta acerte este evento, pela perspectiva do teste de hipóteses, a obtenção de tal evento será uma evidência de que a nossa hipótese nula de honestidade da moeda não é muito plausível. Assim não dizemos que a moeda não é honesta, concluímos que não há evidência suficiente para apoiar a hipótese nula <span class="citation">(Farias <a href="#ref-fariaestatistic">2010</a>)</span>.</p>
<p>A definição dessa pequena probabilidade se faz por meio da escolha do <strong>nível de significância <span class="math inline">\(\alpha\)</span></strong> do teste, expressa como:</p>
<p><span class="math display" id="eq:nivelsignificancia">\[\begin{equation} 
 \alpha=Pr(\mbox{Erro tipo I})=Pr(\mbox{rejeitar} \ H_0|H_0 \ \mbox{é verdadeira})
  \tag{3.16}
\end{equation}\]</span></p>
<p>Geralmente é utilizado em <span class="math inline">\(\alpha=0,05\)</span>, <span class="math inline">\(\alpha=0,01\)</span> ou <span class="math inline">\(\alpha=0,10\)</span> como nível de significância, com isso torna-se possível estabelecer a região crítica usando a distribuição amostral da estatística de teste <span class="citation">(Farias <a href="#ref-fariaestatistic">2010</a>)</span>.</p>
<p>De <span class="citation">Farias (<a href="#ref-fariaestatistic">2010</a>)</span>, segue:</p>
<ul>
<li><strong>Exemplo:</strong> Considere uma população representada por uma variável aleatória normal com média <span class="math inline">\(\mu\)</span> e variância 400. Queremos testar:</li>
</ul>
<p><span class="math display">\[H_0:\mu = 100\]</span>
<span class="math display">\[H_A:\mu \neq 100\]</span>
Com base em uma amostra aleatória simples de tamanho <span class="math inline">\(n=16\)</span>. Para tal define-se a região crítica como RC: <span class="math inline">\(\overline{X}&lt;85\)</span> ou <span class="math inline">\(\overline{X}&gt;115\)</span>. Qual é a probabilidade do erro tipo I?</p>
<p><span class="math display">\[\alpha=Pr(\mbox{Erro tipo I})=Pr(\mbox{rejeitar} \ H_0|H_0 \ \mbox{é verdadeira})\]</span>
<span class="math display">\[=Pr[\{\overline{X}&lt;85\} \cup \{\overline{X}&gt;115\} \ | \ \overline{X}\sim N(100;\frac{400}{16}=25)]\]</span>
<span class="math display">\[=Pr[\overline{X}&lt;85 \ | \  \overline{X}\sim N(100;25)]+Pr[\overline{X}&gt;115 \ | \ \overline{X}\sim N(100;25)]\]</span>
<span class="math display">\[=Pr\bigg( Z&lt;\frac{85-100}{5} \bigg)+Pr\bigg( z&gt;\frac{115-100}{5} \bigg)\]</span>
<span class="math display">\[=Pr(Z&gt;-3)+Pr(Z&gt;3) \rightarrow 2.Pr(Z&gt;3)\]</span>
<span class="math display">\[\alpha=0,0027\]</span></p>
<ol start="14" style="list-style-type: decimal">
<li><strong>Aplicações do Teste de Hipóteses - Média com a Variância Conhecida</strong></li>
</ol>
<p>Ao caso de interessarmos na média de uma população normal e supondo que a variância seja conhecida. Pelo teste temos que:</p>
<p><span class="math display" id="eq:testehipmedia">\[\begin{equation} 
z=\frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}
  \tag{3.17}
\end{equation}\]</span></p>
<p>onde <span class="math inline">\(\mu_o\)</span> é o valor da média que ocorre sobre a hipótese nula. Trabalhar com unidades padronizadas <span class="math inline">\(z\)</span> nos permitem formular vários critérios que se aplicam a muitos problemas diferentes. Lembrando que são amostras suficientemente grandes para que a distribuição amostral da média possa ser próxima por uma distribuição normal padrão.</p>
<ul>
<li><strong>Exemplo</strong> <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>: Uma oceanógrafa com base numa amostra aleatória de tamanho <span class="math inline">\(n=35\)</span> e ao nível 0,05 de significância, quer testar se a profundidade média do oceano numa determinada área é de 72,4 metros conforme registrado. O que ela decidirá se obtiver <span class="math inline">\(\overline{x}=73,2\)</span> metros e se puder supor, usando informações de estudos anteriores análogos, que <span class="math inline">\(\sigma=2,1\)</span> metros?</li>
</ul>
<p><span class="math display">\[H_0: \mu=72,4 \mbox{metros}\]</span>
<span class="math display">\[H_A: \mu \neq 72,4 \mbox{metros}\]</span>
Temos que <span class="math inline">\(\alpha=0,05\)</span>, ou seja, pela tabela de distribuição normal bilateral AQUI PONHO A TABELA ANEXADA teríamos 0,025 para cada lado e portanto, ciente de que cada lado da curva equivale a 0,5 (ou 50%) e subtraindo os 0,025, obtemos <span class="math inline">\(0,475\)</span>. Pela tabela verificamos então que iremos rejeitar a hipótese nula se <span class="math inline">\(Z \leq -1,96\)</span> ou <span class="math inline">\(z \geq 1,96\)</span>. Logo:
<span class="math display">\[z=\frac{73,2-72,4}{2,1/\sqrt{35}}\approx 2,25\]</span>
Como <span class="math inline">\(z-2,25\)</span> pertence a região critíca, então a hipótese nula deve ser rejeitada, a diferença entre <span class="math inline">\(\overline{x}=73,2\)</span> e <span class="math inline">\(\mu=72,4\)</span> é significante. Note que se a oceanógrafa tivesse utilizado o nível de 0,01 de significância nesse exemplo, ela não poderia ter rejeitado a hipótese nula. Pelo <strong>valor <span class="math inline">\(p\)</span> (probabilidade de cauda)</strong> que é muito utilizado atualmente como medida de significância e é dado pela área total sob a curvada esquerda de <span class="math inline">\(Z=-2,25\)</span> e da direita <span class="math inline">\(z=2,25\)</span>, observando a tabela temos que <span class="math inline">\(2(0,5000-0,4878)=0,0244\)</span>, poderíamos rejeitar a hipótese nula ao nível de 0,0244 de significância.</p>
<ol start="15" style="list-style-type: decimal">
<li><strong>Aplicações do Teste de Hipóteses - Média com a variância desconhecida</strong></li>
</ol>
<p>Vamos supor que não sabemos o valor da variância e agora a um caso de <span class="math inline">\(n\)</span> não suficientemente grande, ou seja, utilizamos o teste <span class="math inline">\(t\)</span>:</p>
<p><span class="math display" id="eq:testehipvardesc">\[\begin{equation} 
t=\frac{\overline{X}-\mu_0}{S/\sqrt{n}}
  \tag{3.18}
\end{equation}\]</span></p>
<ul>
<li><strong>Exemplo</strong> <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>: A safra de alfafa de uma amostra aleatória de seis lotes de teste é dada por 1,4; 1,6; 0,9; 1,9; 2,2; e 1,2 tonelada por acre. Ao nível de 0,05 de significância, teste se isso corrobora a alegação de que a safra média para este tipo de alfafa é de 1,5 tonelada por acre.</li>
</ul>
<p><span class="math display">\[H_0: \mu=1,5\]</span>
<span class="math display">\[H_A: \mu\neq1,5\]</span>
Temos que <span class="math inline">\(\alpha=0,05\)</span>, temos que <span class="math inline">\(n=6\)</span> observações e portanto 6-1=5 graus de liberdade. Portanto pela tabela XXXXXXXXX de distribuição <span class="math inline">\(t\)</span> de <em>student</em> , em área na cauda superior de 0,025 e 5 G.L, rejeitaremos a hipótese nula se <span class="math inline">\(t\leq -2,75\)</span> ou <span class="math inline">\(t\geq 2,75\)</span>. Calculando a médio e o desvio-padrão dos dados, substituindo na expressão anterior, temos:
<span class="math display">\[t=\frac{1,533-1,5}{0,472/\sqrt{6}}\approx 0,171\]</span>
Portanto não podemos rejeitar a hipótese nula, ou seja, os dados tendem a apoiar a alegação de que a safra média para esse tipo de alfafa é de 1,5 tonelada por acre.</p>
<p>Existe diversos outros como o teste para duas médias com amostras independentes ou dependentes, com proporções, etc. Ao leitor que pretende se aprofundar nesse conteúdo sugiro buscar literaturas complementares no campo de estatística.</p>
<ol start="16" style="list-style-type: decimal">
<li><strong>Estatística F</strong></li>
</ol>
<p>Para a comparação de duas variâncias, utilizamos a estatística F como razão de variâncias. Sua hipótese nula será rejeitada se F for grande (variação entre <span class="math inline">\(\overline{x}\)</span> é muito grande para ser atribuído ao acaso). Utiliza-se a tabela XXXX (ao caso de 5% de significância) e XXXX (para 10% de significância) onde compara-se as médias de <span class="math inline">\(k\)</span> amostras aleatórias de tamanho <span class="math inline">\(n\)</span>, ou seja, os <strong>graus de liberdade do denominador e do numerador</strong> são respectivamente <span class="math inline">\(k-1\)</span> e <span class="math inline">\(k(n-1)\)</span>.</p>
<p><span class="math display" id="eq:testef">\[\begin{equation} 
F=\frac{\mbox{estimativa de }\sigma^2 \ \mbox{baseada na variação entre as }\overline{x}s}{\mbox{estimativa de }  \sigma^2 \ \mbox{baseada na variação dentro das amostras}}
  \tag{3.19}
\end{equation}\]</span></p>
<p>Em “Análise de Variância” haverá um exemplo aplicado.</p>
<ol start="17" style="list-style-type: decimal">
<li><strong>Análise de Variância</strong></li>
</ol>
<p>Também chamada de <strong>ANOVA</strong>, expressa uma medida da variação total num conjunto de dados como uma soma de termos, cada um dos quais é atribuído a uma fonte ou causa específica de variação <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>. Ao caso de apenas uma fonte de variação, dizemos <strong>análise de variância de um critério</strong>. Ela é utilizada para comparar a variabilidade entre as médias amostrais dos grupos e a variação dentro desses grupos.</p>
<p>Para medir a variação total de <span class="math inline">\(kn\)</span>, em que consiste de <span class="math inline">\(k\)</span> amostras com tamanho <span class="math inline">\(n\)</span>, é utilizado a <strong>soma de quadrados total</strong>:</p>
<p><span class="math display" id="eq:sqtanova">\[\begin{equation} 
STQ= \displaystyle \sum^k_{i=1} \displaystyle \sum^k_{n=1}(x_{ij}-\overline{x})^2
  \tag{3.20}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(x_{ij}\)</span> é a <span class="math inline">\(j\)</span>-ésima observação da <span class="math inline">\(i\)</span>-ésima amostra (<span class="math inline">\(i=1,2,...,k\)</span> e <span class="math inline">\(j=1,2,...,n\)</span>) e <span class="math inline">\(\overline{x}\)</span> é a média de todas as <span class="math inline">\(kn\)</span> observações. Caso dividirmos a STQ ´pr <span class="math inline">\(kn-1\)</span>, obteremos variância dos dados combinados <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>.</p>
<p>Ela também pode ser expressa pela seguinte identidade:</p>
<p><span class="math display" id="eq:sqtanova2">\[\begin{equation} 
STQ= n \displaystyle \sum^k_{i=1} (\overline{x}_i-\overline{x})^2+ \displaystyle \sum^k_{n=1}(x_{ij}-\overline{x}_i)^2
  \tag{3.21}
\end{equation}\]</span></p>
<p>onde <span class="math inline">\(n \displaystyle \sum^k_{i=1} (\overline{x}_i-\overline{x})^2\)</span> é a <strong>soma de quadrados de tratamentos SQ(Tr)</strong> que mede a variação entre as médias amostrais e <span class="math inline">\(\displaystyle \sum^k_{n=1}(x_{ij}-\overline{x}_i)^2\)</span> é a <strong>soma dos quadrados de erros SQE</strong> que mede a variação dentro das amostrais individuais. Em SQE, refere-se ao erro experimental, que nada mais é a diferença entre STQ e SQ(Tr).</p>
<p>A identidadade de ANOVA de um critério simplificada fica:</p>
<p><span class="math display" id="eq:sqtanova3">\[\begin{equation} 
STQ= SQ(Tr)+SQE
  \tag{3.22}
\end{equation}\]</span></p>
<p>Ao dividirmos a SQ(Tr) por <span class="math inline">\(k-1\)</span> obtemos a grandeza que utiliza-se na estatística F, que é conhecida como <strong>quadrado médio de tratamento</strong> que tem como objetivo medir a variação entre as médias amostrais.</p>
<p><span class="math display" id="eq:qmtr1">\[\begin{equation} 
QM(Tr)=\frac{SQ(Tr)}{k-1}
  \tag{3.23}
\end{equation}\]</span></p>
<p>Da mesma forma, ao dividir SQE por <span class="math inline">\(k(n-1)\)</span>, obtemos o <strong>quadrado médio de erro</strong> que mede a variação dentre das amostrais.</p>
<p><span class="math display" id="eq:qme1">\[\begin{equation} 
QME=\frac{SQE}{k(n-1)}
  \tag{3.24}
\end{equation}\]</span></p>
<p>A estatítistica F será observada pela tabela de acordo com os graus de liberdade dos tratamentos e do erro, portanto será:</p>
<p><span class="math display" id="eq:fanova">\[\begin{equation} 
F=\frac{QM(Tr)}{QME}
  \tag{3.25}
\end{equation}\]</span></p>
<p>Podemos apresentar todos os cálculos com a finalidade de determinar F com a <strong>tabela de análise de variância</strong>:</p>
<table>
<caption><span id="tab:anova1">Tabela 3.2: </span> Tabela de ANOVA de um critério.</caption>
<thead>
<tr class="header">
<th><strong>Fonte de Variação</strong></th>
<th><strong>Graus de Liberdade</strong></th>
<th><strong>Soma de Quadrados</strong></th>
<th><strong>Quadrado Médio</strong></th>
<th><strong>F</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tratamentos</td>
<td><span class="math inline">\(k-1\)</span></td>
<td><span class="math inline">\(SQ(Tr)\)</span></td>
<td><span class="math inline">\(QM(Tr)\)</span></td>
<td><span class="math inline">\(\frac{QM(Tr)}{QME}\)</span></td>
</tr>
<tr class="even">
<td>Erro</td>
<td><span class="math inline">\(k(n-1)\)</span></td>
<td><span class="math inline">\(SQE\)</span></td>
<td><span class="math inline">\(QME\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(kn-1\)</span></td>
<td><span class="math inline">\(STQ\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Calculado F, temos a suposição de que os dados são compostos de amostras de populações normais, seguindo a hipótese nula das médias serem iguais e como hipótese alternativa de que as médias <span class="math inline">\(\mu\)</span> não todas iguais.</p>
<p>Ao caso de houver dois critérios, ocorre o acréscimo de blocos e teremos então a <strong>soma de quadrados de blocos</strong>:</p>
<p><span class="math display" id="eq:sqblocoss">\[\begin{equation} 
SQB=\frac{1}{k}. \displaystyle \sum^n_{j=1} T^2_j-\frac{1}{kn}.T^2
  \tag{3.26}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(T_j\)</span> é o total de todos os valores do <em>j</em>-ésimo bloco.</p>
<p>A STQ portanto, será expressao como:</p>
<p><span class="math display" id="eq:sqtanova2crit">\[\begin{equation} 
STQ=SQ(Tr)+SQB+SQE
  \tag{3.27}
\end{equation}\]</span></p>
<p>Da mesma forma, para a estatística F utilizará os graus de liberdade de acordo com os blocos e os erros.</p>
<p>Por fim, a <strong>tabela de ANOVA de dois critérios</strong>:</p>
<table>
<caption><span id="tab:anova2">Tabela 3.3: </span> Tabela de ANOVA de dois critérios.</caption>
<thead>
<tr class="header">
<th><strong>Fonte de Variação</strong></th>
<th><strong>Graus de Liberdade</strong></th>
<th><strong>Soma de Quadrados</strong></th>
<th><strong>Quadrado Médio</strong></th>
<th><strong>F</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tratamentos</td>
<td><span class="math inline">\(k-1\)</span></td>
<td><span class="math inline">\(SQ(Tr)\)</span></td>
<td><span class="math inline">\(QM(Tr)\)</span></td>
<td><span class="math inline">\(\frac{QM(Tr)}{QME}\)</span></td>
</tr>
<tr class="even">
<td>Blocos</td>
<td><span class="math inline">\(n-1\)</span></td>
<td>SQB</td>
<td><span class="math inline">\(QMB\)</span></td>
<td><span class="math inline">\(\frac{QMB}{QME}\)</span></td>
</tr>
<tr class="odd">
<td>Erro</td>
<td><span class="math inline">\(k(n-1)\)</span></td>
<td><span class="math inline">\(SQE\)</span></td>
<td><span class="math inline">\(QME\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Total</td>
<td><span class="math inline">\(kn-1\)</span></td>
<td><span class="math inline">\(STQ\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Os métodos discutidos anteriormente referem-se quando o tamanho das amostras são todos iguais, ao caso de tamanhos diferentes podemos expressar a soma dos quadrados por:</p>
<p><span class="math display" id="eq:stqdiferente">\[\begin{equation} 
STQ=\displaystyle \sum^k_{i=1} \displaystyle \sum^{n_i}_{j=1}x^2_{ij}-\frac{1}{N}T^2
  \tag{3.28}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:sqtrdiferente">\[\begin{equation} 
SQ(Tr)=\displaystyle \sum^k_{i=1}\frac{T_i^2}{n_i}-\frac{1}{N}.T^2
  \tag{3.29}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:sqediferente">\[\begin{equation} 
SQE=STQ-SQ(Tr)
  \tag{3.30}
\end{equation}\]</span></p>
<p>onde os graus de liberdade para o total será <span class="math inline">\(N-1\)</span> e para os tratamentos e o erro será respectivamente <span class="math inline">\(k-1\)</span> e <span class="math inline">\(N-k\)</span>.</p>
<ul>
<li><strong>Exemplo 1</strong> (adaptado de <span class="citation">(Freund <a href="#ref-freund2009estatistica">2009</a>)</span>): segue os dados das notas de alunos da oitava série de quatro escolas num teste de compreensão de leitura. Possui respectivamente médias baixa, típica e alta:</li>
</ul>
<table>
<thead>
<tr class="header">
<th></th>
<th>Média Baixa</th>
<th>Média Típica</th>
<th>Média Alta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Escola A</td>
<td>71</td>
<td>92</td>
<td>89</td>
</tr>
<tr class="even">
<td>Escola B</td>
<td>44</td>
<td>51</td>
<td>85</td>
</tr>
<tr class="odd">
<td>Escola C</td>
<td>50</td>
<td>64</td>
<td>72</td>
</tr>
<tr class="even">
<td>Escola D</td>
<td>67</td>
<td>81</td>
<td>86</td>
</tr>
</tbody>
</table>
<p>Supondo que os dados consistam em amostras independentes de populações normais, todas com o mesmo desvio-padrão teste, ao nível de 0,05 de significância, se as diferenças entre as médias obtidas para as quatro escolas (tratamentos) são significantes e as diferenças entre as médias obtidas para os três níveis também são significantes.</p>
<p>Lembrando que <span class="math inline">\(STQ=\displaystyle \sum^k_{i=1} \displaystyle \sum^{n_i}_{j=1}x^2_{ij}-\frac{1}{N}T^2\)</span> temos:</p>
<p><span class="math display">\[STQ=71^2+92^2+...+81^2+86^2-\frac{(71+92+...+81+86)^2}{12}=2922\]</span>
Para o tratamentos temos que <span class="math inline">\(SQ(Tr)=\displaystyle \sum^k_{i=1}\frac{T_i^2}{n_i}-\frac{1}{N}.T^2\)</span>, com as escolas, temos:</p>
<p><span class="math display">\[SQ(Tr)=\frac{1}{3}(252^2+180^2+186^2+234^2)-60492=1260\]</span>
em que <span class="math inline">\(n_i=3\)</span> pois cada um dos quatro tratamentos (escolas) possui três elementos.</p>
<p>Para o caso do blocos, procedimento semelhante aos tratamentos porém por blocos (escolas), com três níveis de média com quatro elementos cada <span class="math inline">\((n_i=4)\)</span>, temos que:</p>
<p><span class="math display">\[SQB=\frac{1}{4}(232^2+288^2+332^2)-60492=1256\]</span>
Então, a soma dos quadrados do erro será:</p>
<p><span class="math display">\[SQE=2922-(1260+1256)=406\]</span>
Os graus de liberdade serão <span class="math inline">\(k-1=3\)</span> para os tratamentos, <span class="math inline">\(n-1=2\)</span> aos blocos, <span class="math inline">\((k-1)(n-1)=6\)</span> e <span class="math inline">\(kn-1=11\)</span>. Podemos obtermos por fim os quadrados médios de cada:</p>
<p><span class="math display">\[QM(Tr)=\frac{1260}{3}=420 \ \ QMB=\frac{1256}{2}=628 \ \ QME=\frac{406}{6}\approx 67,67\]</span>
E por fim a estatística F:
<span class="math display">\[F_{tratamentos}\approx \frac{420}{67,67}\approx 6,21 \ \ \ F_{blocos}\approx \frac{628}{67,67}\approx 9,28\]</span>
A tabela de ANOVA será:</p>
<table>
<thead>
<tr class="header">
<th>Fonte de variação</th>
<th>G.L</th>
<th>SQ</th>
<th>QM</th>
<th>F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tratamentos</td>
<td>3</td>
<td>1260</td>
<td>420</td>
<td>6,21</td>
</tr>
<tr class="even">
<td>Blocos</td>
<td>2</td>
<td>1256</td>
<td>628</td>
<td>9,28</td>
</tr>
<tr class="odd">
<td>Erro</td>
<td>6</td>
<td>406</td>
<td>67,67</td>
<td></td>
</tr>
<tr class="even">
<td>Total</td>
<td>11</td>
<td>2922</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Pela tabela XXXX, observa-se que em F(3;6;5%) é 4,757, temos que o valor F = 6,21 excede e conclui-se que a hipótese nula para os tratamentos deve ser rejeitada. Da mesma forma para os blocos temos que F(2;5;5%) é 5,143, seu valor de F=9,28 excedente e também pode rejeitar a hipótese nula. Portanto, conclui-se que o grau médio de compreensão de leitura dos alunos da oitava série não é o mesmo para as quatro escolas e que o grau médio de leitura de alunos da oitava série não é o mesmo para os níveis de nota média.</p>
<ol start="18" style="list-style-type: decimal">
<li><strong>Multiplicadores de Lagrange</strong></li>
</ol>
<p>O método dos Multiplicadores de Lagrange é utilizado para problemas de minimização com restrição em problemas sem restrição, através da inserção de um novo parâmetro - denominamos de Multiplicador de Lagrange.</p>
<p>Seja <span class="math inline">\(F:\mathbb{R}^n \rightarrow \mathbb{R}\)</span>. Diz-se que <span class="math inline">\(F^*:\mathbb{R}^n \rightarrow \mathbb{R}\)</span> é uma condição mínima necessária de <span class="math inline">\(F\)</span> com respeito a <span class="math inline">\(u \in \mathbb{R}^n\)</span> se <span class="math inline">\(F^*(u)\leq f(u)\)</span> <span class="citation">(Cardoso <a href="#ref-cardoso2014analise">2014</a>)</span>. A condição mínima necessária de <span class="math inline">\(F\)</span> com respeito a <span class="math inline">\(v=(v_1,v_2,...,v_n)\)</span> que satisfaz <span class="math inline">\(G_i(v)=C_i\)</span> em que <span class="math inline">\(i=1,2,...,n\)</span> e <span class="math inline">\(C_i\)</span> são constantes arbitrárias é <span class="citation">(Weinstock <a href="#ref-weinstock1974calculus">1974</a>)</span>:</p>
<p><span class="math display" id="eq:parcialfuncaolagrange">\[\begin{equation} 
\frac{\partial F^*}{\partial v_1}=\frac{\partial F^*}{\partial v_2}=...=\frac{\partial F^*}{\partial v_n}=0
  \tag{3.31}
\end{equation}\]</span></p>
<p>Tal que</p>
<p><span class="math display" id="eq:funcaomutlagrange">\[\begin{equation} 
F^*=F+\displaystyle \sum^n_{i=1}\lambda_i G_i
  \tag{3.32}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(\lambda_i\)</span> são os Multiplicadores de Lagrange.</p>
<p>A Função Lagrangiana é muito utilizada em problemas de otimização com restrição. Supondo uma função <span class="math inline">\(f\)</span> sujeita a uma função de restrição <span class="math inline">\(g=c\)</span> (constante arbitrária), podemos expressar:</p>
<p><span class="math display" id="eq:funcaolagrange">\[\begin{equation} 
\mathcal{L}(x_1,...,x_n,\lambda)=f(x_1,...,x_n)-\lambda(g(x_1,...,x_n)-c)
  \tag{3.33}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:lagrangepartialx">\[\begin{equation} 
\frac{\partial \mathcal{L}}{\partial x_i}=\frac{\partial f}{\partial x_i}-\lambda \frac{\partial g}{\partial x_i} \ \ \ \ i=1,2,...,n
  \tag{3.34}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:lagrangepartiallambda">\[\begin{equation} 
\frac{\partial \mathcal{L}}{\partial \lambda}=-(g(x_1,...,x_n)-c)
  \tag{3.35}
\end{equation}\]</span></p>
<p>Para <span class="math inline">\(x_{1_0},...,x_{n_0}\)</span> é um ponto extremo de <span class="math inline">\(f\)</span> com restrição da constante arbitrária e <span class="math inline">\(\lambda_0\)</span> sendo o Multiplicador de Lagrange. Podemos:</p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial x_i}=0 \ \ \ \ i=1,2,...,n\]</span>
<span class="math display">\[\frac{\partial \mathcal{L}}{\partial \lambda}=0\]</span>
E assim, <span class="math inline">\(x_{1_0},...,x_{n_0},\lambda_0\)</span> é um ponto crítico para a função Lagrangiana sem restrição <span class="math inline">\(\mathcal{L}(x_1,...,x_n,\lambda)\)</span> <span class="citation">(Cardoso <a href="#ref-cardoso2014analise">2014</a>)</span> e a Função Lagrangiana transforma um problema de otimização com restrição para um sem restrição.</p>
<ul>
<li><strong>Exemplo</strong> <span class="citation">(Tan <a href="#ref-tan2008">2008</a>)</span>: utilizando o método dos Multiplicadores de Lagrange, encontre o mínimo relativo da função
<span class="math display">\[f(x,y)=2x^2+y^2\]</span>
com a condição x+y=1.</li>
</ul>
<p>Podemos expressar a condição imposta na forma <span class="math inline">\(g(x,y)=x+y-1=0\)</span>. Desse modo, a função lagrangeana será dada por:</p>
<p><span class="math display">\[\mathcal{L}(x,y,\lambda)=f(x,y)+\lambda g(x,y)\]</span>
<span class="math display">\[=2x^2+y^2+\lambda(x+y-1)\]</span>
Ao aplicarmos a derivada parcial em relação a <span class="math inline">\(x\)</span>,<span class="math inline">\(y\)</span> e <span class="math inline">\(\lambda\)</span> a fim de identificar o(s) ponto(s) crítico(s), obtemos as seguintes equações:
<span class="math display">\[\mathcal{L}_x=4x+\lambda=0\]</span>
<span class="math display">\[\mathcal{L}_y=2y+\lambda=0\]</span>
<span class="math display">\[\mathcal{L}_\lambda=x+y-1=0\]</span>
Ao resolvermos as duas primeiras equações e deixarmos em função de <span class="math inline">\(\lambda\)</span>, obtemos:</p>
<p><span class="math display">\[x=- \frac{1}{4}\lambda \ \ \ \ y=- \frac{1}{2}\lambda\]</span>
Substituindo na terceira equação obtemos <span class="math inline">\(\lambda=-\frac{4}{3}\)</span> e portanto <span class="math inline">\(x=\frac{1}{3}\)</span> e <span class="math inline">\(y=\frac{2}{3}\)</span> e <span class="math inline">\((\frac{1}{3},\frac{2}{3})\)</span> é um mínimo restrito da função <span class="math inline">\(f\)</span>.</p>

</div>
<div id="medidasimport" class="section level2">
<h2><span class="header-section-number">3.3</span> Medidas de Importância</h2>
<blockquote>
<p>Um atributo é dito importante se quando removido a medida de importância considerada em relação aos atributos restantes é deteriorada , seja a precisão da medida, consistência, informação, distância ou dependência</p>
<p>Tradução de <span class="citation">Liu and Motoda (<a href="#ref-liu2012feature">2012</a>)</span>.</p>
</blockquote>
<p>É fundamental estimarmos a importância de um atributo, tanto uma avaliação individual quanto à avaliação de subconjuntos de atributos. É uma questão complexa e multidimensional <span class="citation">(Liu and Motoda <a href="#ref-liu2012feature">2012</a>)</span>. Podemos avaliar se os atributos selecionados pela etapa do pré-processamento auxiliam a melhorar a precisão do classificador ou a simplifcar algum modelo construído. A seguir, apresenta-se algumas medidas utilizadas <span class="citation">(Lee <a href="#ref-lee2005seleccao">2005</a>)</span>.</p>
<div id="medidasdep" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Medidas de Dependência</h3>
<p>Conhecidas como medidas de <strong>correlação</strong> ou <strong>associação</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:correlacao"></span>
<img src="Figuras/correlacao.png" alt="Padrões de correlação. Elaborado por Gujarati and Porter (2011) e adaptado Henri (1978)." width="70%" />
<p class="caption">
Figura 3.8: Padrões de correlação. Elaborado por <span class="citation">Gujarati and Porter (<a href="#ref-gujarati2011econometria">2011</a>)</span> e adaptado <span class="citation">Henri (<a href="#ref-theil1978">1978</a>)</span>.
</p>
</div>

</div>
<div id="medinfo" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Medidas de Informação</h3>
<p>As medidas de informação determinam o ganho de informação a partir de um atributo. O ganho de informação é definido como a diferença entre a incerteza a <em>priori</em> e a incerteza a <em>posteriori</em> considerando-se o atributo <span class="math inline">\(X_i\)</span>. <span class="math inline">\(X_i\)</span> é preferido ao atributo <span class="math inline">\(X_j\)</span> se seu ganho de informação for maior que de <span class="math inline">\(X_j\)</span>. Uma das mais utilizadas é a entropia que normalmente é usada na teoria da informação para medir a pureza ou impureza de um determinado conjunto.</p>
<p><span class="citation">Shannon (<a href="#ref-shannon1948mathematical">1948</a>)</span>, tomou como “ponto de partida” encontrar uma forma matemática de medir o quanto de informação existe na transmissão de uma mensagem de um ponto a outro, denominando-a entropia. Sua proposta baseava-se na ideia de que o aumento da probabilidade do próximo símbolo diminuiria o tamanho da informação. Com isso, a entropia pode ser definida como a quantidade de incerteza que há em uma mensagem e que diminui à medida que os símbolos são transmitidos (vai se conhecendo a mensagem), tendo-se então a informação, que pode ser vista como redução da incerteza <span class="citation">(Shannon <a href="#ref-shannon1948mathematical">1948</a>; Paviotti and Magossi <a href="#ref-paviotti2019consideraccoes">2019</a>)</span>. Por exemplo: ao utilizarmos como idioma a nossa língua portuguesa e ao transmitir como símbolo a letra “q”, a probabilidade do próximo símbolo ser a letra “u” é maior que a de ser qualquer outro símbolo, enquanto que a probabilidade de ser novamente a letra “q” é praticamente nula <span class="citation">(Paviotti and Magossi <a href="#ref-paviotti2019consideraccoes">2019</a>)</span>.</p>
<p>Shannon define que a entropia pode ser calculada por meio da soma das probabilidades de ocorrência de cada símbolo pela expressão <span class="math inline">\(∑ p_i = 1 = 100\%\)</span>, em que <span class="math inline">\(p_i\)</span> representa a probabilidade do i-ésimo símbolo que compõe a mensagem. Segundo ele, estes símbolos devem ser representados através de sequências binárias, utilizando das propostas de <span class="citation">Nyquist (<a href="#ref-nyquist1924certain">1924</a>)</span> e <span class="citation">Hartley (<a href="#ref-hartley1928transmission">1928</a>)</span>. Sua proposta consistia em representar símbolos de um alfabeto através de um logaritmo de acordo com suas respectivas unidades de informação. A entropia proposta por ele é obtida pela média das medidas de Hartley <span class="citation">(Moser and Chen <a href="#ref-moser2012student">2012</a>)</span>.</p>
<p>Se A é discreto com distribuição de probabilidade <span class="math inline">\(p(A)\)</span>, a entropia será:</p>
<p><span class="math display" id="eq:entropia">\[\begin{equation} 
  H(A)=- \sum p(A)log_2(p(A)) 
  \tag{3.36}
\end{equation}\]</span></p>
<p>Para facilitar a compreensão, vamos supor um exemplo de um questionário com resposta binária entre “sim” e “não”: quanto mais distribuído as probabilidades das respostas, mais desorganizada é, logo maior suaa entropia, do contrário caso for uma probabilidade de ser zero “sim”/“não” ou de ser 1 (100%), ou seja, ter apenas uma opção de resposta, será menos distribuído e portanto menor usa entropia.</p>
<div class="figure" style="text-align: center"><span id="fig:entropia"></span>
<img src="Figuras/entropia.jpg" alt="Gráfico de Probabilidade x Entropia." width="70%" />
<p class="caption">
Figura 3.9: Gráfico de Probabilidade x Entropia.
</p>
</div>

<p>O ganho de informação portanto mede a redução da entropia (nesse caso) causada pela partição dos exemplos de acordo com os valores do atributo.</p>
<p><span class="math display" id="eq:ganhodeinf">\[\begin{equation} 
  \mbox{Ganho de Informação}(D,T)=\mbox{entropia}(D)-\displaystyle \sum_{i=1}^k \frac{|D_i|}{|D|}. \mbox{entropia}(D_i) 
  \tag{3.37}
\end{equation}\]</span></p>
<p>É muito utilizado em algoritmo de <strong>Árvore de decisão</strong> que será apresentado na seção <a href="ptII.html#ptII">7</a> com um exemplo de seu uso.</p>
</div>
<div id="meddist" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Medidas de Similaridade e Dissimilaridade</h3>
<p>É provável que já tenha ouvido falar em algo como medidas de distância, separabilidade, discriminação, divergência, similaridade ou dissimilaridade. É uma questão importante decidir até que ponto dois elementos de um conjunto de dados podem ser considerados com caractéristicas semelhantes ou não.</p>
<p>Supondo que possuímos um conjunto de dados constituído por <span class="math inline">\(n\)</span> elementos amostrais. O objetivo é agrupar esses elementos em <span class="math inline">\(g\)</span> grupos de acordo com um vetor <span class="math inline">\(X_j=[X_{1j}X_{2j}...X_{pj}]&#39;, j=1,2,...,n\)</span> que representa o valor observado da variável <span class="math inline">\(i\)</span> medida no elemento <span class="math inline">\(j\)</span>. Lembrando que existem diversas medidas diferentes e que produzem um determinado tipo de agrupamento de acordo com sua metodologia. A seguir será apresentado algumas comuns no ramo.</p>
<div id="disteuclidana" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> Distância Euclidiana</h4>
<p>A distância Euclidiana entre dois elementos <span class="math inline">\(X_l\)</span> e <span class="math inline">\(X_k\)</span>, com <span class="math inline">\(l \neq k\)</span>, é definida por:</p>
<p><span class="math display" id="eq:euclidiana">\[\begin{equation} 
  d(X_l,X_k)=[(X_l-X_k)&#39;(X_l - X_k)]^{1/2}=[\displaystyle \sum^p_{i=1}(X_{il}-X_{ik})^2]^{1/2}
  \tag{3.38}
\end{equation}\]</span></p>
<p>sendo comparado os dois elementos amostrais em cada variável pertencente ao vetor. Por exemplo, a tabela a seguir apresenta a renda mensal (em salários mínimos) e a idade de seis indivíduos de uma localidade <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</p>
<table>
<caption><span id="tab:dadossrendaa">Tabela 3.4: </span> Renda e Idade de 6 indíviduos <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Indivíduo</strong></th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
<th align="center">D</th>
<th align="center">E</th>
<th align="center">F</th>
<th align="center">Média</th>
<th align="center">Desvio Padrão</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Renda</strong></td>
<td align="center">9,6</td>
<td align="center">8,4</td>
<td align="center">2,4</td>
<td align="center">18,2</td>
<td align="center">3,9</td>
<td align="center">6,4</td>
<td align="center">8,15</td>
<td align="center">5,61</td>
</tr>
<tr class="even">
<td align="center"><strong>Idade</strong></td>
<td align="center">28</td>
<td align="center">31</td>
<td align="center">42</td>
<td align="center">38</td>
<td align="center">25</td>
<td align="center">41</td>
<td align="center">34,17</td>
<td align="center">7,14</td>
</tr>
</tbody>
</table>
<p>A distância Euclidiana entre os indíviduos A e B nas variáveis Renda e Idade será:</p>
<p><span class="math display">\[d(X_A,X_B)=[(9,60-8,40)^2+(28-31)^2]^{1/2}=3,23\]</span>
Assim sucessivamente para cada uma das observações, obtemos a matriz:</p>
<p><span class="math display">\[D_{6x6}=\begin{bmatrix}\\
 &amp;A&amp;B&amp;C&amp;D&amp;E&amp;F \\
 A&amp;0&amp;&amp;&amp;&amp;&amp;\\
 B&amp;3,23&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 15,74&amp; 12,53&amp;0&amp;&amp;&amp;\\
 D&amp; 13,19&amp; 12,04&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,44&amp; 7,50&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 13,39&amp; 10,19&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Podendo agora analisar quais observações estão mais próximas entre si ou mais distantes de acordo com as características da Renda e da Idade.</p>
</div>
<div id="distponderada" class="section level4">
<h4><span class="header-section-number">3.3.3.2</span> Distância Ponderada</h4>
<p>Também conhecido como <strong>distância generalzada</strong>, esta metologia tem como base uma matriz <span class="math inline">\(A_{pxp}\)</span> de ponderação. Quando <span class="math inline">\(A_{pxp}\)</span> for uma matriz identidade, esta distância generalizada será a <strong>distância Euclidiana</strong>; se <span class="math inline">\(A_{pxp}\)</span> for a matriz inversa da matriz de covariâncias amostrais <span class="math inline">\(S^{-1}_{pxp}\)</span>, será a <strong>distância de <span class="citation">Mahalanobis (<a href="#ref-mahalanobis1936generalized">1936</a>)</span></strong> e se <span class="math inline">\(A_{pxp}=diag(\frac{1}{p})\)</span>, teremos a distância <strong>Euclidiana Média</strong>. A escolha dessa matriz <span class="math inline">\(A_{pxp}\)</span> reflete o tipo de informação que o pesquisador pretende utilizar na ponderação das diferenças das coordernadas dos vetores em estudo <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>, a distância de Mahalanobis, por exemplo, leva em consideração as possíveis diferenças de variâncias e as relações lineares entre as variáveis, em termos de variância, na ponderação.</p>
<p><span class="math display" id="eq:distpond">\[\begin{equation} 
  d(X_l,X_k)=[(X_l-X_k)&#39;A_{pxp}(X_l - X_k)]^{1/2}
  \tag{3.39}
\end{equation}\]</span></p>
<p>Continuando com dados da <a href="dicio.html#tab:dadossrendaa">3.4</a> e tomando como exemplo de ponderada pelo método de Mahalanobis. Ao calcular a matriz de covariância e sua respectiva inversa, obtemos:</p>
<p><span class="math display">\[S=
\begin{bmatrix}
31.471&amp; 2.15000 \\
2.150&amp; 50.96667 
\end{bmatrix}\]</span>
<span class="math display">\[S^{-1}=
\begin{bmatrix}
0,0032&amp; -0,0013 \\
-0,0013&amp; 0,019 
\end{bmatrix}\]</span></p>
<p>Portanto pelo cálculo da distância Ponderada por Mahalanobis:</p>
<p><span class="math display">\[d(X_A,X_B)=\bigg[(1,2 \ \ -3) S^{-1} \begin{pmatrix} 1,2 \\ -3  \end{pmatrix} \bigg]^{1/2}=0,46\]</span>
E sucessivamente calcula-se para às outras observações.</p>
</div>
</div>
<div id="medidas-de-precisão" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Medidas de Precisão</h3>
<p>Referente a tarefas de precisão, dado um algoritmo de aprendizado com sua amostra de dados. o algoritmo de maior desempenho preditivo ao modelo será selecionado <span class="citation">(Kohavi, John, and others <a href="#ref-kohavi1997wrappers">1997</a>)</span>. Não necessariamente precisa de um único subconjunto ótimo de atributos, pois é possível alcançar a mesma precisão com diferentes subconjuntos de atributos.</p>
<p>A <strong>Utilidade Incremental</strong> por exemplo <span class="citation">(Caruana and Freitag <a href="#ref-caruana1994useful">1994</a>)</span>, onde uma dada amostra de dados <span class="math inline">\(S\)</span>, com um determinado algoritmo de aprendizado e um subconjunto de atributos. Um atributo <span class="math inline">\(X_i\)</span> é incrementalmente útil para o modelo em relação ao subconjunto de dados <span class="math inline">\(F\)</span> se a precisão da hipótese produzida pelo modelo considerando o conjunto de atributos <span class="math inline">\(X_i \cup F\)</span> é melhor que a precisão alcançanda utilizando-se apenas o subconjunto <span class="math inline">\(F\)</span> <span class="citation">(Lee <a href="#ref-lee2005seleccao">2005</a>)</span>.</p>
<p>É muito comum seu uso em algoritmos de seleção de atributos que realizam a busca no espaço de subconjuntos de atributos, removendo e adicionando-os com abordagens como <em>wrapper</em> e <em>embedded</em> (apresentadas na seção seguinte). Importante lembrar que um atributo considerado importante não implica que o mesmo estará no subconjunto ótimo de atributos.</p>
</div>
<div id="medidas-de-consistência" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Medidas de consistência</h3>
<p>São medidas dependentes do conjunto de treinamento que permitem encontrar um subconjunto mínimo de atributos que satisfaz a proporção de inconsistência aceita (definida geralmente pelo pesquisador e com base alguma fundamentação teórica). O objetivo da análise por consistência é proporcionar a construção de hipóteses lógicas consistentes em um conjunto de treinamento. Note que elas não detectam a ocorrência de atributos redundantes, pois não possibilitam a distinção entre atributos igualmente adequados <span class="citation">(Parmezan et al. <a href="#ref-parmezan2012avaliaccao">2012</a>)</span>.</p>
<p>Um atributo <span class="math inline">\(X_i\)</span> é importante se aparece em toda fórmula <em>booleana</em> e do contrário não importante <span class="citation">(Almuallim and Dietterich <a href="#ref-almuallim1994learning">1994</a>; Lee <a href="#ref-lee2005seleccao">2005</a>)</span>, por exemplo:</p>
<p><span class="math display">\[X_1=1 \ \mbox{e} \ X_2=0 \ \mbox{então classe}=1\]</span>
<span class="math display">\[X_1=1 \ \mbox{e} \ X_3=0 \ \mbox{então classe}=1\]</span>
<span class="math display">\[X_1=0 \ \mbox{e} \ X_2=1 \ \mbox{então classe}=0\]</span>
Partindo dessa definição, <span class="math inline">\(X_1\)</span> é considerado importante pois é encontrado em todas as regras delimitadas, e portanto, <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_3\)</span> não são importantes.</p>
<p><span class="citation">Dash and Liu (<a href="#ref-dash2003consistency">2003</a>)</span> e <span class="citation">Liu, Setiono, and others (<a href="#ref-liu1996probabilistic">1996</a>)</span> definem como critério de avaliação que um subconjunto de atributos importantes é definido por meio de uma <strong>taxa de inconsistência</strong>.</p>
<ol style="list-style-type: decimal">
<li><p>Um exemplo será considerado <strong>inconsistente</strong> se existirem pelo menos dois exemplos exatamente iguais exceto pelo valor da classe;</p></li>
<li><p>A <strong>contagem de inconsistência</strong> é dada pelo número de vezes que este exemplo ocorre nos dados subtraído o maior número entre as diferentes classes;</p></li>
<li><p>a <strong>taxa de inconsistência</strong> de um subconjunto de atributos é a soma de todas as contagens de inconsistência de todos os exemplos do subconjunto nos dados dividido pelo número total de exemplos.</p></li>
</ol>
<p>Por exemplo: um exemplo <span class="math inline">\(E_i\)</span> inconsistente aparece <span class="math inline">\(N_{Ei}\)</span> vezes dos quais <span class="math inline">\(N_{C1}\)</span> pertencem à classe <span class="math inline">\(C_1\)</span>, <span class="math inline">\(N_{C2}\)</span> pertencem à classe <span class="math inline">\(C_2\)</span> e <span class="math inline">\(N_{C3}\)</span> pertencem à classe <span class="math inline">\(C_3\)</span>. Portanto <span class="math inline">\(N_{Ei}=N_{C1}+N_{C2}N_{C3}\)</span>. Supondo que <span class="math inline">\(N_{C3}\)</span> é o maior valor entre todos, a contagem de inconsistência é <span class="math inline">\(N_{Ei}-N_{C3}\)</span>. Com dado o subconjunto e um valor mínim da taxa delimitado pelo pesquisador, se a taxa de inconsistência for menor que o definido, poderá ser dito consistente <span class="citation">(Lee <a href="#ref-lee2005seleccao">2005</a>)</span>.</p>
<p>Existe muitos critérios de importância de atributos em muitas literaturas e torna dificultoso em identificar quais algoritmos e metodologias são mais apropriados para o conjunto de dados. Com as medidas de importância, torna-se possível avaliar se os atributos selecionados auxiliam no modelo proposto pelo pesquisador ou o oposto. Cabe ao pesquisador com base em literaturas verificar qual utilizador de acordo com suas preferências e conjunto de dados em análise.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-almuallim1994learning">
<p>Almuallim, Hussein, and Thomas G Dietterich. 1994. “Learning Boolean Concepts in the Presence of Many Irrelevant Features.” <em>Artificial Intelligence</em> 69 (1-2): 279–305.</p>
</div>
<div id="ref-banzatto1992experimentaccao">
<p>Banzatto, David Ariovaldo, and S do N Kronka. 1992. “Experimentação Agrı́cola.” <em>Jaboticabal: Funep</em> 2.</p>
</div>
<div id="ref-bolfarine2001introduccao">
<p>Bolfarine, Heleno, and Mônica Carneiro Sandoval. 2001. <em>Introdução à Inferência Estatı́stica</em>. Vol. 2. SBM.</p>
</div>
<div id="ref-cardoso2014analise">
<p>Cardoso, Onézimo Carlos Viana. 2014. “Análise Particionada de Turbinas Eólicas Offshore Utilizando O Método de Multiplicadores de Lagrange Localizados.”</p>
</div>
<div id="ref-caruana1994useful">
<p>Caruana, Rich, and Dayne Freitag. 1994. “How Useful Is Relevance?” <em>FOCUS</em> 14 (8): 2.</p>
</div>
<div id="ref-dash2003consistency">
<p>Dash, Manoranjan, and Huan Liu. 2003. “Consistency-Based Search in Feature Selection.” <em>Artificial Intelligence</em> 151 (1-2): 155–76.</p>
</div>
<div id="ref-fariaestatistic">
<p>Farias, Ana Maria Lima de. 2010. <em>Métodos Estatísticos Ii</em>. V. único. Rio de Janeiro, RJ: Fundação CECIERJ.</p>
</div>
<div id="ref-freund2009estatistica">
<p>Freund, John E. 2009. <em>Estatı́stica Aplicada-: Economia, Administração E Contabilidade</em>. Bookman Editora.</p>
</div>
<div id="ref-gujarati2011econometria">
<p>Gujarati, Damodar N, and Dawn C Porter. 2011. <em>Econometria Básica-5</em>. Amgh Editora.</p>
</div>
<div id="ref-hartley1928transmission">
<p>Hartley, Ralph VL. 1928. “Transmission of Information 1.” <em>Bell System Technical Journal</em> 7 (3): 535–63.</p>
</div>
<div id="ref-theil1978">
<p>Henri, Theil. 1978. <em>Introduction to Econometrics</em>. Englewood Cliffs, New Jersey: Prentice Hall.</p>
</div>
<div id="ref-kohavi1997wrappers">
<p>Kohavi, Ron, George H John, and others. 1997. “Wrappers for Feature Subset Selection.” <em>Artificial Intelligence</em> 97 (1-2): 273–324.</p>
</div>
<div id="ref-lee2005seleccao">
<p>Lee, Huei Diana. 2005. “Seleção de Atributos Importantes Para a Extração de Conhecimento de Bases de Dados.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div id="ref-liu2012feature">
<p>Liu, Huan, and Hiroshi Motoda. 2012. <em>Feature Selection for Knowledge Discovery and Data Mining</em>. Vol. 454. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-liu1996probabilistic">
<p>Liu, Huan, Rudy Setiono, and others. 1996. “A Probabilistic Approach to Feature Selection-a Filter Solution.” In <em>ICML</em>, 96:319–27. Citeseer.</p>
</div>
<div id="ref-mahalanobis1936generalized">
<p>Mahalanobis, Prasanta Chandra. 1936. “On the Generalized Distance in Statistics.” In. National Institute of Science of India.</p>
</div>
<div id="ref-mingoti2007analise">
<p>Mingoti, Sueli Aparecida. 2007. “Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada.” In <em>Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada</em>, 295–95.</p>
</div>
<div id="ref-moser2012student">
<p>Moser, Stefan M, and Po-Ning Chen. 2012. <em>A Student’s Guide to Coding and Information Theory</em>. Cambridge University Press.</p>
</div>
<div id="ref-nyquist1924certain">
<p>Nyquist, Harry. 1924. “Certain Factors Affecting Telegraph Speed.” <em>Transactions of the American Institute of Electrical Engineers</em> 43: 412–22.</p>
</div>
<div id="ref-parmezan2012avaliaccao">
<p>Parmezan, Antonio Rafael Sabino, Huei Diana Lee, Newton Spolaôr, and Wu Feng Chung. 2012. “Avaliação de Métodos Para Seleção de Atributos Importantes Para Aprendizado de Máquina Supervisionado No Processo de Mineração de Dados.”</p>
</div>
<div id="ref-paviotti2019consideraccoes">
<p>Paviotti, José Renato, and Carlos J Magossi. 2019. “Considerações Sobre O Conceito de Entropia Na Teoria Da Informação.”</p>
</div>
<div id="ref-algebrajeronimo">
<p>Pellegrini, Jeronimo C. 2015. <em>Álgebra Linear</em>. Vol. versão 130. <a href="https://www.ime.unicamp.br/~deleo/MA327/ld4.pdf">https://www.ime.unicamp.br/~deleo/MA327/ld4.pdf</a>.</p>
</div>
<div id="ref-shannon1948mathematical">
<p>Shannon, Claude E. 1948. “A Mathematical Theory of Communication.” <em>The Bell System Technical Journal</em> 27 (3): 379–423.</p>
</div>
<div id="ref-rossprobability">
<p>S.M, Ross. 2010. <em>A First Course in Probability</em>. 8th Edition. New York: Pearson Education, Inc.</p>
</div>
<div id="ref-tan2008">
<p>Tan, S. T. 2008. <em>Matemática Aplicada a Administração E Economia</em>. 2. ed. São Paulo, SP: Cengage Learning.</p>
</div>
<div id="ref-weinstock1974calculus">
<p>Weinstock, Robert. 1974. <em>Calculus of Variations: With Applications to Physics and Engineering</em>. Courier Corporation.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machinelearning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="preprocesso.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "github", "instagram"]
},
"fontsettings": ["white", "sepia", "night"],
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04.1-fundamentosIIanova.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
