<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Fundamentos de Machine Learning</title>
  <meta name="description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Fundamentos de Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Fundamentos de Machine Learning" />
  
  <meta name="twitter:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2021-03-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deeplearning.html"/>

<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>1</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="1.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>1.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="1.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>1.2</b> A arte de uma IA</a></li>
<li class="chapter" data-level="1.3" data-path="i-a.html"><a href="i-a.html#vertentes-de-uma-ia-e-fundamentação-filosófica"><i class="fa fa-check"></i><b>1.3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>2</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="2.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>2.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dicio.html"><a href="dicio.html"><i class="fa fa-check"></i><b>3</b> Uma breve revisão</a><ul>
<li class="chapter" data-level="3.1" data-path="dicio.html"><a href="dicio.html#um-pouco-de-álgebra-linear"><i class="fa fa-check"></i><b>3.1</b> Um pouco de Álgebra Linear</a></li>
<li class="chapter" data-level="3.2" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística"><i class="fa fa-check"></i><b>3.2</b> Um pouco de Estatística</a></li>
<li class="chapter" data-level="3.3" data-path="dicio.html"><a href="dicio.html#medidasimport"><i class="fa fa-check"></i><b>3.3</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="3.3.1" data-path="dicio.html"><a href="dicio.html#medidasdep"><i class="fa fa-check"></i><b>3.3.1</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="3.3.2" data-path="dicio.html"><a href="dicio.html#medinfo"><i class="fa fa-check"></i><b>3.3.2</b> Medidas de Informação</a></li>
<li class="chapter" data-level="3.3.3" data-path="dicio.html"><a href="dicio.html#meddist"><i class="fa fa-check"></i><b>3.3.3</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="3.3.4" data-path="dicio.html"><a href="dicio.html#medidas-de-precisão"><i class="fa fa-check"></i><b>3.3.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="3.3.5" data-path="dicio.html"><a href="dicio.html#medidas-de-consistência"><i class="fa fa-check"></i><b>3.3.5</b> Medidas de consistência</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>4</b> Pré-processamento</a><ul>
<li class="chapter" data-level="4.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>4.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="4.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>4.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="4.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>4.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>4.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="4.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="4.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>4.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>4.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>5</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="5.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>5.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="5.1.1" data-path="valid.html"><a href="valid.html#overfitting"><i class="fa fa-check"></i><b>5.1.1</b> <strong>Overfitting</strong></a></li>
<li class="chapter" data-level="5.1.2" data-path="valid.html"><a href="valid.html#underfitting"><i class="fa fa-check"></i><b>5.1.2</b> <strong>Underfitting</strong></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="valid.html"><a href="valid.html#holdout"><i class="fa fa-check"></i><b>5.2</b> Validação cruzada Hold-out</a></li>
<li class="chapter" data-level="5.3" data-path="valid.html"><a href="valid.html#kfold"><i class="fa fa-check"></i><b>5.3</b> Validação Cruzada <em>K-fold</em></a></li>
<li class="chapter" data-level="5.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>5.4</b> ROC e AUC</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>6</b> Modelos de Aprendizagem I</a><ul>
<li class="chapter" data-level="6.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="6.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>6.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>6.2</b> Regressão</a><ul>
<li class="chapter" data-level="6.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>6.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="6.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>6.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="6.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>6.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="6.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>6.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>6.3</b> Gradiente Descendente (GD)</a><ul>
<li class="chapter" data-level="6.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>6.3.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regularizacao"><i class="fa fa-check"></i><b>6.4</b> Regularização</a><ul>
<li class="chapter" data-level="6.4.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#penalizacoes"><i class="fa fa-check"></i><b>6.4.1</b> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></a></li>
<li class="chapter" data-level="6.4.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#elasticnet"><i class="fa fa-check"></i><b>6.4.2</b> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#knn-k-vizinhos-mais-próximos-k-nearest-neighbors"><i class="fa fa-check"></i><b>6.5</b> KNN: K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</a><ul>
<li class="chapter" data-level="6.5.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exknn"><i class="fa fa-check"></i><b>6.5.1</b> Exemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>7</b> Modelos de Aprendizagem II</a><ul>
<li class="chapter" data-level="7.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>7.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="7.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>7.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="7.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>7.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>7.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>7.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>7.3</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>7.3.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="7.3.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>7.3.2</b> Estatísticas</a></li>
<li class="chapter" data-level="7.3.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>7.3.3</b> A ACP</a></li>
<li class="chapter" data-level="7.3.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>7.3.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>7.4</b> Análise de Agrupamentos - <em>Clusters</em></a><ul>
<li class="chapter" data-level="7.4.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>7.4.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="7.4.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>7.4.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="7.4.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>7.4.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ptII.html"><a href="ptII.html#redesneurais"><i class="fa fa-check"></i><b>7.5</b> Redes Neurais Artificiais (RNA)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptIII.html"><a href="ptIII.html"><i class="fa fa-check"></i><b>8</b> Os métodos <em>Ensemble</em></a><ul>
<li class="chapter" data-level="8.1" data-path="ptIII.html"><a href="ptIII.html#bagging"><i class="fa fa-check"></i><b>8.1</b> <em>Bagging</em></a></li>
<li class="chapter" data-level="8.2" data-path="ptIII.html"><a href="ptIII.html#boost"><i class="fa fa-check"></i><b>8.2</b> <em>Boosting</em></a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptIII.html"><a href="ptIII.html#adaboost"><i class="fa fa-check"></i><b>8.2.1</b> <em>AdaBoost</em></a></li>
<li class="chapter" data-level="8.2.2" data-path="ptIII.html"><a href="ptIII.html#gradientboost"><i class="fa fa-check"></i><b>8.2.2</b> <em>Gradient Boosting</em></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptIII.html"><a href="ptIII.html#bagboost"><i class="fa fa-check"></i><b>8.3</b> <em>Bagging x Boosting</em></a></li>
<li class="chapter" data-level="8.4" data-path="ptIII.html"><a href="ptIII.html#stacking"><i class="fa fa-check"></i><b>8.4</b> <em>Stacking</em></a></li>
<li class="chapter" data-level="8.5" data-path="ptIII.html"><a href="ptIII.html#rf"><i class="fa fa-check"></i><b>8.5</b> Floresta Aleatória - <em>Random Forest</em></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ptII.html"><a href="ptII.html#redesneurais"><i class="fa fa-check"></i><b>9</b> Redes Neurais</a></li>
<li class="chapter" data-level="10" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>10</b> <em>Deep Learning</em></a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de <em>Machine Learning</em></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered">
<h1>References</h1>

<div id="refs" class="references">
<div>
<p>Almuallim, Hussein, and Thomas G Dietterich. 1994. “Learning Boolean Concepts in the Presence of Many Irrelevant Features.” <em>Artificial Intelligence</em> 69 (1-2): 279–305.</p>
</div>
<div>
<p>ANALYTICS VIDHYA. 2016. “Tree Based Algorithms: A Complete Tutorial from Scratch (in R &amp; Python).” In. <a href="https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/">https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/</a>.</p>
</div>
<div>
<p>Andrade, Dalton Francisco de, Adriano Ferreti Borgatto, Pedro Henrique Araujo, and Jeovani Schmitt. 2019. <em>Caderno de Pesquisa 1: Técnicas de Imputação de Dados Na Análise de Questionários Contextuais</em>. Brasília: Cebraspe.</p>
</div>
<div>
<p>AQUARELA. 2017. “Otimizando Agendamentos Médicos Com Inteligência Artificial.” <em>AQUARELA</em>. <a href="https://www.aquare.la/otimizando-agendamentos-medicos-com-inteligencia-artificial/">https://www.aquare.la/otimizando-agendamentos-medicos-com-inteligencia-artificial/</a>.</p>
</div>
<div>
<p>Assunção, Fernando. 2012. “Estratégias Para Tratamento de Variáveis Com Dados Faltantes Durante O Desenvolvimento de Modelos Preditivos.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Banzatto, David Ariovaldo, and S do N Kronka. 1992. “Experimentação Agrı́cola.” <em>Jaboticabal: Funep</em> 2.</p>
</div>
<div>
<p>Batista, Gustavo Enrique de Almeida Prado, and others. 2003. “Pré-Processamento de Dados Em Aprendizado de Máquina Supervisionado.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Bernardini, Flávia Cristina. 2002. “Combinação de Classificadores Simbólicos Para Melhorar O Poder Preditivo E Descritivo de Ensembles.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Beserra, Gustavo Freitas. 2020. “Aplicação de Técnicas de Machine Learning Para Predição Em Uma Campanha de Marketing.”</p>
</div>
<div>
<p>Bezdek, James C. 1981. “Objective Function Clustering.” In <em>Pattern Recognition with Fuzzy Objective Function Algorithms</em>, 43–93. Springer.</p>
</div>
<div>
<p>Bhattacharya, Binay K, Ronald S Poulsen, and Godfried T Toussaint. 1981. “Application of Proximity Graphs to Editing Nearest Neighbor Decision Rule.” In <em>International Symposium on Information Theory, Santa Monica</em>.</p>
</div>
<div>
<p>Bhattacharya, Binay, Kaustav Mukherjee, and Godfried Toussaint. 2005. “Geometric Decision Rules for Instance-Based Learning Problems.” In <em>International Conference on Pattern Recognition and Machine Intelligence</em>, 60–69. Springer.</p>
</div>
<div>
<p>Bobrow, Daniel Gureasko. 1967. “Problems in Natural Language Communication with Computers.” <em>IEEE Transactions on Human Factors in Electronics</em>, no. 1: 52–55.</p>
</div>
<div>
<p>Bolfarine, Heleno, and Mônica Carneiro Sandoval. 2001. <em>Introdução à Inferência Estatı́stica</em>. Vol. 2. SBM.</p>
</div>
<div>
<p>Borra, Simone, and Agostino Di Ciaccio. 2010. “Measuring the Prediction Error. A Comparison of Cross-Validation, Bootstrap and Covariance Penalty Methods.” <em>Computational Statistics &amp; Data Analysis</em> 54 (12): 2976–89.</p>
</div>
<div>
<p>Box, George EP, and Gwilym M Jenkins. 1976. “Time Series Analysis: Forecasting and Control San Francisco.” <em>Calif: Holden-Day</em>.</p>
</div>
<div>
<p>Breiman, Leo. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24 (2): 123–40.</p>
</div>
<div>
<p>———. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div>
<p>Breiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. <em>Classification and Regression Trees</em>. CRC press.</p>
</div>
<div>
<p>Breiman, Leo, and others. 1996. “Heuristics of Instability and Stabilization in Model Selection.” <em>Annals of Statistics</em> 24 (6): 2350–83.</p>
</div>
<div>
<p>Buchanan, Bruce G, and Edward H Shortliffe. 1984. “Rule-Based Expert Systems: The Mycin Experiments of the Stanford Heuristic Programming Project.”</p>
</div>
<div>
<p>Buchanan, B, Georgia Sutherland, and EA Feigenbaum. 1969. “Heuristic Dendral: A Program for Generating Explanatory Hypotheses.” <em>Organic Chemistry</em>.</p>
</div>
<div>
<p>Burman, Prabir. 1989. “A Comparative Study of Ordinary Cross-Validation, V-Fold Cross-Validation and the Repeated Learning-Testing Methods.” <em>Biometrika</em> 76 (3): 503–14.</p>
</div>
<div>
<p>Cardoso, Onézimo Carlos Viana. 2014. “Análise Particionada de Turbinas Eólicas Offshore Utilizando O Método de Multiplicadores de Lagrange Localizados.”</p>
</div>
<div>
<p>Caruana, Rich, and Dayne Freitag. 1994. “How Useful Is Relevance?” <em>FOCUS</em> 14 (8): 2.</p>
</div>
<div>
<p>CARVALHO, ACPLF, K Faceli, A LORENA, and J Gama. 2011. “Inteligência Artificial–Uma Abordagem de Aprendizado de Máquina.” <em>Rio de Janeiro: LTC</em>.</p>
</div>
<div>
<p>Casella, George, and Roger L Berger. 2010. “Inferência Estatı́stica.” <em>São Paulo: Cengage Learning</em>.</p>
</div>
<div>
<p>Chaves, Bruno Butilhão. 2012. “Estudo Do Algoritmo Adaboost de Aprendizagem de Máquina Aplicado a Sensores E Sistemas Embarcados.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Cordeiro, Gauss Moutinho. 1999. <em>Introduçao a Teoria Assintótica</em>. IMPA.</p>
</div>
<div>
<p>Covões, Thiago Ferreira. 2010. “Seleção de Atributos via Agrupamento.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Cox, DR. 1970. “Analysis of Binary Data London: Methuen &amp;Co.” Ltd.</p>
</div>
<div>
<p>Cross, Stephen E, and Edward Walker. 1994. “Dart: Applying Knowledge-Based Planning and Scheduling to Crisis Action Planning.” <em>Intelligent Scheduling. Morgan Kaufmann</em>.</p>
</div>
<div>
<p>Cunha, João Paulo Zanola. 2019. “Um Estudo Comparativo Das Técnicas de Validação Cruzada Aplicadas a Modelos Mistos.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Dash, Manoranjan, and Huan Liu. 2003. “Consistency-Based Search in Feature Selection.” <em>Artificial Intelligence</em> 151 (1-2): 155–76.</p>
</div>
<div>
<p>Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the Em Algorithm.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 39 (1): 1–22.</p>
</div>
<div>
<p>Dennett, Daniel C. 2009. “The Part of Cognitive Science That Is Philosophy.” <em>Topics in Cognitive Science</em> 1 (2): 231–36.</p>
</div>
<div>
<p>Devroye, Luc, and Terry Wagner. 1979. “Distribution-Free Performance Bounds for Potential Function Rules.” <em>IEEE Transactions on Information Theory</em> 25 (5): 601–4.</p>
</div>
<div>
<p>Dietterich, Thomas G. 2000. “Ensemble Methods in Machine Learning.” In <em>International Workshop on Multiple Classifier Systems</em>, 1–15. Springer.</p>
</div>
<div>
<p>Ding, Chris HQ, and Inna Dubchak. 2001. “Multi-Class Protein Fold Recognition Using Support Vector Machines and Neural Networks.” <em>Bioinformatics</em> 17 (4): 349–58.</p>
</div>
<div>
<p>Dougherty, James, Ron Kohavi, and Mehran Sahami. 1995. “Supervised and Unsupervised Discretization of Continuous Features.” In <em>Machine Learning Proceedings 1995</em>, 194–202. Elsevier.</p>
</div>
<div>
<p>Egan, James P, and James Pendleton Egan. 1975. <em>Signal Detection Theory and Roc-Analysis</em>. Academic press.</p>
</div>
<div>
<p>Enders, Craig K. 2010. <em>Applied Missing Data Analysis</em>. Guilford press.</p>
</div>
<div>
<p>Evans, Thomas G. 1964. “A Program for the Solution of a Class of Geometric-Analogy Intelligence-Test Questions.” AIR FORCE CAMBRIDGE RESEARCH LABS LG HANSCOM FIELD MASS.</p>
</div>
<div>
<p>Fahlman, Scott Elliott. 1974. “A Planning System for Robot Construction Tasks.” <em>Artificial Intelligence</em> 5 (1): 1–49.</p>
</div>
<div>
<p>Farias, Ana Maria Lima de. 2010. <em>Métodos Estatísticos Ii</em>. V. único. Rio de Janeiro, RJ: Fundação CECIERJ.</p>
</div>
<div>
<p>Felix, F. N. 2004. “Aplicando Bootstrap Para Determinação de Intervalos de Confiança Para O Número de Grupos No Procedimento Hierárquico Aglomerativo de Ward.” <em>Dissertação-Mestrado</em>.</p>
</div>
<div>
<p>Fisher, Ronald A. 1912. “On an Absolute Criterion for Fitting Frequency Curves.” <em>Messenger of Mathematics</em> 41: 155–56.</p>
</div>
<div>
<p>Freund, John E. 2009. <em>Estatı́stica Aplicada-: Economia, Administração E Contabilidade</em>. Bookman Editora.</p>
</div>
<div>
<p>Freund, Yoav, Robert Schapire, and Naoki Abe. 1999. “A Short Introduction to Boosting.” <em>Journal-Japanese Society for Artificial Intelligence</em> 14 (771-780): 1612.</p>
</div>
<div>
<p>Freund, Yoav, and Robert E Schapire. 1997. “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.” <em>Journal of Computer and System Sciences</em> 55 (1): 119–39.</p>
</div>
<div>
<p>Freund, Yoav, Robert E Schapire, and others. 1996. “Experiments with a New Boosting Algorithm.” In <em>Icml</em>, 96:148–56. Citeseer.</p>
</div>
<div>
<p>Friedman, Jerome H, and Nicholas I Fisher. 1999. “Bump Hunting in High-Dimensional Data.” <em>Statistics and Computing</em> 9 (2): 123–43.</p>
</div>
<div>
<p>Garcia, Salvador, Julian Luengo, José Antonio Sáez, Victoria Lopez, and Francisco Herrera. 2012. “A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning.” <em>IEEE Transactions on Knowledge and Data Engineering</em> 25 (4): 734–50.</p>
</div>
<div>
<p>Gastwirth, Joseph L. 1971. “A General Definition of the Lorenz Curve.” <em>Econometrica: Journal of the Econometric Society</em>, 1037–9.</p>
</div>
<div>
<p>Geurts, Pierre, Damien Ernst, and Louis Wehenkel. 2006. “Extremely Randomized Trees.” <em>Machine Learning</em> 63 (1): 3–42.</p>
</div>
<div>
<p>Glen, Stephanie. 2019. “ROC Curve Explained in One Picture.” In. <a href="https://www.datasciencecentral.com/profiles/blogs/roc-curve-explained-in-one-picture">https://www.datasciencecentral.com/profiles/blogs/roc-curve-explained-in-one-picture</a>.</p>
</div>
<div>
<p>Gonçalves, André Ricardo. 2008. “Máquina de Vetores Suporte.” <em>Universidade Estadual de Londrina</em> 21.</p>
</div>
<div>
<p>Gómez, Silvio Normey, and others. 2012. “Random Forests Estocástico.”</p>
</div>
<div>
<p>Green, David Marvin, John A Swets, and others. 1966. <em>Signal Detection Theory and Psychophysics</em>. Vol. 1. Wiley New York.</p>
</div>
<div>
<p>Guimarães, Rodrigo Régnier Chemim. 2019. “A Inteligência Artificial E a Disputa Por Diferentes Caminhos Em Sua Utilização Preditiva No Processo Penal.” <em>Revista Brasileira de Direito Processual Penal</em> 5 (3): 1555–88.</p>
</div>
<div>
<p>Gujarati, Damodar N, and Dawn C Porter. 2011. <em>Econometria Básica-5</em>. Amgh Editora.</p>
</div>
<div>
<p>Hartigan, John A, and Manchek A Wong. 1979. “Algorithm as 136: A K-Means Clustering Algorithm.” <em>Journal of the Royal Statistical Society. Series c (Applied Statistics)</em> 28 (1): 100–108.</p>
</div>
<div>
<p>Hartley, Ralph VL. 1928. “Transmission of Information 1.” <em>Bell System Technical Journal</em> 7 (3): 535–63.</p>
</div>
<div>
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer Science &amp; Business Media.</p>
</div>
<div>
<p>Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC press.</p>
</div>
<div>
<p>Hebb, Donald Olding. 1949. <em>The Organization of Behavior: A Neuropsychological Theory</em>. J. Wiley; Chapman &amp; Hall.</p>
</div>
<div>
<p>Henri, Theil. 1978. <em>Introduction to Econometrics</em>. Englewood Cliffs, New Jersey: Prentice Hall.</p>
</div>
<div>
<p>Hoerl, Arthur E. 1959. “Optimum Solution of Many Variables Equations.” <em>Chemical Engineering Progress</em> 55 (11): 69–78.</p>
</div>
<div>
<p>Holsheimer, Marcel, and Arno PJM Siebes. 1994. <em>Data Mining: The Search for Knowledge in Databases</em>. Centrum voor Wiskunde en Informatica.</p>
</div>
<div>
<p>Hongyu, Kuang, Vera Lúcia Martins Sandanielo, and Gilmar Jorge de Oliveira Junior. 2016. “Análise de Componentes Principais: Resumo Teórico, Aplicação E Interpretação.” <em>E&amp;S Engineering and Science</em> 5 (1): 83–90.</p>
</div>
<div>
<p>Hotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” <em>Journal of Educational Psychology</em> 24 (6): 417.</p>
</div>
<div>
<p>Huffman, David A. 1971. “Impossible Object as Nonsense Sentences.” <em>Machine Intelligence</em> 6: 295–324.</p>
</div>
<div>
<p>Ingargiola, Giorgio. 1996. “Building Classification Models: ID3 and C4. 5.” <em>Disponı́vel Por WWW Em: Http://Www. Cis. Temple. Edu/~ Ingargio/Cis587/Readings/Id3-C45. Html</em>.</p>
</div>
<div>
<p>John, George H, Ron Kohavi, and Karl Pfleger. 1994. “Irrelevant Features and the Subset Selection Problem.” In <em>Machine Learning Proceedings 1994</em>, 121–29. Elsevier.</p>
</div>
<div>
<p>Kaiser, Henry F. 1960. “The Application of Electronic Computers to Factor Analysis.” <em>Educational and Psychological Measurement</em> 20 (1): 141–51.</p>
</div>
<div>
<p>Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. “Lightgbm: A Highly Efficient Gradient Boosting Decision Tree.” <em>Advances in Neural Information Processing Systems</em> 30: 3146–54.</p>
</div>
<div>
<p>Kennedy, Peter E. 1981. “The ‘Ballentine’: A Graphical Aid for Econometrics.” <em>Australian Economic Papers</em> 20 (37): 414–16.</p>
</div>
<div>
<p>Kohavi, Ron, George H John, and others. 1997. “Wrappers for Feature Subset Selection.” <em>Artificial Intelligence</em> 97 (1-2): 273–324.</p>
</div>
<div>
<p>Kohavi, Ron, and others. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” In <em>Ijcai</em>, 14:1137–45. 2. Montreal, Canada.</p>
</div>
<div>
<p>Langley, Pat, and others. 1994. “Selection of Relevant Features in Machine Learning.” In <em>Proceedings of the Aaai Fall Symposium on Relevance</em>, 184:245–71. Citeseer.</p>
</div>
<div>
<p>Lee, Huei Diana. 2005. “Seleção de Atributos Importantes Para a Extração de Conhecimento de Bases de Dados.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Liaw, Andy, Matthew Wiener, and others. 2002. “Classification and Regression by randomForest.” <em>R News</em> 2 (3): 18–22.</p>
</div>
<div>
<p>Light, Richard J, and Barry H Margolin. 1971. “An Analysis of Variance for Categorical Data.” <em>Journal of the American Statistical Association</em> 66 (335): 534–44.</p>
</div>
<div>
<p>Lima, Allan Reffson Granja. 2002. “Máquinas de Vetores Suporte Na Classificaçao de Impressoes Digitais.” <em>Universidade Federal Do Ceará, Departamento de Computação, Fortaleza-Ceará</em>.</p>
</div>
<div>
<p>Little, Roderick JA, and Donald B Rubin. 2019. <em>Statistical Analysis with Missing Data</em>. Vol. 793. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Liu, H, and H Motoda. 2008. “Computational Methods of Feature Selection (Chapman &amp; Hall/Crc Data Mining and Knowledge Discovery Series).”</p>
</div>
<div>
<p>Liu, Huan, and Hiroshi Motoda. 1998. <em>Feature Extraction, Construction and Selection: A Data Mining Perspective</em>. Vol. 453. Springer Science &amp; Business Media.</p>
</div>
<div>
<p>———. 2012. <em>Feature Selection for Knowledge Discovery and Data Mining</em>. Vol. 454. Springer Science &amp; Business Media.</p>
</div>
<div>
<p>Liu, Huan, Rudy Setiono, and others. 1996. “A Probabilistic Approach to Feature Selection-a Filter Solution.” In <em>ICML</em>, 96:319–27. Citeseer.</p>
</div>
<div>
<p>Lorena, Ana Carolina, and André CPLF de Carvalho. 2003. “Introduçaoas Máquinas de Vetores Suporte.” <em>Relatório Técnico Do Instituto de Ciências Matemáticas E de Computaçao (USP/Sao Carlos)</em> 192: 11.</p>
</div>
<div>
<p>Machado, Wylken dos Santos, and others. 2020. “Avaliação de Modelos de Classificação Automática de Atividades Diárias Para Dispositivos de Baixo Custo.”</p>
</div>
<div>
<p>MACHADO-LIMA, Ariane. 2020. “Reconhecimento de Padrões - Vídeo 4 Do Tema 9: Random Forests.” In. Universidade de São Paulo - Portal de vídeoaulas. <a href="http://eaulas.usp.br/portal/video.action?idItem=13856">http://eaulas.usp.br/portal/video.action?idItem=13856</a>.</p>
</div>
<div>
<p>Mahalanobis, Prasanta Chandra. 1936. “On the Generalized Distance in Statistics.” In. National Institute of Science of India.</p>
</div>
<div>
<p>Maroco, João. 2014. “Análise Estatı́stica Com O Spss.” <em>Statistics</em> 6.</p>
</div>
<div>
<p>Marsh, B. 2016. “Multivariate Analysis of the Vector Boson Fusion Higgs Boson.” <em>University of Missouri</em> 8.</p>
</div>
<div>
<p>Mayrink, VTDM. 2015. “Avaliação Do Algoritmo Gradient Boosting Em Aplicações de Previsão de Carga Elétrica a Curto Prazo.” <em>Technical Report</em>.</p>
</div>
<div>
<p>McCarthy, J. 1968. “Programs with Common Sense’in Minsky M (Ed) Semantic Information Processing.” MIT Press: Cambridge Mass.</p>
</div>
<div>
<p>McCarthy, John, Marvin L Minsky, Nathaniel Rochester, and Claude E Shannon. 2006. “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955.” <em>AI Magazine</em> 27 (4): 12–12.</p>
</div>
<div>
<p>MCCLISH, D.K. 1989. “Analysing a Portion of the Roc Curve.” <em>Medical Decision Making</em> 9 (3): 190–96.</p>
</div>
<div>
<p>McCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” <em>The Bulletin of Mathematical Biophysics</em> 5 (4): 115–33.</p>
</div>
<div>
<p>Meehl, Paul E. 1954. “Clinical Versus Statistical Prediction: A Theoretical Analysis and a Review of the Evidence Minneapolis: University of Minnesota Press.[Reprinted with New Preface.” In <em>In Proceedings of the 1955 Invitational Conference on Testing Problems</em>. Citeseer.</p>
</div>
<div>
<p>Mercer, James. 1909. “Xvi. Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations.” <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em> 209 (441-458): 415–46.</p>
</div>
<div>
<p>Merjildo, Fernandez, Diego Alonso, and others. 2013. “Algoritmo Adaboost Robusto Ao Ruı́do: Aplicação à Detecção de Faces Em Imagens de Baixa Resolução.”</p>
</div>
<div>
<p>Mingoti, Sueli Aparecida. 2007. “Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada.” In <em>Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada</em>, 295–95.</p>
</div>
<div>
<p>Minsky, Marvin L, and Seymour Papert. 1969. “Perceptrons: An Introduction to.” <em>Computational Geometry</em>.</p>
</div>
<div>
<p>Montgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2012. <em>Introduction to Linear Regression Analysis</em>. Vol. 821. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Morettin, Pedro Alberto, and WILTON OLIVEIRA BUSSAB. 2017. <em>Estatı́stica Básica</em>. Saraiva Educação SA.</p>
</div>
<div>
<p>MOSER, Joana de Mello. 2006. “O Golem.” <em>Estudos Em Homenagem a Margarida Llosa</em>, 323–36.</p>
</div>
<div>
<p>Moser, Stefan M, and Po-Ning Chen. 2012. <em>A Student’s Guide to Coding and Information Theory</em>. Cambridge University Press.</p>
</div>
<div>
<p>Mylne, Kenneth R. 2002. “Decision-Making from Probability Forecasts Based on Forecast Value.” <em>Meteorological Applications</em> 9 (3): 307–15.</p>
</div>
<div>
<p>Nascimento CHAGAS, Elcio do, Camila Carvalho MENEZES, Marcelo Angelo CIRILLO, and Soraia Vilela BORGES. 2009. “Método ‘Ridge’ Em Modelo de Superfı́cie de Resposta: Otimização de Condições Experimentais Na Elaboração de Doce de Goiaba.” <em>Rev. Bras. Biom</em> 26 (4): 71–81.</p>
</div>
<div>
<p>Newell, A, and JC Shaw. 1959. “A Variety Op Intelligent Learning in a General Problem Solver.” <em>RAND Report P-1742, Dated July</em> 6.</p>
</div>
<div>
<p>NG, Andrew Y. 2019. “Gradient Descent Algorithm.” In. <a href="https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM">https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM</a>.</p>
</div>
<div>
<p>Nyquist, Harry. 1924. “Certain Factors Affecting Telegraph Speed.” <em>Transactions of the American Institute of Electrical Engineers</em> 43: 412–22.</p>
</div>
<div>
<p>Orgânica Digital. 2019. “Algoritmo de Classificação Naive Bayes.” In. <a href="https://www.organicadigital.com/blog/algoritmo-de-classificacao-naive-bayes/">https://www.organicadigital.com/blog/algoritmo-de-classificacao-naive-bayes/</a>.</p>
</div>
<div>
<p>Oshiro, Thais Mayumi. 2013. “Uma Abordagem Para a Construção de Uma única árvore a Partir de Uma Random Forest Para Classificação de Bases de Expressão Gênica.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Parmezan, Antonio Rafael Sabino, Huei Diana Lee, Newton Spolaôr, and Wu Feng Chung. 2012. “Avaliação de Métodos Para Seleção de Atributos Importantes Para Aprendizado de Máquina Supervisionado No Processo de Mineração de Dados.”</p>
</div>
<div>
<p>Paviotti, José Renato, and Carlos J Magossi. 2019. “Considerações Sobre O Conceito de Entropia Na Teoria Da Informação.”</p>
</div>
<div>
<p>Pearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> 2 (11): 559–72.</p>
</div>
<div>
<p>Pellegrini, Jeronimo C. 2015. <em>Álgebra Linear</em>. Vol. versão 130. <a href="https://www.ime.unicamp.br/~deleo/MA327/ld4.pdf">https://www.ime.unicamp.br/~deleo/MA327/ld4.pdf</a>.</p>
</div>
<div>
<p>Pereira, Simone Guimarães. 2019. “Inserção de Dados Faltantes Não Aleatórios Para Estimativa de Variável Geometalúrgica.”</p>
</div>
<div>
<p>Powell, Victor and Lehe, Lewis. 2014. “Análise Do Componente Principal.” In. <a href="https://setosa.io/ev/principal-component-analysis/">https://setosa.io/ev/principal-component-analysis/</a>.</p>
</div>
<div>
<p>Prati, RC, GEAPA Batista, MC Monard, and others. 2008. “Curvas Roc Para Avaliação de Classificadores.” <em>Revista IEEE América Latina</em> 6 (2): 215–22.</p>
</div>
<div>
<p>Provost, Foster, and Tom Fawcett. 1997. “Analysis and Visualization of Classifier Performance with Nonuniform Class and Cost Distributions.” In <em>Proceedings of Aaai-97 Workshop on Ai Approaches to Fraud Detection &amp; Risk Management</em>, 57–63.</p>
</div>
<div>
<p>Punj, Girish, and David W Stewart. 1983. “Cluster Analysis in Marketing Research: Review and Suggestions for Application.” <em>Journal of Marketing Research</em> 20 (2): 134–48.</p>
</div>
<div>
<p>Rauber, Thomas Walter. 2005. “Redes Neurais Artificiais.” <em>Universidade Federal Do Espı́rito Santo</em>, 29.</p>
</div>
<div>
<p>Rendle, Steffen, and Lars Schmidt-Thieme. 2008. “Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems.” In <em>Proceedings of the 2008 Acm Conference on Recommender Systems</em>, 251–58.</p>
</div>
<div>
<p>Rosenthal, Robert. 1994. “Science and Ethics in Conducting, Analyzing, and Reporting Psychological Research.” <em>Psychological Science</em> 5 (3): 127–34.</p>
</div>
<div>
<p>Rubin, Donald B. 2004. <em>Multiple Imputation for Nonresponse in Surveys</em>. Vol. 81. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Russel, Stuart, and Peter NORVIG. 2004. “Inteligência Artificial. 2. Edição.” <em>Rio de Janeiro: Campus</em>.</p>
</div>
<div>
<p>RUSSEL, Stuart, and Peter Norvig. 2013. “Inteligência Artificial. Tradução de Regina célia Smille.” <em>Rio de Janeiro: Campus Elsevier</em>.</p>
</div>
<div>
<p>Samuel, Arthur L. 1959. “Some Studies in Machine Learning Using the Game of Checkers.” <em>IBM Journal of Research and Development</em> 3 (3): 210–29.</p>
</div>
<div>
<p>Schafer, Joseph L. 1999. “Multiple Imputation: A Primer.” <em>Statistical Methods in Medical Research</em> 8 (1): 3–15.</p>
</div>
<div>
<p>Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” <em>Psychological Methods</em> 7 (2): 147.</p>
</div>
<div>
<p>Searle, John R. 1980. “Minds, Brains, and Programs, from the Behavioral and Brain Sciences, Vol. 3.” <em>Cambridge University Press Http://Members. Aol. Com/NeoNoetics/MindsBrainsPrograms. Html From</em> 23: 2004.</p>
</div>
<div>
<p>Shannon, Claude E. 1948. “A Mathematical Theory of Communication.” <em>The Bell System Technical Journal</em> 27 (3): 379–423.</p>
</div>
<div>
<p>———. 1950. “XXII. Programming a Computer for Playing Chess.” <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> 41 (314): 256–75.</p>
</div>
<div>
<p>Shapiro, Samuel Sanford, and Martin B Wilk. 1965. “An Analysis of Variance Test for Normality (Complete Samples).” <em>Biometrika</em> 52 (3/4): 591–611.</p>
</div>
<div>
<p>Shelley, Mary. 1818. “Frankenstein or the Modem Prometheus.” <em>London: Printed for Lackington, Hughes, Harding, Mayor &amp; Jones</em>.</p>
</div>
<div>
<p>Shiba, Marcelo Hiroshi, Rosangela Leal Santos, José Alberto Quintanilha, and Hae Yong Kim. 2005. “Classificação de Imagens de Sensoriamento Remoto Pela Aprendizagem Por árvore de Decisão: Uma Avaliação de Desempenho.” <em>Simpósio Brasileiro de Sensoriamento Remoto</em> 12: 4319–26.</p>
</div>
<div>
<p>Silva, Carina Brunehilde Pinto da. 2018. “A Técnica Lasso E Suas Potencialidades Na Seleção de Variáveis Para Modelos Lineares.”</p>
</div>
<div>
<p>Silva Meloni, Raphael Belo da. 2009. “Classificação de Imagens de Sensoriamento Remoto Usando Svm.” PhD thesis, PUC-Rio.</p>
</div>
<div>
<p>Silveira, José Atı́lio Pires da. 2013. “Searle E Dennett: Duas Perspectivas de Estudo Da Mente.” <em>Problemata: Revista Internacional de Filosofı́a</em> 4 (2): 238–58.</p>
</div>
<div>
<p>Silver, Nate. 2013. <em>O Sinal E O Ruı́do</em>. Editora Intrinseca.</p>
</div>
<div>
<p>Simon, Phil. 2013. <em>Too Big to Ignore: The Business Case for Big Data</em>. Vol. 72. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Slagle, James R. 1963. “A Heuristic Program That Solves Symbolic Integration Problems in Freshman Calculus.” <em>Journal of the ACM (JACM)</em> 10 (4): 507–20.</p>
</div>
<div>
<p>S.M, Ross. 2010. <em>A First Course in Probability</em>. 8th Edition. New York: Pearson Education, Inc.</p>
</div>
<div>
<p>Smola, Alexander J, Peter Bartlett, Bernhard Schölkopf, and Dale Schuurmans. 2000. “Introduction to Large Margin Classifiers.”</p>
</div>
<div>
<p>Sneath, Peter HA. 1957. “The Application of Computers to Taxonomy.” <em>Microbiology</em> 17 (1): 201–26.</p>
</div>
<div>
<p>Souza, Francisco Alexandre de. 2014. “Computational Intelligence Methodologies for Soft Sensors Development in Industrial Processes.” PhD thesis.</p>
</div>
<div>
<p>Spackman, Kent A. 1989. “Signal Detection Theory: Valuable Tools for Evaluating Inductive Learning.” In <em>Proceedings of the Sixth International Workshop on Machine Learning</em>, 160–63. Elsevier.</p>
</div>
<div>
<p>Speece, Deborah L, James D McKinney, and Mark I Appelbaum. 1985. “Classification and Validation of Behavioral Subtypes of Learning-Disabled Children.” <em>Journal of Educational Psychology</em> 77 (1): 67.</p>
</div>
<div>
<p>Sung, Andrew H, and Srinivas Mukkamala. 2003. “Identifying Important Features for Intrusion Detection Using Support Vector Machines and Neural Networks.” In <em>2003 Symposium on Applications and the Internet, 2003. Proceedings.</em>, 209–16. IEEE.</p>
</div>
<div>
<p>Tan, S. T. 2008. <em>Matemática Aplicada a Administração E Economia</em>. 2. ed. São Paulo, SP: Cengage Learning.</p>
</div>
<div>
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 58 (1): 267–88.</p>
</div>
<div>
<p>TURING, INTELLIGENCE BY AM. 1950. “Computing Machinery and Intelligence-Am Turing.” <em>Mind</em> 59 (236): 433.</p>
</div>
<div>
<p>Vapnik, Vladimir. 2013. <em>The Nature of Statistical Learning Theory</em>. Springer science &amp; business media.</p>
</div>
<div>
<p>Waltz, David. 1975. “Understanding Line Drawings of Scenes with Shadows.” In <em>The Psychology of Computer Vision</em>. Citeseer.</p>
</div>
<div>
<p>Ward Jr, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective Function.” <em>Journal of the American Statistical Association</em> 58 (301): 236–44.</p>
</div>
<div>
<p>Weinstock, Robert. 1974. <em>Calculus of Variations: With Applications to Physics and Engineering</em>. Courier Corporation.</p>
</div>
<div>
<p>Winograd, Terry. 1972. “Understanding Natural Language.” <em>Cognitive Psychology</em> 3 (1): 1–191.</p>
</div>
<div>
<p>Winston, Patrick H. 1970. “Learning Structural Descriptions from Examples.”</p>
</div>
<div>
<p>Wolpert, David H. 1992. “Stacked Generalization.” <em>Neural Networks</em> 5 (2): 241–59.</p>
</div>
<div>
<p>Yamagishi, Junichi, Hisashi Kawai, and Takao Kobayashi. 2008. “Phone Duration Modeling Using Gradient Tree Boosting.” <em>Speech Communication</em> 50 (5): 405–15.</p>
</div>
<div>
<p>Zhang, Yanru, and Ali Haghani. 2015. “A Gradient Boosting Method to Improve Travel Time Prediction.” <em>Transportation Research Part C: Emerging Technologies</em> 58: 308–24.</p>
</div>
<div>
<p>Zhou, Xiao-Hua, Donna K McClish, and Nancy A Obuchowski. 2009. <em>Statistical Methods in Diagnostic Medicine</em>. Vol. 569. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Zhou, Zhi-Hua. 2012. <em>Ensemble Methods: Foundations and Algorithms</em>. CRC press.</p>
</div>
<div>
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deeplearning.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "github", "instagram"]
},
"fontsettings": ["white", "sepia", "night"],
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/99-references.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
