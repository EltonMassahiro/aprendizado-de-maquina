<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Machine Learning</title>
  <meta name="description" content="Tutorial de Machine Learning." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Tutorial de Machine Learning." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Machine Learning" />
  
  <meta name="twitter:description" content="Tutorial de Machine Learning." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2020-12-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="valid.html"/>

<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#dicas-de-estudo"><i class="fa fa-check"></i><b>1.1</b> Dicas de estudo</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#dicio"><i class="fa fa-check"></i><b>1.2</b> Dicionário</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>2</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="2.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>2.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="2.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>2.2</b> A arte de uma IA</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="vertentes-de-uma-ia-e-fundamentação-filosófica.html"><a href="vertentes-de-uma-ia-e-fundamentação-filosófica.html"><i class="fa fa-check"></i><b>3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
<li class="chapter" data-level="4" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>4</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="4.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>4.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fundamentamos.html"><a href="fundamentamos.html"><i class="fa fa-check"></i><b>5</b> Fundamentamos</a><ul>
<li class="chapter" data-level="5.1" data-path="fundamentamos.html"><a href="fundamentamos.html#medidasimport"><i class="fa fa-check"></i><b>5.1</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="5.1.1" data-path="fundamentamos.html"><a href="fundamentamos.html#medinfo"><i class="fa fa-check"></i><b>5.1.1</b> Medidas de Informação</a></li>
<li class="chapter" data-level="5.1.2" data-path="fundamentamos.html"><a href="fundamentamos.html#meddist"><i class="fa fa-check"></i><b>5.1.2</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="5.1.3" data-path="fundamentamos.html"><a href="fundamentamos.html#medidasdep"><i class="fa fa-check"></i><b>5.1.3</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="5.1.4" data-path="fundamentamos.html"><a href="fundamentamos.html#medidas-de-precisão"><i class="fa fa-check"></i><b>5.1.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="5.1.5" data-path="fundamentamos.html"><a href="fundamentamos.html#medidas-de-consistência"><i class="fa fa-check"></i><b>5.1.5</b> Medidas de consistência</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="fundamentamos.html"><a href="fundamentamos.html#testesanova"><i class="fa fa-check"></i><b>5.2</b> Teste de hipóteses e Análise de Variância</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>6</b> Pré-processamento</a><ul>
<li class="chapter" data-level="6.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>6.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="6.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>6.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="6.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>6.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>6.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="6.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>6.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="6.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>6.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>6.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>7</b> Algoritmos de Aprendizagem - Parte I</a><ul>
<li class="chapter" data-level="7.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>7.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="7.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>7.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>7.2</b> Regressão</a><ul>
<li class="chapter" data-level="7.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>7.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="7.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>7.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="7.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>7.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="7.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>7.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>7.3</b> Gradiente Descendente (GD)</a><ul>
<li class="chapter" data-level="7.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>7.3.1</b> Exemplos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>8</b> Algoritmos de Aprendizagem - Parte II</a><ul>
<li class="chapter" data-level="8.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>8.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="8.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>8.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="8.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>8.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>8.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>8.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptII.html"><a href="ptII.html#elastic-net"><i class="fa fa-check"></i><b>8.3</b> Elastic Net</a></li>
<li class="chapter" data-level="8.4" data-path="ptII.html"><a href="ptII.html#knn"><i class="fa fa-check"></i><b>8.4</b> KNN</a></li>
<li class="chapter" data-level="8.5" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>8.5</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="8.5.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>8.5.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="8.5.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>8.5.2</b> Estatísticas</a></li>
<li class="chapter" data-level="8.5.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>8.5.3</b> A ACP</a></li>
<li class="chapter" data-level="8.5.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>8.5.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>8.6</b> Análise de Agrupamentos - Clusters</a><ul>
<li class="chapter" data-level="8.6.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>8.6.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="8.6.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>8.6.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="8.6.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>8.6.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="ptII.html"><a href="ptII.html#modelos-nivel-iii"><i class="fa fa-check"></i><b>8.7</b> modelos nivel III</a></li>
<li class="chapter" data-level="8.8" data-path="ptII.html"><a href="ptII.html#grad-boosting---estudar-boosting-e-bagging-dentro-de-emseamble"><i class="fa fa-check"></i><b>8.8</b> grad boosting -&gt; estudar boosting e bagging dentro de emseamble</a></li>
<li class="chapter" data-level="8.9" data-path="ptII.html"><a href="ptII.html#redes-neurais"><i class="fa fa-check"></i><b>8.9</b> Redes Neurais</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>9</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="9.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>9.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="9.1.1" data-path="valid.html"><a href="valid.html#underfitting-no-cenário-underfitting-o-desempenho-já-é-ruim-no-próprio-treinamento-de-seu-algoritmo."><i class="fa fa-check"></i><b>9.1.1</b> <strong>Underfitting</strong>: No cenário underfitting, o desempenho já é ruim no próprio treinamento de seu algoritmo.</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="valid.html"><a href="valid.html#validação-cruzada"><i class="fa fa-check"></i><b>9.2</b> Validação Cruzada</a></li>
<li class="chapter" data-level="9.3" data-path="valid.html"><a href="valid.html#como-escolher-um-bom-modelo"><i class="fa fa-check"></i><b>9.3</b> Como escolher um bom modelo?</a></li>
<li class="chapter" data-level="9.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>9.4</b> AOC e ROC</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered">
<h1>References</h1>

<div id="refs" class="references">
<div>
<p>ANALYTICS VIDHYA. 2016. “Tree Based Algorithms: A Complete Tutorial from Scratch (in R &amp; Python).” In. <a href="https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/">https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/</a>.</p>
</div>
<div>
<p>Andrade, Dalton Francisco de, Adriano Ferreti Borgatto, Pedro Henrique Araujo, and Jeovani Schmitt. 2019. <em>Caderno de Pesquisa 1: Técnicas de Imputação de Dados Na Análise de Questionários Contextuais</em>. Brasília: Cebraspe.</p>
</div>
<div>
<p>AQUARELA. 2017. “Otimizando Agendamentos Médicos Com Inteligência Artificial.” <em>AQUARELA</em>. <a href="https://www.aquare.la/otimizando-agendamentos-medicos-com-inteligencia-artificial/">https://www.aquare.la/otimizando-agendamentos-medicos-com-inteligencia-artificial/</a>.</p>
</div>
<div>
<p>Assunção, Fernando. 2012. “Estratégias Para Tratamento de Variáveis Com Dados Faltantes Durante O Desenvolvimento de Modelos Preditivos.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Banzatto, David Ariovaldo, and S do N Kronka. 1992. “Experimentação Agrı́cola.” <em>Jaboticabal: Funep</em> 2.</p>
</div>
<div>
<p>Batista, Gustavo Enrique de Almeida Prado, and others. 2003. “Pré-Processamento de Dados Em Aprendizado de Máquina Supervisionado.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Bezdek, James C. 1981. “Objective Function Clustering.” In <em>Pattern Recognition with Fuzzy Objective Function Algorithms</em>, 43–93. Springer.</p>
</div>
<div>
<p>Bobrow, Daniel Gureasko. 1967. “Problems in Natural Language Communication with Computers.” <em>IEEE Transactions on Human Factors in Electronics</em>, no. 1: 52–55.</p>
</div>
<div>
<p>Bolfarine, Heleno, and Mônica Carneiro Sandoval. 2001. <em>Introdução à Inferência Estatı́stica</em>. Vol. 2. SBM.</p>
</div>
<div>
<p>Box, George EP, and Gwilym M Jenkins. 1976. “Time Series Analysis: Forecasting and Control San Francisco.” <em>Calif: Holden-Day</em>.</p>
</div>
<div>
<p>Breiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. <em>Classification and Regression Trees</em>. CRC press.</p>
</div>
<div>
<p>Buchanan, Bruce G, and Edward H Shortliffe. 1984. “Rule-Based Expert Systems: The Mycin Experiments of the Stanford Heuristic Programming Project.”</p>
</div>
<div>
<p>Buchanan, B, Georgia Sutherland, and EA Feigenbaum. 1969. “Heuristic Dendral: A Program for Generating Explanatory Hypotheses.” <em>Organic Chemistry</em>.</p>
</div>
<div>
<p>CARVALHO, ACPLF, K Faceli, A LORENA, and J Gama. 2011. “Inteligência Artificial–Uma Abordagem de Aprendizado de Máquina.” <em>Rio de Janeiro: LTC</em>.</p>
</div>
<div>
<p>Casella, George, and Roger L Berger. 2010. “Inferência Estatı́stica.” <em>São Paulo: Cengage Learning</em>.</p>
</div>
<div>
<p>Cordeiro, Gauss Moutinho. 1999. <em>Introduçao a Teoria Assintótica</em>. IMPA.</p>
</div>
<div>
<p>Covões, Thiago Ferreira. 2010. “Seleção de Atributos via Agrupamento.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Cox, DR. 1970. “Analysis of Binary Data London: Methuen &amp;Co.” Ltd.</p>
</div>
<div>
<p>Cross, Stephen E, and Edward Walker. 1994. “Dart: Applying Knowledge-Based Planning and Scheduling to Crisis Action Planning.” <em>Intelligent Scheduling. Morgan Kaufmann</em>.</p>
</div>
<div>
<p>Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the Em Algorithm.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 39 (1): 1–22.</p>
</div>
<div>
<p>Dennett, Daniel C. 2009. “The Part of Cognitive Science That Is Philosophy.” <em>Topics in Cognitive Science</em> 1 (2): 231–36.</p>
</div>
<div>
<p>Ding, Chris HQ, and Inna Dubchak. 2001. “Multi-Class Protein Fold Recognition Using Support Vector Machines and Neural Networks.” <em>Bioinformatics</em> 17 (4): 349–58.</p>
</div>
<div>
<p>Dougherty, James, Ron Kohavi, and Mehran Sahami. 1995. “Supervised and Unsupervised Discretization of Continuous Features.” In <em>Machine Learning Proceedings 1995</em>, 194–202. Elsevier.</p>
</div>
<div>
<p>Enders, Craig K. 2010. <em>Applied Missing Data Analysis</em>. Guilford press.</p>
</div>
<div>
<p>Evans, Thomas G. 1964. “A Program for the Solution of a Class of Geometric-Analogy Intelligence-Test Questions.” AIR FORCE CAMBRIDGE RESEARCH LABS LG HANSCOM FIELD MASS.</p>
</div>
<div>
<p>Fahlman, Scott Elliott. 1974. “A Planning System for Robot Construction Tasks.” <em>Artificial Intelligence</em> 5 (1): 1–49.</p>
</div>
<div>
<p>Felix, F. N. 2004. “Aplicando Bootstrap Para Determinação de Intervalos de Confiança Para O Número de Grupos No Procedimento Hierárquico Aglomerativo de Ward.” <em>Dissertação-Mestrado</em>.</p>
</div>
<div>
<p>Fisher, Ronald A. 1912. “On an Absolute Criterion for Fitting Frequency Curves.” <em>Messenger of Mathematics</em> 41: 155–56.</p>
</div>
<div>
<p>Freund, John E. 2009. <em>Estatı́stica Aplicada-: Economia, Administração E Contabilidade</em>. Bookman Editora.</p>
</div>
<div>
<p>Garcia, Salvador, Julian Luengo, José Antonio Sáez, Victoria Lopez, and Francisco Herrera. 2012. “A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning.” <em>IEEE Transactions on Knowledge and Data Engineering</em> 25 (4): 734–50.</p>
</div>
<div>
<p>Gonçalves, André Ricardo. 2008. “Máquina de Vetores Suporte.” <em>Universidade Estadual de Londrina</em> 21.</p>
</div>
<div>
<p>Guimarães, Rodrigo Régnier Chemim. 2019. “A Inteligência Artificial E a Disputa Por Diferentes Caminhos Em Sua Utilização Preditiva No Processo Penal.” <em>Revista Brasileira de Direito Processual Penal</em> 5 (3): 1555–88.</p>
</div>
<div>
<p>Gujarati, Damodar N, and Dawn C Porter. 2011. <em>Econometria Básica-5</em>. Amgh Editora.</p>
</div>
<div>
<p>Hartigan, John A, and Manchek A Wong. 1979. “Algorithm as 136: A K-Means Clustering Algorithm.” <em>Journal of the Royal Statistical Society. Series c (Applied Statistics)</em> 28 (1): 100–108.</p>
</div>
<div>
<p>Hartley, Ralph VL. 1928. “Transmission of Information 1.” <em>Bell System Technical Journal</em> 7 (3): 535–63.</p>
</div>
<div>
<p>Hebb, Donald Olding. 1949. <em>The Organization of Behavior: A Neuropsychological Theory</em>. J. Wiley; Chapman &amp; Hall.</p>
</div>
<div>
<p>Henri, Theil. 1978. <em>Introduction to Econometrics</em>. Englewood Cliffs, New Jersey: Prentice Hall.</p>
</div>
<div>
<p>Holsheimer, Marcel, and Arno PJM Siebes. 1994. <em>Data Mining: The Search for Knowledge in Databases</em>. Centrum voor Wiskunde en Informatica.</p>
</div>
<div>
<p>Hongyu, Kuang, Vera Lúcia Martins Sandanielo, and Gilmar Jorge de Oliveira Junior. 2016. “Análise de Componentes Principais: Resumo Teórico, Aplicação E Interpretação.” <em>E&amp;S Engineering and Science</em> 5 (1): 83–90.</p>
</div>
<div>
<p>Hotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” <em>Journal of Educational Psychology</em> 24 (6): 417.</p>
</div>
<div>
<p>Huffman, David A. 1971. “Impossible Object as Nonsense Sentences.” <em>Machine Intelligence</em> 6: 295–324.</p>
</div>
<div>
<p>Ingargiola, Giorgio. 1996. “Building Classification Models: ID3 and C4. 5.” <em>Disponı́vel Por WWW Em: Http://Www. Cis. Temple. Edu/~ Ingargio/Cis587/Readings/Id3-C45. Html</em>.</p>
</div>
<div>
<p>John, George H, Ron Kohavi, and Karl Pfleger. 1994. “Irrelevant Features and the Subset Selection Problem.” In <em>Machine Learning Proceedings 1994</em>, 121–29. Elsevier.</p>
</div>
<div>
<p>Kaiser, Henry F. 1960. “The Application of Electronic Computers to Factor Analysis.” <em>Educational and Psychological Measurement</em> 20 (1): 141–51.</p>
</div>
<div>
<p>Kennedy, Peter E. 1981. “The ‘Ballentine’: A Graphical Aid for Econometrics.” <em>Australian Economic Papers</em> 20 (37): 414–16.</p>
</div>
<div>
<p>Langley, Pat, and others. 1994. “Selection of Relevant Features in Machine Learning.” In <em>Proceedings of the Aaai Fall Symposium on Relevance</em>, 184:245–71. Citeseer.</p>
</div>
<div>
<p>Lee, Huei Diana. 2005. “Seleção de Atributos Importantes Para a Extração de Conhecimento de Bases de Dados.” PhD thesis, Universidade de São Paulo.</p>
</div>
<div>
<p>Light, Richard J, and Barry H Margolin. 1971. “An Analysis of Variance for Categorical Data.” <em>Journal of the American Statistical Association</em> 66 (335): 534–44.</p>
</div>
<div>
<p>Lima, Allan Reffson Granja. 2002. “Máquinas de Vetores Suporte Na Classificaçao de Impressoes Digitais.” <em>Universidade Federal Do Ceará, Departamento de Computação, Fortaleza-Ceará</em>.</p>
</div>
<div>
<p>Little, Roderick JA, and Donald B Rubin. 2019. <em>Statistical Analysis with Missing Data</em>. Vol. 793. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Liu, H, and H Motoda. 2008. “Computational Methods of Feature Selection (Chapman &amp; Hall/Crc Data Mining and Knowledge Discovery Series).”</p>
</div>
<div>
<p>Liu, Huan, and Hiroshi Motoda. 1998. <em>Feature Extraction, Construction and Selection: A Data Mining Perspective</em>. Vol. 453. Springer Science &amp; Business Media.</p>
</div>
<div>
<p>———. 2012. <em>Feature Selection for Knowledge Discovery and Data Mining</em>. Vol. 454. Springer Science &amp; Business Media.</p>
</div>
<div>
<p>Lorena, Ana Carolina, and André CPLF de Carvalho. 2003. “Introduçaoas Máquinas de Vetores Suporte.” <em>Relatório Técnico Do Instituto de Ciências Matemáticas E de Computaçao (USP/Sao Carlos)</em> 192: 11.</p>
</div>
<div>
<p>Mahalanobis, Prasanta Chandra. 1936. “On the Generalized Distance in Statistics.” In. National Institute of Science of India.</p>
</div>
<div>
<p>Maroco, João. 2014. “Análise Estatı́stica Com O Spss.” <em>Statistics</em> 6.</p>
</div>
<div>
<p>McCarthy, J. 1968. “Programs with Common Sense’in Minsky M (Ed) Semantic Information Processing.” MIT Press: Cambridge Mass.</p>
</div>
<div>
<p>McCarthy, John, Marvin L Minsky, Nathaniel Rochester, and Claude E Shannon. 2006. “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955.” <em>AI Magazine</em> 27 (4): 12–12.</p>
</div>
<div>
<p>McCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” <em>The Bulletin of Mathematical Biophysics</em> 5 (4): 115–33.</p>
</div>
<div>
<p>Meehl, Paul E. 1954. “Clinical Versus Statistical Prediction: A Theoretical Analysis and a Review of the Evidence Minneapolis: University of Minnesota Press.[Reprinted with New Preface.” In <em>In Proceedings of the 1955 Invitational Conference on Testing Problems</em>. Citeseer.</p>
</div>
<div>
<p>Mingoti, Sueli Aparecida. 2007. “Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada.” In <em>Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada</em>, 295–95.</p>
</div>
<div>
<p>Minsky, Marvin L, and Seymour Papert. 1969. “Perceptrons: An Introduction to.” <em>Computational Geometry</em>.</p>
</div>
<div>
<p>Montgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2012. <em>Introduction to Linear Regression Analysis</em>. Vol. 821. John Wiley &amp; Sons.</p>
</div>
<div>
<p>MOSER, Joana de Mello. 2006. “O Golem.” <em>Estudos Em Homenagem a Margarida Llosa</em>, 323–36.</p>
</div>
<div>
<p>Moser, Stefan M, and Po-Ning Chen. 2012. <em>A Student’s Guide to Coding and Information Theory</em>. Cambridge University Press.</p>
</div>
<div>
<p>Newell, A, and JC Shaw. 1959. “A Variety Op Intelligent Learning in a General Problem Solver.” <em>RAND Report P-1742, Dated July</em> 6.</p>
</div>
<div>
<p>NG, Andrew Y. 2019. “Gradient Descent Algorithm.” In. <a href="https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM">https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM</a>.</p>
</div>
<div>
<p>Nyquist, Harry. 1924. “Certain Factors Affecting Telegraph Speed.” <em>Transactions of the American Institute of Electrical Engineers</em> 43: 412–22.</p>
</div>
<div>
<p>Orgânica Digital. 2019. “Algoritmo de Classificação Naive Bayes.” In. <a href="https://www.organicadigital.com/blog/algoritmo-de-classificacao-naive-bayes/">https://www.organicadigital.com/blog/algoritmo-de-classificacao-naive-bayes/</a>.</p>
</div>
<div>
<p>Parmezan, Antonio Rafael Sabino, Huei Diana Lee, Newton Spolaôr, and Wu Feng Chung. 2012. “Avaliação de Métodos Para Seleção de Atributos Importantes Para Aprendizado de Máquina Supervisionado No Processo de Mineração de Dados.”</p>
</div>
<div>
<p>Paviotti, José Renato, and Carlos J Magossi. 2019. “Considerações Sobre O Conceito de Entropia Na Teoria Da Informação.”</p>
</div>
<div>
<p>Pearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> 2 (11): 559–72.</p>
</div>
<div>
<p>Pereira, Simone Guimarães. 2019. “Inserção de Dados Faltantes Não Aleatórios Para Estimativa de Variável Geometalúrgica.”</p>
</div>
<div>
<p>Powell, Victor and Lehe, Lewis. 2014. “Análise Do Componente Principal.” In. <a href="https://setosa.io/ev/principal-component-analysis/">https://setosa.io/ev/principal-component-analysis/</a>.</p>
</div>
<div>
<p>Punj, Girish, and David W Stewart. 1983. “Cluster Analysis in Marketing Research: Review and Suggestions for Application.” <em>Journal of Marketing Research</em> 20 (2): 134–48.</p>
</div>
<div>
<p>Rendle, Steffen, and Lars Schmidt-Thieme. 2008. “Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems.” In <em>Proceedings of the 2008 Acm Conference on Recommender Systems</em>, 251–58.</p>
</div>
<div>
<p>Rosenthal, Robert. 1994. “Science and Ethics in Conducting, Analyzing, and Reporting Psychological Research.” <em>Psychological Science</em> 5 (3): 127–34.</p>
</div>
<div>
<p>Rubin, Donald B. 2004. <em>Multiple Imputation for Nonresponse in Surveys</em>. Vol. 81. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Russel, Stuart, and Peter NORVIG. 2004. “Inteligência Artificial. 2. Edição.” <em>Rio de Janeiro: Campus</em>.</p>
</div>
<div>
<p>RUSSEL, Stuart, and Peter Norvig. 2013. “Inteligência Artificial. Tradução de Regina célia Smille.” <em>Rio de Janeiro: Campus Elsevier</em>.</p>
</div>
<div>
<p>Samuel, Arthur L. 1959. “Some Studies in Machine Learning Using the Game of Checkers.” <em>IBM Journal of Research and Development</em> 3 (3): 210–29.</p>
</div>
<div>
<p>Schafer, Joseph L. 1999. “Multiple Imputation: A Primer.” <em>Statistical Methods in Medical Research</em> 8 (1): 3–15.</p>
</div>
<div>
<p>Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” <em>Psychological Methods</em> 7 (2): 147.</p>
</div>
<div>
<p>Searle, John R. 1980. “Minds, Brains, and Programs, from the Behavioral and Brain Sciences, Vol. 3.” <em>Cambridge University Press Http://Members. Aol. Com/NeoNoetics/MindsBrainsPrograms. Html From</em> 23: 2004.</p>
</div>
<div>
<p>Shannon, Claude E. 1948. “A Mathematical Theory of Communication.” <em>The Bell System Technical Journal</em> 27 (3): 379–423.</p>
</div>
<div>
<p>———. 1950. “XXII. Programming a Computer for Playing Chess.” <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> 41 (314): 256–75.</p>
</div>
<div>
<p>Shelley, Mary. 1818. “Frankenstein or the Modem Prometheus.” <em>London: Printed for Lackington, Hughes, Harding, Mayor &amp; Jones</em>.</p>
</div>
<div>
<p>Shiba, Marcelo Hiroshi, Rosangela Leal Santos, José Alberto Quintanilha, and Hae Yong Kim. 2005. “Classificação de Imagens de Sensoriamento Remoto Pela Aprendizagem Por árvore de Decisão: Uma Avaliação de Desempenho.” <em>Simpósio Brasileiro de Sensoriamento Remoto</em> 12: 4319–26.</p>
</div>
<div>
<p>Silva Meloni, Raphael Belo da. 2009. “Classificação de Imagens de Sensoriamento Remoto Usando Svm.” PhD thesis, PUC-Rio.</p>
</div>
<div>
<p>Silveira, José Atı́lio Pires da. 2013. “Searle E Dennett: Duas Perspectivas de Estudo Da Mente.” <em>Problemata: Revista Internacional de Filosofı́a</em> 4 (2): 238–58.</p>
</div>
<div>
<p>Silver, Nate. 2013. <em>O Sinal E O Ruı́do</em>. Editora Intrinseca.</p>
</div>
<div>
<p>Simon, Phil. 2013. <em>Too Big to Ignore: The Business Case for Big Data</em>. Vol. 72. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Slagle, James R. 1963. “A Heuristic Program That Solves Symbolic Integration Problems in Freshman Calculus.” <em>Journal of the ACM (JACM)</em> 10 (4): 507–20.</p>
</div>
<div>
<p>Smola, Alexander J, Peter Bartlett, Bernhard Schölkopf, and Dale Schuurmans. 2000. “Introduction to Large Margin Classifiers.”</p>
</div>
<div>
<p>Sneath, Peter HA. 1957. “The Application of Computers to Taxonomy.” <em>Microbiology</em> 17 (1): 201–26.</p>
</div>
<div>
<p>Souza, Francisco Alexandre de. 2014. “Computational Intelligence Methodologies for Soft Sensors Development in Industrial Processes.” PhD thesis.</p>
</div>
<div>
<p>Speece, Deborah L, James D McKinney, and Mark I Appelbaum. 1985. “Classification and Validation of Behavioral Subtypes of Learning-Disabled Children.” <em>Journal of Educational Psychology</em> 77 (1): 67.</p>
</div>
<div>
<p>Sung, Andrew H, and Srinivas Mukkamala. 2003. “Identifying Important Features for Intrusion Detection Using Support Vector Machines and Neural Networks.” In <em>2003 Symposium on Applications and the Internet, 2003. Proceedings.</em>, 209–16. IEEE.</p>
</div>
<div>
<p>TURING, INTELLIGENCE BY AM. 1950. “Computing Machinery and Intelligence-Am Turing.” <em>Mind</em> 59 (236): 433.</p>
</div>
<div>
<p>Vapnik, Vladimir. 2013. <em>The Nature of Statistical Learning Theory</em>. Springer science &amp; business media.</p>
</div>
<div>
<p>Waltz, David. 1975. “Understanding Line Drawings of Scenes with Shadows.” In <em>The Psychology of Computer Vision</em>. Citeseer.</p>
</div>
<div>
<p>Ward Jr, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective Function.” <em>Journal of the American Statistical Association</em> 58 (301): 236–44.</p>
</div>
<div>
<p>Winograd, Terry. 1972. “Understanding Natural Language.” <em>Cognitive Psychology</em> 3 (1): 1–191.</p>
</div>
<div>
<p>Winston, Patrick H. 1970. “Learning Structural Descriptions from Examples.”</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="valid.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/99-references.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
