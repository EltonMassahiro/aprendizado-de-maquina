<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 Algoritmos de Aprendizagem II | Fundamentos de Machine Learning</title>
  <meta name="description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 Algoritmos de Aprendizagem II | Fundamentos de Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 Algoritmos de Aprendizagem II | Fundamentos de Machine Learning" />
  
  <meta name="twitter:description" content="Entenda Machine Learning desde sua história até seus principais conceitos." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2021-02-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Algoritmosaprendizagem.html"/>
<link rel="next" href="ptIII.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>1</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="1.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>1.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="1.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>1.2</b> A arte de uma IA</a></li>
<li class="chapter" data-level="1.3" data-path="i-a.html"><a href="i-a.html#vertentes-de-uma-ia-e-fundamentação-filosófica"><i class="fa fa-check"></i><b>1.3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>2</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="2.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>2.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dicio.html"><a href="dicio.html"><i class="fa fa-check"></i><b>3</b> Uma breve revisão</a><ul>
<li class="chapter" data-level="3.1" data-path="dicio.html"><a href="dicio.html#um-pouco-de-álgebra-linear"><i class="fa fa-check"></i><b>3.1</b> Um pouco de Álgebra Linear</a></li>
<li class="chapter" data-level="3.2" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística"><i class="fa fa-check"></i><b>3.2</b> Um pouco de Estatística</a></li>
<li class="chapter" data-level="3.3" data-path="dicio.html"><a href="dicio.html#medidasimport"><i class="fa fa-check"></i><b>3.3</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="3.3.1" data-path="dicio.html"><a href="dicio.html#medidasdep"><i class="fa fa-check"></i><b>3.3.1</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="3.3.2" data-path="dicio.html"><a href="dicio.html#medinfo"><i class="fa fa-check"></i><b>3.3.2</b> Medidas de Informação</a></li>
<li class="chapter" data-level="3.3.3" data-path="dicio.html"><a href="dicio.html#meddist"><i class="fa fa-check"></i><b>3.3.3</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="3.3.4" data-path="dicio.html"><a href="dicio.html#medidas-de-precisão"><i class="fa fa-check"></i><b>3.3.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="3.3.5" data-path="dicio.html"><a href="dicio.html#medidas-de-consistência"><i class="fa fa-check"></i><b>3.3.5</b> Medidas de consistência</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>4</b> Pré-processamento</a><ul>
<li class="chapter" data-level="4.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>4.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="4.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>4.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="4.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>4.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>4.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="4.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>4.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="4.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>4.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>4.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>5</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="5.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>5.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="5.1.1" data-path="valid.html"><a href="valid.html#overfitting"><i class="fa fa-check"></i><b>5.1.1</b> <strong>Overfitting</strong></a></li>
<li class="chapter" data-level="5.1.2" data-path="valid.html"><a href="valid.html#underfitting"><i class="fa fa-check"></i><b>5.1.2</b> <strong>Underfitting</strong></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="valid.html"><a href="valid.html#holdout"><i class="fa fa-check"></i><b>5.2</b> Validação cruzada Hold-out</a></li>
<li class="chapter" data-level="5.3" data-path="valid.html"><a href="valid.html#kfold"><i class="fa fa-check"></i><b>5.3</b> Validação Cruzada <em>K-fold</em></a></li>
<li class="chapter" data-level="5.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>5.4</b> ROC e AUC</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>6</b> Modelos de Aprendizagem I</a><ul>
<li class="chapter" data-level="6.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="6.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>6.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>6.2</b> Regressão</a><ul>
<li class="chapter" data-level="6.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>6.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="6.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>6.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="6.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>6.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="6.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>6.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>6.3</b> Gradiente Descendente (GD)</a><ul>
<li class="chapter" data-level="6.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>6.3.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regularizacao"><i class="fa fa-check"></i><b>6.4</b> Regularização</a><ul>
<li class="chapter" data-level="6.4.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#penalizacoes"><i class="fa fa-check"></i><b>6.4.1</b> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></a></li>
<li class="chapter" data-level="6.4.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#elasticnet"><i class="fa fa-check"></i><b>6.4.2</b> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>7</b> Algoritmos de Aprendizagem II</a><ul>
<li class="chapter" data-level="7.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>7.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="7.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>7.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="7.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>7.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="7.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>7.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>7.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>7.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>7.3</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>7.3.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="7.3.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>7.3.2</b> Estatísticas</a></li>
<li class="chapter" data-level="7.3.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>7.3.3</b> A ACP</a></li>
<li class="chapter" data-level="7.3.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>7.3.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>7.4</b> Análise de Agrupamentos - <em>Clusters</em></a><ul>
<li class="chapter" data-level="7.4.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>7.4.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="7.4.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>7.4.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="7.4.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>7.4.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ptII.html"><a href="ptII.html#knn-k-vizinhos-mais-próximos-k-nearest-neighbors"><i class="fa fa-check"></i><b>7.5</b> KNN: K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ptII.html"><a href="ptII.html#exknn"><i class="fa fa-check"></i><b>7.5.1</b> Exemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptIII.html"><a href="ptIII.html"><i class="fa fa-check"></i><b>8</b> Os métodos <em>Ensemble</em></a><ul>
<li class="chapter" data-level="8.1" data-path="ptIII.html"><a href="ptIII.html#bagging"><i class="fa fa-check"></i><b>8.1</b> <em>Bagging</em></a></li>
<li class="chapter" data-level="8.2" data-path="ptIII.html"><a href="ptIII.html#boost"><i class="fa fa-check"></i><b>8.2</b> <em>Boosting</em></a></li>
<li class="chapter" data-level="8.3" data-path="ptIII.html"><a href="ptIII.html#rf"><i class="fa fa-check"></i><b>8.3</b> Floresta Aleatória - <em>Random Forest</em></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="redesneurais.html"><a href="redesneurais.html"><i class="fa fa-check"></i><b>9</b> Redes Neurais</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ptII" class="section level1">
<h1><span class="header-section-number">Capítulo 7</span> Algoritmos de Aprendizagem II</h1>
<div id="svm" class="section level2">
<h2><span class="header-section-number">7.1</span> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></h2>
<p>A Máquina de Vetores de Suporte (SVMs, do inglês <em>Support Vectors Machine</em>), teve como alicerce a teoria de aprendizado estatístico, desenvolvida por <span class="citation">Vapnik (<a href="#ref-vapnik2013nature">2013</a>)</span> com o propósito de resolver problemas de classificação de padrões, foi originalmente desenvolvida para classificação binária, construindo um hiperplano como superfície de decisão que separa classes linearmente separáveis (ao caso de não-linearmente separáveis utiliza-se função de mapeamento). Muitos a comparam com redes neurais pelo fato de ser eficiente em trabalhar com dados de alta dimensionalidade <span class="citation">(Sung and Mukkamala <a href="#ref-sung2003identifying">2003</a>; Ding and Dubchak <a href="#ref-ding2001multi">2001</a>)</span>. É utilizada atualmente tanto para regressão quanto para qualissificação e é uma análise supervisionada.</p>
<p>Vamos supor um gráfico com características <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_2\)</span> e já classificados na amostra os indivíduos que estão e não estão doentes, quanto maior o valor de ambos maior a probabilidade do indivíduo ser classificado como doente.</p>
<div class="figure" style="text-align: center"><span id="fig:svm1"></span>
<img src="Figuras/svm1.png" alt="Gráfico de \(X_1\) e \(X_2\) classificado em doentes e não doentes." width="70%" />
<p class="caption">
Figura 7.1: Gráfico de <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_2\)</span> classificado em doentes e não doentes.
</p>
</div>

<p>Um exemplo como esse é simples observar que podemos separar os dados traçando uma linha reta. Classificando os doentes para a direita e acima do gráfico e não doentes a esquerda e abaixo. Esta linha é o que chamamos de <strong>hiperplano de separação</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:svm2"></span>
<img src="Figuras/svm2.png" alt="Gráfico de \(X_1\) e \(X_2\) classificado em doentes e não doentes por meio de um hiperplano." width="70%" />
<p class="caption">
Figura 7.2: Gráfico de <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_2\)</span> classificado em doentes e não doentes por meio de um hiperplano.
</p>
</div>

<p>Lembrando que hiperplano é uma generalização de um plano: uma dimensão é um ponto, duas dimensões é uma linha e três é um plano. Quando tratamos de mais dimensões é o que denominamos hiperplano. O SVM pode trabalhar com qualquer dimensão.</p>
<p>Podemos encontrar em uma amostra vários hiperplanos de separação e válidos para a classficação de um <em>dataset</em>. Mas não necessariamente este é o melhor. Até mesmo pode ser que classifique alguns errados.</p>
<div class="figure" style="text-align: center"><span id="fig:svm3"></span>
<img src="Figuras/svm3.png" alt="Gráfico de \(X_1\) e \(X_2\) com a classificação de doentes e não doentes por mais de um hiperplano de separação." width="70%" />
<p class="caption">
Figura 7.3: Gráfico de <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_2\)</span> com a classificação de doentes e não doentes por mais de um hiperplano de separação.
</p>
</div>

<p>O objetivo do algoritmo de SVM é identificar um hiperplano “ideal” que busca classificar o conjunto de dados da melhor maneira possível (menor erro). Ao verificar as distâncias perpendiculares entre as observações e o hiperplano de separação, obtemos uma <strong>Margem</strong>. Os pontos menos distantes do hiperplano são os que a definem. Estes pontos são os <strong>Vetores de Suporte (VS)</strong>, que têm este nome pois eles dão suporte ao hiperplano. Caso forem movidos, a margem acompanhará o movimento.</p>
<div class="figure" style="text-align: center"><span id="fig:svm4"></span>
<img src="Figuras/svm4.png" alt="Gráfico de \(X_1\) e \(X_2\) com a classificação de doentes e não doentes pelo hiperplano e sua margem." width="70%" />
<p class="caption">
Figura 7.4: Gráfico de <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_2\)</span> com a classificação de doentes e não doentes pelo hiperplano e sua margem.
</p>
</div>

<div id="classificação-de-padrões-linearmente-separáveis" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Classificação de Padrões Linearmente Separáveis</h3>
<p>Uma classificação linear consiste em determinar uma função <span class="math inline">\(f: X \subseteq \mathbb{R}^N \rightarrow \mathbb{R}^N\)</span> que atribuirá um valor de <span class="math inline">\(+1\)</span> se <span class="math inline">\(f(x)\geq 0\)</span> e <span class="math inline">\(-1\)</span> se <span class="math inline">\(f(x)&lt;0\)</span>. Sendo assim pelo produto interno (ver Produto Interno em <a href="dicio.html#dicio">3</a>):</p>
<p><span class="math display" id="eq:classlin">\[\begin{equation}
f(x)= \langle\vec{w}.\vec{x}\rangle+b
\tag{7.1}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:classlin2">\[\begin{equation}
= \displaystyle \sum^n_{i=1} \vec{w_i} \vec{x}_i+b
\tag{7.2}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(\vec{w}\)</span> e <span class="math inline">\(b\)</span> são popularmente conhecido como <strong>vetor peso</strong> e <strong><em>bias</em></strong> que são os parâmetros responsáveis em controlar a função e a regra da decisão <span class="citation">(Lima <a href="#ref-lima2002maquinas">2002</a>)</span>. Os valores destes parâmetros são obtidos pelo processo de aprendizagem a partir dos dados de entrada <span class="citation">(Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>. Sendo o vetor peso <span class="math inline">\(\vec{w}\)</span> que define uma direção perpendicular ao hiperplano e com a variação de <span class="math inline">\(b\)</span> o hiperplano é movido paralelamente a ele mesmo.</p>
<p>Para facilitar a compreensão do <em>Bias</em> pense o seguinte: após suas aulas da faculdade você sempre toma um cafézinho por R$ 1,50. Tem dias que você almoça num restaurante mais caro, outros dias almoça no restaurante universitário, outros dias que não está com fome não almoça. Mas essa invariante, o <em>Bias</em> que seria o consumo do café sempre tem. Você nunca está no “zero” sem consumir nada. Um valor mínimo. Podemos dizer que o parâmetro <em>Bias</em> é como o intercepto da regressão linear simples (parâmetro <span class="math inline">\(\beta_1\)</span>).</p>
<p>Ao caso desse parâmetro no modelo SVM, sem ele o classificador sempre irá passar pela origem. O SVM não fornece o hiperplano de separação com a margem máxima se não passar pela origem a menos que possua um “víes”, o <em>Bias</em>.</p>
<p>Um SVM linear procura encontrar um hiperplano ótimo que separe da melhor maneira possível os dados de cada classe (margem máxima).</p>
<div class="figure" style="text-align: center"><span id="fig:svm5"></span>
<img src="Figuras/svm5.PNG" alt="Interpretação geométrica de \(\vec{w}\) e \(b\) sobre um hiperplano (Lima 2002; Gonçalves 2008)." width="70%" />
<p class="caption">
Figura 7.5: Interpretação geométrica de <span class="math inline">\(\vec{w}\)</span> e <span class="math inline">\(b\)</span> sobre um hiperplano <span class="citation">(Lima <a href="#ref-lima2002maquinas">2002</a>; Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>.
</p>
</div>

</div>
<div id="margmax" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Hiperplano de Separação Ótima / Margem Máxima</h3>
<p>Um hiperplano é considerado de margem máxima se separa um conjunto de vetores sem erros e a distância entre os vetores de classes diferentes mais próximos do hiperplano é a máxima possível.</p>
<div class="figure" style="text-align: center"><span id="fig:margmax"></span>
<img src="Figuras/margmax.PNG" alt="(a) Um hiperplano de separação com uma pequena margem. (b) Um hiperplano de Margem máxima (Silva Meloni 2009)." width="70%" />
<p class="caption">
Figura 7.6: (a) Um hiperplano de separação com uma pequena margem. (b) Um hiperplano de Margem máxima <span class="citation">(Silva Meloni <a href="#ref-da2009classificaccao">2009</a>)</span>.
</p>
</div>

<p>Assumindo que o conjunto de dados é linearmente separável, o hiperplano ótimo é que possuir a maior margem:</p>
<p><span class="math display" id="eq:maiormarg">\[\begin{equation}
\langle\vec{w}.\vec{x}\rangle+b=0
\tag{7.3}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(\vec{w}\)</span> e <span class="math inline">\(b\)</span>, o vetor peso e <em>bias</em> respectivamente.</p>
<p>Assumindo a restrição:</p>
<p><span class="math display">\[\langle\vec{w}.\vec{x}_i\rangle+b\geq + 1 \ , \ \mbox{para} \ y_i=+1\]</span>
<span class="math display" id="eq:restricaoclassif">\[\begin{equation}
\langle\vec{w}.\vec{x}_i\rangle+b\leq - 1 \ , \ \mbox{para} \ y_i=-1
\tag{7.4}
\end{equation}\]</span></p>
<p>Os classificadores lineares que separam o conjunto de dados em treinamento possuem margem positiva. Esta restrição nos mostra que não há dados entre 0 e <span class="math inline">\(\pm1\)</span>, tendo como a margem sempre maior que a distância entre os hiperplanos <span class="math inline">\(\langle\vec{w}.\vec{x}_i\rangle+b=0\)</span> e <span class="math inline">\(|\langle\vec{w}.\vec{x}_i\rangle+b= 1|\)</span>. Fazendo com que as SVMs sejam chamadas de <strong>Margens Rígidas</strong>, do inglês <em>Hard Margin</em>. Com isso, ao combinar ambas equações restrições:</p>
<p><span class="math display" id="eq:restricaoclassifcom">\[\begin{equation}
y_i(\langle\vec{w}.\vec{x}_i\rangle+b)\geq  1 \ ,  \ i=\{1,2,...,n\}
\tag{7.5}
\end{equation}\]</span></p>
<p>Ou:
<span class="math display" id="eq:restricaoclassifcomb2">\[\begin{equation}
y_i(w^T.\vec{x}_i+b)\geq  1 \ ,  \ i=\{1,2,...,n\}
\tag{7.6}
\end{equation}\]</span></p>
<p>Aplicando a distância Euclidiana (<span class="math inline">\(d_+\)</span> e <span class="math inline">\(d_-\)</span>) entre os vetores de suporte positivos/negativos e o hiperplano, definido a margem <span class="math inline">\(\rho\)</span> de um hiperplano de separação como sendo a maior geométrica entre todos os hiperplano, é possível representar <span class="math inline">\(\rho(d_+ + d_-)\)</span> <span class="citation">(Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>.</p>
<p><span class="math display" id="eq:distanciahip">\[\begin{equation}
d_i(\vec{w},b;\vec{x}_i)=\frac{|\langle\vec{w}.\vec{x}_i\rangle+b|}{||\vec{w}||}=\frac{y_i(|\langle\vec{w}.\vec{x}_i\rangle+b)}{||\vec{w}||}
\tag{7.7}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(d_(\vec{w},b;\vec{x}_i)\)</span> é a distância de um dado <span class="math inline">\(\vec{x}_i\)</span> ao hiperplano <span class="math inline">\((\vec{w},b)\)</span> <span class="citation">(Lima <a href="#ref-lima2002maquinas">2002</a>)</span>. Ao levarmos em consideração a restrição de <a href="ptII.html#eq:restricaoclassifcomb2">(7.6)</a>, podemos expressar:</p>
<p><span class="math display" id="eq:distcrestri">\[\begin{equation}
d(\vec{w},b;\vec{x}_i)\geq \frac{1}{||\vec{w}||}
\tag{7.8}
\end{equation}\]</span></p>
<p>Identificando <span class="math inline">\(\frac{1}{||\vec{w}||}\)</span> como o limite inferior da distância entre os vetores de suporte <span class="math inline">\(\vec{x}_i\)</span> e o hiperplano <span class="math inline">\((\vec{w},b)\)</span>. Logo, as distância serão:</p>
<p><span class="math display" id="eq:distanciamarg">\[\begin{equation}
d_+=d_-=\frac{1}{||\vec{w}||}
\tag{7.9}
\end{equation}\]</span></p>
<p>A margem é sempre maior que a última instância, a minimização de <span class="math inline">\(||\vec{w}||\)</span> nos traz a maximização da margem. Podemos definir a margem <span class="math inline">\(\rho\)</span> como <span class="citation">(Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>:</p>
<p><span class="math display" id="eq:margemsvm">\[\begin{equation}
\rho=(d_+=d_-)=\frac{2}{||\vec{w}||}
\tag{7.10}
\end{equation}\]</span></p>
<p>Assim teremos a distância entre hiperplanos e os vetores de suporte:</p>
<div class="figure" style="text-align: center"><span id="fig:svm6"></span>
<img src="Figuras/svm6.PNG" alt="Distância entre hiperplanos e vetores de suporte (Gonçalves 2008)." width="70%" />
<p class="caption">
Figura 7.7: Distância entre hiperplanos e vetores de suporte <span class="citation">(Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>.
</p>
</div>

<p>Para minimizarmos <span class="math inline">\(||\vec{w}||\)</span> (maximizarmos esta margem), podemos utilizar a teoria dos multiplicadores de Lagrange (ver Multiplicadores de Lagrange em <a href="dicio.html#dicio">3</a>):</p>
<p><span class="math display" id="eq:lagrangemargem">\[\begin{equation}
L(\vec{w},b,\alpha)=\frac{1}{2}||\vec{w}||^2-\displaystyle \sum^n_{i=1}\alpha_i(y_i(\langle\vec{w}.\vec{x}_i\rangle+b)-1)
\tag{7.11}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(\alpha_i\)</span> são os multiplicadores de Lagrange. Então agora visamos minimizar <span class="math inline">\(L(\vec{w},b,\alpha)\)</span>, em relação a <span class="math inline">\(\vec{w}\)</span> e <span class="math inline">\(b\)</span> e a maximização dos <span class="math inline">\(\alpha_i\)</span> (encontrar os pontos ótimos pelas derivadas parciais iguals a zero). Portanto:</p>
<p><span class="math display">\[\frac{\partial L}{\partial b}=0\]</span>
<span class="math display">\[\frac{\partial L}{\partial \vec{w}}=\bigg(\frac{\partial L}{\partial w_1},\frac{\partial L}{\partial w_2},...,\frac{\partial L}{\partial w_n}\bigg)=0\]</span>
Obtemos por meio delas:</p>
<p><span class="math display" id="eq:lagrangemarge1">\[\begin{equation}
\displaystyle \sum^n_{i=1}\alpha_i y_i=0
\tag{7.12}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:lagrangemarge2">\[\begin{equation}
\vec{w}=\displaystyle \sum^n_{i=1}\alpha_i y_i \vec{w}_i
\tag{7.13}
\end{equation}\]</span></p>
<p>Substituindo as equações <a href="ptII.html#eq:lagrangemarge1">(7.12)</a> e <a href="ptII.html#eq:lagrangemarge2">(7.13)</a> em <a href="ptII.html#eq:lagrangemargem">(7.11)</a>, chegamos em:</p>
<p><span class="math display" id="eq:lagrangemarge3">\[\begin{equation}
L =\displaystyle \sum^n_{i=1}\alpha_i-\frac{1}{2} \displaystyle \sum^n_{i=1}\sum^n_{j=1}\alpha_i\alpha_j y_i y_j \langle x_i.x_j\rangle
\tag{7.14}
\end{equation}\]</span></p>
<p>com <span class="math inline">\(\alpha_i\geq0,i\{1,2,...,n\}\)</span>. Serão representados os valores ótimos de <span class="math inline">\((\vec{w},b)\)</span> por <span class="math inline">\((\vec{w}^*,b^*)\)</span>. E <span class="math inline">\(\alpha^*_i\)</span> assume valores positivos para os exemplos de treinamento que estão a uma distância do hiperplano ótimo igual a largura da margem, os vetores de suporte <span class="citation">(Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>. Podemos perceber que o hiperplano de separação ótimo é obtido pelos vetores de suporte e não de todo o conjunto <span class="citation">(Lorena and Carvalho <a href="#ref-lorena2003introduccaoas">2003</a>)</span>.</p>
<p>Com um vetor suporte dado <span class="math inline">\(\vec{x}_j\)</span>, obtemos valor de <span class="math inline">\(b^*\)</span> pela condição de KKT (condição de primeira ordem que faz com que a solução de um problema não linear seja ótima):</p>
<p><span class="math display" id="eq:kktsvm">\[\begin{equation}
b^* =y_j-\langle\vec{w}^*.\vec{x}_j \rangle.
\tag{7.15}
\end{equation}\]</span></p>
<p>Com todos os valores dos parâmetros calculados podemos, por fim, ter um novo padrão <span class="math inline">\(z\)</span> calculando:</p>
<p><span class="math display" id="eq:sgn">\[\begin{equation}
sgn(\langle \vec{w}^* . \vec{z}\rangle+b^*)
\tag{7.16}
\end{equation}\]</span></p>
<p>sendo <span class="math inline">\(sgn\)</span> a função sinal que fornece o valor 1 se o número for positivo, valor 0 se o número for zero e -1 se for negativo.</p>
<p>Pode-se também utilizar as <strong>Margens Flexíveis</strong>, que busca não garantir todas as observações no lado certo, tolerando algumas violações. Basicamente atribui-se um erro para cada observação que viola o hiperplano proporcional o quanto passou a margem e o violou. Acrescentando na função este erro para compensar. Nos algoritmos dos <em>softwares</em> estatísticos, é comum o uso de uma constante <strong>C</strong> que controla a severidade do modelo, dando limite do tanto do erro que pode haver no algoritmo. Seu valor depende muito da pesquisa e de sua fundamentação teórica, pois pode ser que aumente a quantidade de vetores de suporte e até mesmo podendo causando problemas de <strong>overfitting</strong> (<a href="valid.html#fitt">5.1</a>).</p>
</div>
<div id="classificação-de-padrões-não-linearmente-separáveis" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Classificação de Padrões Não-Linearmente Separáveis</h3>
<p>Nas situações reais a maioria dos padrões são mais complexos e não-lineares. O conjunto de dados é classificado como não-linearmente separável ao caso de não ser possível separar os dados com um hiperplano:</p>
<div class="figure" style="text-align: center"><span id="fig:svm7"></span>
<img src="Figuras/svm7.PNG" alt="Padrões linearmente e não-linearmente separável respectivamente (Gonçalves 2008)." width="70%" />
<p class="caption">
Figura 7.8: Padrões linearmente e não-linearmente separável respectivamente <span class="citation">(Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>.
</p>
</div>

<p>Segundo <span class="citation">Smola et al. (<a href="#ref-smola2000introduction">2000</a>)</span>, o teorema de Cover afima que um problema não-linear possui maior probabilidade de ser linearmente separável em um espaço de mais alta dimensionalidade. Com isso, a SVM não-linear faz uma mudança de dimensionalidade por meio das funções <em>Kernel</em> para tratarmos de uma problema de classificação linear e permitindo elaborar o hiperplano ótimo (note que ACP possui uma ideia similar dado suas propriedades).</p>
<p>Um conjunto de entrada <span class="math inline">\(X\)</span> com pares <span class="math inline">\(\{(x_1,y_1);(x_2,y_2),...,(x_n,y_n)\}\)</span> de uma amostra de treinamento (não-linearmente separável), são mapeados por meio de uma função <span class="math inline">\(\phi\)</span> a fim de obter um novo conjunto de dados <span class="math inline">\(X&#39;\)</span> linearmente separável em um espaço de maior dimensionalidade, representado por <span class="math inline">\(\{(\phi(x_1),y_1),...,(\phi(x_n),y_n)\}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:svm8"></span>
<img src="Figuras/svm8.png" alt="Mapeamento de um conjunto de entrada \(X\) para o espaço caracerística. Um novo conjunto \(X&#39;\)." width="70%" />
<p class="caption">
Figura 7.9: Mapeamento de um conjunto de entrada <span class="math inline">\(X\)</span> para o espaço caracerística. Um novo conjunto <span class="math inline">\(X&#39;\)</span>.
</p>
</div>

<p>Com os dados de treinamento mapeados para o espaço de características, utiliza-se os valores mapeados <span class="math inline">\(\phi(x)\)</span> ao invés de <span class="math inline">\(x\)</span>, sendo assim o problema consiste em:</p>
<p><span class="math display" id="eq:svnphi">\[\begin{equation} 
L =\displaystyle \sum^n_{i=1}\alpha_i-\frac{1}{2} \displaystyle \sum^n_{i=1}\sum^n_{j=1}\alpha_i\alpha_j y_i y_j \langle \phi(x_i).\phi(x_j)\rangle
\tag{7.17}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(\alpha_i \geq 0\)</span>. Para classificação não-linear as mesmas considerações de KKT descrito no linear. O hiperplano fica expresso como:
<span class="math display" id="eq:hipotimophi">\[\begin{equation}
(\vec{w}\phi \ . \ (\vec{x}))+b=0
\tag{7.18}
\end{equation}\]</span></p>
<p>Ao problema de classificação não-linear de um novo padrão <strong>z</strong>:
<span class="math display" id="eq:naolinzphi">\[\begin{equation}
sgn(\langle \vec{w}^* \ . \ \phi(z)\rangle+b^*)
\tag{7.19}
\end{equation}\]</span></p>
<p>Uma função <em>Kernel</em>, pertencente a um domínio que permita calcular o produto interno para calculá-lo, recebe dois dados de entrada <span class="math inline">\(x_i\)</span> e <span class="math inline">\(x_j\)</span> destes dados no espaço de características.
<span class="math display" id="eq:kernel">\[\begin{equation}
\kappa=(x_i,x_j)=\langle \phi(x_i) \ . \ \phi(x_j)\rangle
\tag{7.20}
\end{equation}\]</span></p>
<p>A função precisa satisfazer as condições do <strong>Teorema de Mercer</strong>, portanto a matriz <span class="math inline">\(K\)</span> é positivamente definida (autovalores maiores que zero). <span class="math inline">\(K\)</span> é obtida por:</p>
<p><span class="math display" id="eq:mercer">\[\begin{equation}
K=K_{ij}=\kappa(x_i,x_j)
\tag{7.21}
\end{equation}\]</span></p>
<p>As funções <em>Kernel</em> mais utilizadas são:</p>
<table>
<caption><span id="tab:kernel">Tabela 7.1: </span> <em>Kernels</em> mais populares <span class="citation">(Gonçalves <a href="#ref-gonccalves2015maquina">2008</a>)</span>.</caption>
<thead>
<tr class="header">
<th><strong>Tipo de Kernel</strong></th>
<th><strong>Função <span class="math inline">\(\kappa(x_i,x_j)\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Polinomial</td>
<td><span class="math inline">\((\langle x_i.x_j\rangle +1)^p\)</span></td>
</tr>
<tr class="even">
<td>Gaussiano</td>
<td><span class="math inline">\(e^{(-\frac{\|\|x_i-x_j\|\|^2}{2\sigma^2})}\)</span></td>
</tr>
<tr class="odd">
<td>Sigmoidal</td>
<td><span class="math inline">\(tanh(\beta_0 \langle x_i.x_j\rangle )+\beta_1\)</span></td>
</tr>
</tbody>
</table>
<p>Lembrando que o pesquisador precisa escolher alguns parâmetros, bem como a definição de qual algoritmo utilizar no SVM <span class="citation">(Lorena and Carvalho <a href="#ref-lorena2003introduccaoas">2003</a>)</span>.</p>
</div>
<div id="exemplosvm" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Exemplos</h3>
<ol style="list-style-type: decimal">
<li>Vamos supor o seguinte conjunto de dados:
<span class="math display">\[\{ (4,1), (4,-1), (5,1), (5,-1), (0,-1), (0,2), (0,1), (1,2) \}\]</span></li>
</ol>
<p>Ao observamos no gráfico, podemos observar que os vetores de suporte são:
<span class="math display">\[\{s_1=(1,2),s_2=(4,1),s_3=(4,-1)\}\]</span></p>
<p>Agora, vamos inserir o valor 1 de entrada do <em>Bias</em>:</p>
<p><span class="math display">\[\{\hat{s_1}=(1,2,1),\hat{s_2}=(4,1,1),\hat{s_3}=(4,-1,1)\}\]</span>
Precisamos agora encontrar o valor de <span class="math inline">\(\alpha_i\)</span>:</p>
<p><span class="math display">\[\alpha_1 \phi(s_1).\phi(s_1)+\alpha_2 \phi(s_2).\phi(s_1)+\alpha_3 \phi (s_3).\phi (s_1)=-1\]</span>
<span class="math display">\[\alpha_1 \phi(s_1).\phi(s_2)+\alpha_2 \phi(s_2).\phi(s_2)+\alpha_3 \phi (s_3).\phi (s_2)=+1\]</span>
<span class="math display">\[\alpha_1 \phi(s_1).\phi(s_3)+\alpha_2 \phi(s_2).\phi(s_3)+\alpha_3 \phi (s_3).\phi (s_3)=+1\]</span></p>
<p>E portanto:
<span class="math display">\[\alpha_1 \hat{s_1}.\hat{s_1}+\alpha_2 \hat{s_2}.\hat{s_1}+\alpha_3 \hat{s_3}.\hat{s_1}=-1\]</span>
<span class="math display">\[\alpha_1 \hat{s_1}.\hat{s_2}+\alpha_2 \hat{s_2}.\hat{s_2}+\alpha_3 \hat{s_3}.\hat{s_2}=+1\]</span>
<span class="math display">\[\alpha_1 \hat{s_1}.\hat{s_3}+\alpha_2 \hat{s_2}.\hat{s_3}+\alpha_3 \hat{s_3}.\hat{s_3}=+1\]</span></p>
<p><span class="math display">\[\left\{
\alpha_1
\begin{pmatrix}
1\\2\\1
\end{pmatrix}
\begin{pmatrix}
1\\2\\1
\end{pmatrix} +
\alpha_2
\begin{pmatrix}
4\\1\\1
\end{pmatrix}
\begin{pmatrix}
1\\2\\1
\end{pmatrix}+
\alpha_3
\begin{pmatrix}
4\\-1\\1
\end{pmatrix}
\begin{pmatrix}
1\\2\\1
\end{pmatrix}
\right\}=-1\]</span></p>
<p><span class="math display">\[\left\{
\alpha_1
\begin{pmatrix}
1\\2\\1
\end{pmatrix}
\begin{pmatrix}
4\\1\\1
\end{pmatrix} +
\alpha_2
\begin{pmatrix}
4\\1\\1
\end{pmatrix}
\begin{pmatrix}
4\\1\\1
\end{pmatrix}+
\alpha_3
\begin{pmatrix}
4\\-1\\1
\end{pmatrix}
\begin{pmatrix}
4\\1\\1
\end{pmatrix}
\right\}=+1\]</span></p>
<p><span class="math display">\[\left\{
\alpha_1
\begin{pmatrix}
1\\2\\1
\end{pmatrix}
\begin{pmatrix}
4\\-1\\1
\end{pmatrix} +
\alpha_2
\begin{pmatrix}
4\\1\\1
\end{pmatrix}
\begin{pmatrix}
4\\-1\\1
\end{pmatrix}+
\alpha_3
\begin{pmatrix}
4\\-1\\1
\end{pmatrix}
\begin{pmatrix}
4\\-1\\1
\end{pmatrix}
\right\}=+1\]</span></p>
<p>Resolvendo as matrizes, temos:
<span class="math display">\[\alpha_1(1+4+1)+\alpha_2(4+2+1)+\alpha_3(4-2+1)=-1\]</span>
<span class="math display">\[\alpha_1(4+2+1)+\alpha_2(16+1+1)+\alpha_3(16-1+1)=+1\]</span>
<span class="math display">\[\alpha_1(4-2+1)+\alpha_2(16-1+1)+\alpha_3(16+1+1)=+1\]</span>
E portanto:
<span class="math display">\[6\alpha_1+7\alpha_2+3\alpha_3=-1\]</span>
<span class="math display">\[7\alpha_1+18\alpha_2+16\alpha_3=+1\]</span>
<span class="math display">\[3\alpha_1+16\alpha_2+18\alpha_3=-1\]</span>
Logo <span class="math inline">\(\alpha_1=2,44\)</span>,<span class="math inline">\(\alpha_2=2,83\)</span> e <span class="math inline">\(\alpha_3=-2,06\)</span>. Encontrando <span class="math inline">\(\vec{w}=\displaystyle \sum^n_{i=1}\alpha_i \hat{s_i}\)</span>:</p>
<p><span class="math display">\[\vec{w}=2,44\begin{pmatrix} 1\\2\\1\end{pmatrix}+
2,83\begin{pmatrix} 4\\1\\1\end{pmatrix}+
-2,06\begin{pmatrix} 4\\-1\\1\end{pmatrix}\]</span>
<span class="math display">\[\begin{pmatrix}5,52\\9,77\\3,21\end{pmatrix}\]</span></p>
<p>Portanto temos que <span class="math inline">\(w=\begin{pmatrix} 2,44\\2,83\end{pmatrix}\)</span> e <span class="math inline">\(b=-2,06\)</span>. Com a equação <span class="math inline">\(y=wx+b\)</span> e todos os dados, podemos plotar o hiperplano.</p>

</div>
</div>
<div id="decisiontree" class="section level2">
<h2><span class="header-section-number">7.2</span> Árvore de Decisão (<em>Decision Tree</em>)</h2>
<p>A Árvore de Decisão - <em>Decision Tree</em> - é muito utilizada em algoritmos para classificação de dados, tem como objetivo construir classificadores que predizem classes baseadas nos valores de atributos de um <em>dataset</em> (análise supervisionada). Ela pode ser aplicada tanto em variáveis categóricas quanto contínuas de entrada e de saída. Na árvore de decisão, dividimos a população ou amostra em dois ou mais conjuntos homogêneos com base nos divisores mais significativos das variáveis de entrada. É um modelo fácil de compreensão, útil para explorar os dados e classificar os dados, sem restrição do tipo de seus dados e pode ser considerada como não paramétrica, porém precisa-se tomar cuidado em ocorrer um sobreajuste e a aplicação em variáveis contínuas pode perder informações.</p>
<p>Há muitos algoritmos de classificação que constroem árvores de decisão. Cada um pode ter melhor desempenho em determinada situação e outro algoritmo pode ser mais eficiente em outros tipos de situações, não há como apontar qual o melhor método. Ela é composta por:</p>
<ol style="list-style-type: decimal">
<li><p>Nó Raiz/Nodos: Representa a população ou uma amostra, podendo ser ainda dividido em dois ou mais conjuntos homogêneos;</p></li>
<li><p>Divisão e Arcos: É o processo de dividir um nó em dois ou mais sub-nós, gerando arcos provenientes destes nodos que recebem os valores possíveis para estes atributos;</p></li>
<li><p>Nó de Decisão: Quando um sub-nó é dividido em sub-nós adicionais;</p></li>
<li><p>Folha/Nó de Término: Os nós não divididos são denominados de Folha ou Nó de Término, representam as diferentes classes de um conjunto de treinamento;</p></li>
<li><p>Poda: O processo de remover sub-nós de um nó de decisão é chamado poda. Existe técnicas para avaliar o bom momento de podar o galho (sub-nó) da árvore.</p></li>
</ol>
<p>Segue abaixo uma demonstração da ramificação da árvore:</p>
<div class="figure" style="text-align: center"><span id="fig:decisiontree"></span>
<img src="Figuras/decisiontree.png" alt="Terminologia de Árvore de Decisão." width="70%" />
<p class="caption">
Figura 7.10: Terminologia de Árvore de Decisão.
</p>
</div>

<p>Um nó que é dividido em sub-nós é chamado de nó pai. Os sub-nós são os nós filhos do nó pai.</p>
<p>Em geral são determinadas regras em seu algoritmo. As regras são escritas considerando o trajeto do nó raiz até uma folha da árvore. Por elas tenderem a crescer muito, de acordo com algumas aplicações, elas são muitas vezes substituídas pelas regras. Isto acontece em virtude das regras poderem ser facilmente modularizadas. Uma regra pode ser compreendida sem que haja a necessidade de se referenciar outras regras <span class="citation">(Ingargiola <a href="#ref-ingargiola1996building">1996</a>)</span>.</p>
<p>Sua função é de particionar recursivamente um conjunto de treinamento, até que cada subconjunto obtido deste particionamento contenha casos de uma única classe. Com esta finalidade, basicamente a árvore de decisão verifica e compara a distribuição de classes durante a construção da árvore. Após finalizar a árvore, sua saída são dados organizados de maneira compacta, que são utilizados para classificar novos casos <span class="citation">(Holsheimer and Siebes <a href="#ref-holsheimer1994data">1994</a>)</span>. De início não é utilizado nenhum modelo estatístico nela, apenas classifica os atributos de acordo com as amostras provenientes - precisa-se de cuidado ao seu uso pois tende a ser sensível com a amostra de treinamento. Mas atualmente utilizam técnicas estatísticas para aperfeiçoar o modelo e avaliar seus resultados <span class="citation">(Shiba et al. <a href="#ref-shiba2005classificaccao">2005</a>)</span>. Ela tem um bom desempenho quando há poucos atributos altamente relevantes, ao caso de complexidade no conjunto de dados poderá haver grandes dificuldades.</p>
<p>Geralmente utilizam-na para classificação (variáveis categóricas), mas também é possível para a Análise de Regressão (variáveis contínuas) que ja vimos. Para as árvores de regressão são utilizadas quando a variável dependente é contínua. O valor obtido pelos nós de término nos dados de treinamento é o valor médio das observações. Para classificação o obtido pelos nós de término nos dados de treinamento é a moda, ou seja, a observação mais recorrente no conjunto de dados. Ambos processos continuam o processo de divisão até atingir algum critério fornecido pelo pesquisador. Mas como fazemos esta divisão para podermos classificar?</p>
<p>A decisão de como fazer estas divisões dos nós pode influenciar muito na precisão do algoritmo e portanto, seus resultados. Pode-se utilizar pelo qui-quadrado, por ganho de informação, Índice de Gini, redução de variância, entre diversos outros.</p>
<p>Ao Índice de Gini, aplicado no sistema <em>Classification and Regression Trees</em> (CART)<span class="citation">(Breiman et al. <a href="#ref-breiman1984classification">1984</a>)</span>, mede a impureza de uma partição de dados, ela basicamente nos mostra que se selecionarmos aleatoriamente dois atributos de uma população ou amostra, ambos devem ter a mesma classe e a soma da probabilidade será se esta amostra/população for pura.</p>
<p>É utilizada portanto para variáveis categóricas como “Sucesso” e “Fracasso”, ou seja, aplica-se em divisões binárias. Quanto maior for este índice, mais homogêneo será. Foi criado como uma medida de variância para dados categóricos <span class="citation">(Light and Margolin <a href="#ref-light1971analysis">1971</a>)</span>, é expresso como:
<span class="math display">\[G(p_1,p_2,...,p_j)=\displaystyle \sum_{j}p_j  \sum_{i\neq j}\]</span>
<span class="math display">\[= \displaystyle \sum_j p_j(1-p_j) \]</span>
<span class="math display" id="eq:indgini1">\[\begin{equation}
= \displaystyle 1-\sum_j p^2_j
\tag{7.22}
\end{equation}\]</span></p>
<p>Para evitarmos problemas como <strong>overfitting</strong> por exemplo, que ocorre quando o seu modelo aprendeu tão bem as relações existentes dos conjuntos de dados para treino que acabou apenas decorando esses dados (será apresentado em <a href="valid.html#fitt">5.1</a>). Existe diversos meios que variam de acordo com propostas de diferentes pesquisas. Pode-se definir um número mínimo de amostras que são necessárias em um nó para se fazer a divisão, ou delimitar amostras mínimas para o nó terminal e o máximo de nós terminais, determir a profundidade máxima da árvore (o quanto ela vai ramificar e expandir-se), atentar ao quanto de recurso será utilizado para treinarmos o modelo e quanto irão para serem testados (visto que não pode ser o mesmo conjunto de dados para ambos pois ela apenas iria decorar e replicar).</p>
<p>A poda (pós-poda) de uma árvore é importante para que se verifique a melhor divisão e chegue até a condição de parada especificada. Um exemplo que gosto muito e creio que facilitará para a compreensão da poda <span class="citation">(ANALYTICS VIDHYA <a href="#ref-analytics">2016</a>)</span> : suponha que há duas pistas, você está em seu veículo verde na pista da direita com uma certa quantidade de carros em sua frente movendo a aproximados 80<span class="math inline">\(km/h\)</span> cada. Na pista da esquerda encontra-se dois caminhões de entrega de encomendas à apenas 30<span class="math inline">\(km/h\)</span> cada. Caso você vá pela esquerda irá alcançar o carro à frente, podendo passar até chegar atrás do caminhão e irá manter seu veículo a 30<span class="math inline">\(km/h\)</span> procurando desesperadamente encontrar alguma oportunidade de voltar para a direita. Entretando todos os outros carros ultrapassam você.</p>
<p>Seria uma ótima escolha caso você precisasse ultrapassá-los em poucos segundos. Mas a um prazo maior poderia ser uma escolha bem ruim. Esta é a diferença entre a árvore de decisão sem e com a poda. O algoritmo de árvore de decisão com restrições não irá visualizar os caminhões a frente e adotará o trajeto interessante naquele momento e iria optar pela esquerda. Porém quando utilizamos a poda, estamos observando alguns passos à frente antes de tomarmos decisões de que lado iríamos. Ao observar que para a esquerda é ruim, poda-se este galho.</p>
<div class="figure" style="text-align: center"><span id="fig:poda"></span>
<img src="Figuras/poda.png" alt="Desenho de veículos em uma pista. Adaptado de (ANALYTICS VIDHYA 2016)." width="70%" />
<p class="caption">
Figura 7.11: Desenho de veículos em uma pista. Adaptado de <span class="citation">(ANALYTICS VIDHYA <a href="#ref-analytics">2016</a>)</span>.
</p>
</div>

<p>Para que se faça a poda, inicia-se o algoritmo de árvore de decisão até uma grande profundidade em sua ramificação; analisa-se a parte inferior da árvore (os filhos e seus resultados) removendo folhas que estão dando retornos negativos quando comparadas ao topo (comparando o erro de cada nó e a soma dos erros dos nós descendentes, ou algum outro método estatístico similar). Tem também algoritmos de pré-poda que buscar não particionar mais o conjunto de treinamento com algum determinado critério como não ultrapassar a uma determinada variação de ganho de informação, parar se todas as instâncias pertencem à mesma classe, valores de atributos iguais, significância estatística, redução de erro, etc. Os algoritmo de “pós-poda” são mais lentos e pode haver um custo bem maior que o de “pré-poda”, mas são mais confiáveis.</p>
<p>Um dos cálculos mais utilizados para a poda da árvore é a <strong>taxa de erro</strong> que representa a razão entre o número de casos com classificação errada (<span class="math inline">\(c_e\)</span>) e o número de casos classificados corretamente (<span class="math inline">\(cc\)</span>) pela partição, caso a taxa de erro aumente conforme a ramificação, irá podar:</p>
<p><span class="math display" id="eq:txerro">\[\begin{equation}
E(T)=\frac{ce}{ce+cc}
\tag{7.23}
\end{equation}\]</span></p>
<p>Na seção</p>
<div id="extree" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Exemplos</h3>
<p>Aos exemplos de Árvore de Decisão, vamos tomar como base da literatura de <span class="citation">(ANALYTICS VIDHYA <a href="#ref-analytics">2016</a>)</span>.</p>
<p>Vamos supor uma amostra com 30 alunos com duas características cada: Sexo (meninos e meninas), Classe (I e II). Temos como propósito elaborar um modelo de árvore de decisão para prevermos quais alunos iriam jogar tênis durante o intervalo. Portanto precisamos classificar estes alunos com base nestas duas características. Supondo valores pré-estabelecidos dos alunos, a árvore segregará os alunos com base nestas variáveis e identificará a variável que cria os melhores conjuntos homogêneos e heterogêneos entre si.</p>
<div class="figure" style="text-align: center"><span id="fig:decision"></span>
<img src="Figuras/decision.jpg" alt="Divisão de alunos que jogam tênis, traduzido de (ANALYTICS VIDHYA 2016)." width="70%" />
<p class="caption">
Figura 7.12: Divisão de alunos que jogam tênis, traduzido de <span class="citation">(ANALYTICS VIDHYA <a href="#ref-analytics">2016</a>)</span>.
</p>
</div>

<ol style="list-style-type: decimal">
<li>Índice de Gini - Usando o Índice de Gini, vamos verificar qual divisão produz sub-nós mais impuros. Dentro de 10 meninas, apenas duas jogam tênis no intervalo, ou seja 20%; aos 20 meninos, 13 que equivale a 65%. Ressalta-se que os dados foram arredondados em duas casas para facilitar a exemplificação.</li>
</ol>
<p><span class="math display">\[\mbox{sub-nó Meninas}= 1 -(0,2^2+0,8^2)=0,320\]</span>
<span class="math display">\[\mbox{sub-nó Meninos}= 1 -(0,65^2+0,35^2)=0,455\]</span></p>
<p>A medição de impureza para as Meninas é 0,320. Como são duas possibilidades (Menina <em>versus</em> Menino), podemos dividir por 0,5 para uma compreensão mais intuitiva, obteremos 0,64. Caso fosse <span class="math inline">\(0,5 / 0,5 = 1\)</span>, significaria que o agrupamento é o mais impuro possível, pois é muito distribuído e não apenas uma variável predominante no conjunto amostral. Ao caso dos Meninos (0,455), obtemos um valor de 0,91 (bem impuro) ao dividirmos por 0,5.</p>
<p>Vamos analisar agora o valor de Gini dado que foi selecionado 10 meninas e 20 meninos de uma amostra de 30 alunos. Um valor ponderado:</p>
<p><span class="math display">\[Gini=\frac{10}{30}.(0,20^2+0,80^2)+\frac{20}{30}.(0,65^2+0,35^2)=0,59\]</span>
<span class="math display">\[\mbox{impureza: }1-0,59=0,41\]</span></p>
<p>Da mesma forma, calculamos para a classe I e II:</p>
<p><span class="math display">\[\mbox{sub-nó Classe I}= 1 -(0,43^2+0,57^2)=0,49\]</span>
<span class="math display">\[\mbox{sub-nó Classe II}= 1 -(0,56^2+0,44^2)=0,49\]</span></p>
<p>O valor ponderado:</p>
<p><span class="math display">\[Gini=\frac{14}{30}.(0,43^2+0,57^2)+\frac{16}{30}.(0,56^2+0,44^2)=0,51\]</span>
<span class="math display">\[\mbox{impureza: }1-0,51=0,49\]</span></p>
<p>Como Sexo possui um Índice de Gini maior que da Classe, ou uma impureza menor, o algoritmo irá fazer com que a divisão do nó ocorra em gênero. Caso houvesse mais variáveis como Altura, iria comparar entre as três. O de maior valor de Índice seria a nova referência de ramificação (se tornando um nó de divisão) caso haja outras características para mais ramificações.</p>
<ol start="2" style="list-style-type: decimal">
<li>Ganho de Informação - Inicialmente precisamos calcular a entropia do nó pai e em seguida calcular a entropia de cada nó individual da divisão e a média ponderada de todos os sub-nós disponíveis na divisão. Sabemos que de 30 alunos, 15 irão jogar tênis e 15 não, logo:</li>
</ol>
<p><span class="math display">\[\mbox{Entropia para o nó pai: } H(A)=- \sum p(A)log_2(p(A))\]</span>
<span class="math display">\[H(A)=-(\frac{15}{30}log_2(15/30)+\frac{15}{30}log_2(15/30))=1\]</span></p>
<p>Portanto o nó é totalmente impuro, o que faz sentido, pois ele é exatamente 50% dos dados distribuído em jogar tênis e outros 50% de não jogar no intervalo (maior distribuição possível entre as duas possibilidades).</p>
<p>Para o nó feminino, 2 que irão jogar e 8 que não irão jogar tênis num total de 10 meninas a entropia será <span class="math inline">\(- ((2/10) log_2 (2/10) + (8/10) log_2 (8/10)) = 0,72\)</span> e para nó masculino com 13 que irão e 7 não no total de 20 meninos temos: <span class="math inline">\(- ((13/20) log_2 (13/20) + (7/20) log_2 (7/20)) = 0,93\)</span>.</p>
<p>Portanto o Ganho de Informação será:</p>
<p><span class="math display">\[\mbox{Ganho de Informação}(D,T)=\mbox{entropia}(D)-\displaystyle \sum_{i=1}^k \frac{|D_i|}{|D|}. \mbox{entropia}(D_i)\]</span>
<span class="math display">\[\mbox{Entropia}_{pai}(D)-\sum_{i=1}^k peso. Entropia_{filho}\]</span>
<span class="math display">\[= 1-((10/30).0,72 + (20/30). 0,93) = 0,14\]</span></p>
<p>Do mesmo modo, vamos calcular em Classe I, <span class="math inline">\(- ((6/14) log_2 (6/14) + (8/14) log_2 (8/14)) = 0,99\)</span> e para nó Classe II, <span class="math inline">\(- ((9/16) log_2 (9/16) + (7/16) log_2 (7/16)) = 0,99\)</span>.</p>
<p>E o Ganho de Informação:</p>
<p><span class="math display">\[G.I = 1-((14/30) . 0,99 + (16/30) . 0,99) = 0,01\]</span>
Como o Ganho de Informação de Sexo é maior que o de Classe (menos impuro), a árvore irá iniciar sua ramificação a partir da característica Sexo. Caso houvesse mais variáveis como Altura, iria comparar entre as três o G.I e a de maior G.I seria considerado a nova “Pai” para que se possa recalcular caso haja outras características para mais ramificações.</p>
<ol start="3" style="list-style-type: decimal">
<li>Exemplo numérico - Adilson tem uma lanchonete e recebe cerca de 300 clientes por mês. Cada cliente gasta em média R$100,00. Uma concorrente vai abrir uma nova unidade próximo de seu estabelecimento, o que reduzirá seu número de clientes a não ser que Adilson amplie seu comércio. Considerando que está apertado financeiramente, está na dúvida se vale este investimento contabilizando num prazo de 5 meses. A análise foi: caso ele investisse R$40.000,00, as chances de ele obter 330 clientes por mês é de 30% e de obter 380 é 70%. Se Adilson não optar expandir, não irá gastar nada porém com a concorrente, irá ter uma probabilidade de 60% de atender 250 clientes por mês e de 40% de atender 290. Qual seria sua decisão, tomando como base o algoritmo de Árvore de Decisão?</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:decisionlanche"></span>
<img src="Figuras/decisionlanche.png" alt="Árvore de Decisão sobre o lucro da lanchonete." width="70%" />
<p class="caption">
Figura 7.13: Árvore de Decisão sobre o lucro da lanchonete.
</p>
</div>

<p>Pode-se calcular pelo Valor Esperado (Média). Para a primeira situação, gastando R$40.000,00, em 5 meses seu valor esperado será:
<span class="math display">\[V.E=(0,3\ .\ 125.000,00)+(0,7\ . \ 150.000,00)= 142.500,00\]</span></p>
<p>Ao segundo caso, em que Adilson não irá optar o investimento em seu estalecimento:
<span class="math display">\[V.E=(0,6\ .\ 125.000,00)+(0,4\ . \ 145.000,00)= 133.000,00\]</span></p>
<p>Portanto Adilson tem uma probabilidade maior de escolher investir os R$40.000,00 para ampliar sua lanchonete. Lembrando que foi utilizado o critério de médias, podendo haver diversos outros.</p>

</div>
</div>
<div id="AC" class="section level2">
<h2><span class="header-section-number">7.3</span> Análise de Componentes Principais</h2>
<p>A Análise de Componentens Principais, popularmente conhecida como ACP ou PCA (<em>Principal Component Analysis</em>), em inglês, foi introduzida por <span class="citation">Pearson (<a href="#ref-pearson1901liii">1901</a>)</span> e fundamentada no artigo de <span class="citation">Hotelling (<a href="#ref-hotelling1933analysis">1933</a>)</span>. É uma <strong>análise multivariada</strong> que tem como objetivo explicar a estrutura de variância e covariância de um vetor aleatório, composto por <span class="math inline">\(p\)</span>-variáveis aleatórias, através da construção de combinações lineares das variáveis originais que são chamadas de componentes principais e não correlacionadas entre si <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>. É uma técnica bastante utilizada em diversas áreas do conhecimento, como a biologia, a agronomia, a zootécnica, a ecologia, a engenharia florestal, a medicina, a economia, entre outras áreas. Muitos sugerem o seu uso quando o volume de dados ou variáveis é grande possibilitando reduzir a dimensão da matriz de dados que compõem o conjunto de variáveis resposta com apenas poucos componentes, ou seja, <span class="math inline">\(p\)</span> variáveis originais substituídas por <span class="math inline">\(k\)</span> (sendo <span class="math inline">\(k &lt; p\)</span>) componentes principais não correlacionadas.</p>
<p>Vamos supor um conjunto de dados em apenas duas dimensões <span class="math inline">\((x, y)\)</span> e que pode ser plotado em um plano cartesiano. Podemos verificar pelo seu comportamento que possuem alta correlação positiva.</p>
<div class="figure" style="text-align: center"><span id="fig:pca1"></span>
<img src="Figuras/pca1.png" alt="Gráfico bidimensional \(x\) por \(y\)." width="70%" />
<p class="caption">
Figura 7.14: Gráfico bidimensional <span class="math inline">\(x\)</span> por <span class="math inline">\(y\)</span>.
</p>
</div>

<p>Mas se quisermos descobrir a variação do conjunto de dados, o ACP busca encontrar um novo sistema de coordenadas em que cada ponto tem um novo valor <span class="math inline">\((x, y)\)</span>. Os eixos não representam algo físico, mas representam combinações de <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> que denominamos <strong>“componentes principais”</strong>, escolhidas para analisar a variação do eixo. Observe que rotacionamos o gráfico na Figura <a href="ptII.html#fig:pca2">7.15</a> e que após a ACP, podemos verificar a possibilidade de dercartar a componente referente ao eixo <span class="math inline">\(y\)</span>, visto que a componente do eixo <span class="math inline">\(x\)</span> explica 99,30% da variação total dos dados, ou seja, o primeiro componente tem uma maior dispersão (variância). Possibilitando pela componente principal do eixo <span class="math inline">\(x\)</span>, analisar e até mesmo classificar as observações, como por exemplo, a observação 1 e 2 como um conjunto e a 3, 4 e 5 como um segundo conjunto.</p>
<div class="figure" style="text-align: center"><span id="fig:pca2"></span>
<img src="Figuras/pca2.png" alt="Gráfico de \(x\) por \(y\) rotacionado." width="70%" />
<p class="caption">
Figura 7.15: Gráfico de <span class="math inline">\(x\)</span> por <span class="math inline">\(y\)</span> rotacionado.
</p>
</div>

<p>Com mais dimensões, o ACP torna-se ainda mais útil pois possibilita observarmos o conjunto de dados num melhor ângulo.</p>
<div class="figure" style="text-align: center"><span id="fig:pca3"></span>
<img src="Figuras/pca3.PNG" alt="Gráfico tridimensional, em Powell, Victor and Lehe, Lewis (2014)." width="70%" />
<p class="caption">
Figura 7.16: Gráfico tridimensional, em <span class="citation">Powell, Victor and Lehe, Lewis (<a href="#ref-powellpca">2014</a>)</span>.
</p>
</div>

<p>Portanto, a ACP assume que os dados originais estão representados por características (variáveis) correlacionadas com o objetivo de transformar essas variáveis em novas (componentes principais) por meio de mudança de base do espaço vetorial que não sejam correlacionadas entre si e que estas novas variáveis (menores que as originais) retenha a maior parte da variação apresentada pelas originais, tornando possível a classificação.</p>
<p>A suposição de normalidade não é requisito para sua técnica, mas ainda sim é conveniente padronizar (<a href="preprocesso.html#normpadro">4.2.2</a>) cada variável, permitindo que todas as variáveis tenham o mesmo peso para evitarmos viés de escala <span class="citation">(Hongyu, Sandanielo, and Oliveira Junior <a href="#ref-hongyu2016analise">2016</a>)</span>. A padronização das variáveis do vetor pelas respectivas médias e desvios padrões, gera novas variáveis centradas em zero e com variâncias iguais a 1. Assim, as componentes principais são determinadas a partir da matriz de covariâncias das variáveis originais padronizadas <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</p>
<p>Agora que sabemos o que é ACP, vamos apresentar alguns conceitos de Álgebra Linear e Estatísticas para compreendermos como é aplicado este método.</p>
<div id="autovalores-e-autovetores" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Autovalores e Autovetores</h3>
<p>Caso ainda não tenha muito contato com a Álgebra Linear, recomendo buscar algumas literaturas a respeito. Em <a href="dicio.html#dicio">3</a> encontra-se sobre Escalar, Vetores, Espaço Vetorial e Transformação Linear que serão tratadas neste tópico.</p>
<p>Dado uma matriz <span class="math inline">\(A_{mxn}\)</span> que define uma transformação linear (não muda sua dimensão), existem vetores onde sua orientação não é afetada por esta transformação, os <strong>autovetores</strong>. Na Figura <a href="ptII.html#fig:autovetor">7.17</a>, <span class="math inline">\(u\)</span> é um autovetor e <span class="math inline">\(V\)</span> não.</p>
<div class="figure" style="text-align: center"><span id="fig:autovetor"></span>
<img src="Figuras/autovetor.png" alt="\(u\) é um autovetor de \(T\), porém \(V\) não." width="70%" />
<p class="caption">
Figura 7.17: <span class="math inline">\(u\)</span> é um autovetor de <span class="math inline">\(T\)</span>, porém <span class="math inline">\(V\)</span> não.
</p>
</div>

<p>Um vetor é dito ser autovetor da matriz <span class="math inline">\(A_{mxn}\)</span> se a transformação linear deste vetor <span class="math inline">\(T(u)\)</span> é colinear a este vetor, ou seja, <span class="math inline">\(A_{mxn}\vec{u}=\lambda \vec{u}\)</span>. Sendo que <span class="math inline">\(\lambda\)</span> é um escalar e chamado de <strong>autovalor</strong> da matriz correspondente ao autovetor. Para encontrarmos o autovetor:
<span class="math display">\[A_{mxn}\vec{u}=\lambda \vec{u} \]</span>
<span class="math display">\[A_{mxn}\vec{u}-\lambda \vec{u}=0 \]</span>
<span class="math display" id="eq:autovetor">\[\begin{equation}
(A_{mxn}-\lambda l)\vec{u}=0
    \tag{7.24}
\end{equation}\]</span></p>
<p>esta equação tem solução trivial, ou seja, diferentes da nula <span class="math inline">\((\vec{v}\neq 0 )\)</span> se e somente se, seu determinante é zero. Conhecido como <strong>Equação caracterísica</strong> e sua solução são os <strong>autovalores</strong>:
<span class="math display" id="eq:eqcarac">\[\begin{equation}
    \mbox{Eq. Característica}\ \  det(A_{mxn}-\lambda l)=0  
    \tag{7.25}
\end{equation}\]</span></p>
<p>Note também que toda transformação linear (matriz) em um espaço
vetorial complexo (números imaginários) tem, pelo menos, um autovetor (real ou complexo).</p>
<div id="exautovetor1" class="section level4">
<h4><span class="header-section-number">7.3.1.1</span> Exemplo</h4>
<ol style="list-style-type: decimal">
<li>Vamos considerar um operador linear <strong><span class="math inline">\(T: R^2 \rightarrow R^2\)</span></strong>. Com <span class="math inline">\(T(x,y)=(4x+5y,2x+2y)\)</span>. Quais são os autovalores a matriz <span class="math inline">\(A=\begin{bmatrix} 4 &amp;5 \\ 2 &amp;2 \end{bmatrix}\)</span>?</li>
</ol>
<p>Vamos resolver a equação característica <span class="math inline">\(det(A_{mxn}-\lambda l)=0\)</span>.</p>
<p><span class="math display">\[det(A_{mxn}-\lambda l)=\begin{bmatrix}
4 &amp;5 \\ 
2 &amp;2 
\end{bmatrix} - \lambda \begin{bmatrix}
1 &amp;0 \\ 
0 &amp;1 
\end{bmatrix} = \begin{bmatrix}
4-\lambda &amp;5 \\ 
2 &amp;2-\lambda 
\end{bmatrix}\]</span></p>
<p>Com <span class="math inline">\(det(A_{mxn}-\lambda l)=0:\)</span>
<span class="math display">\[(4-\lambda)(2-\lambda)-10=0 \]</span>
<span class="math display">\[\lambda^2-6\lambda-2=0 \\ \mbox{resolvendo a equação: } \ 
\lambda_1 \approx 6,32 \  \ \mbox{e} \ \ \lambda_2 \approx -0,32\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Considere amatriz <span class="math inline">\(A=\begin{bmatrix} 1 &amp;0 \\ 1 &amp;-2 \end{bmatrix}\)</span>.</li>
</ol>
<p>Vamos resolver a equação característica <span class="math inline">\(det(A_{mxn}-\lambda l)=0\)</span>.</p>
<p><span class="math display">\[det(A_{mxn}-\lambda l)=\begin{bmatrix}
1 &amp;0 \\ 
1 &amp;-2 
\end{bmatrix} - \lambda \begin{bmatrix}
1 &amp;0 \\ 
0 &amp;1 
\end{bmatrix} = \begin{bmatrix}
1-\lambda &amp;0 \\ 
1 &amp;-2-\lambda 
\end{bmatrix}\]</span></p>
<p>Resovendo este sistema, obtemos os autovalores:
<span class="math display">\[\lambda_1=1 \ \ \lambda_2=-2\]</span>
Vamos encontrar agora seus respectivos autovetores, lembrando que <span class="math inline">\(A_{mxn}\vec{u}=\lambda \vec{u}\)</span>:
<span class="math display">\[ \mbox{Primeiro encontrar os autovetores de }\lambda_1=1:\]</span>
<span class="math display">\[A_{mxn}\vec{u}=\lambda \vec{u}\]</span>
<span class="math display">\[\begin{bmatrix}
1 &amp;0 \\ 
1 &amp;-2 
\end{bmatrix}. \begin{bmatrix}
x \\ 
y  
\end{bmatrix}=1.\begin{bmatrix}
x \\ 
y  
\end{bmatrix} \]</span>
<span class="math display">\[x=x \]</span>
<span class="math display">\[x-2y=y \]</span>
<span class="math display">\[\mbox{logo: } x=3y\]</span></p>
<p>Portanto em <span class="math inline">\(\lambda_1=1\)</span> será <span class="math inline">\(X=\begin{bmatrix}3y\\y\end{bmatrix}\)</span>, com o autovetor de <span class="math inline">\(y=1\)</span> e <span class="math inline">\(x=\begin{bmatrix}3\\1\end{bmatrix}\)</span>.</p>
<p>Agora para o segundo autovalor <span class="math inline">\(\lambda_2=-2\)</span>:
<span class="math display">\[A_{mxn}\vec{u}=\lambda \vec{u} \]</span>
<span class="math display">\[\begin{bmatrix}
1 &amp;0 \\ 
1 &amp;-2 
\end{bmatrix}. \begin{bmatrix}
x \\ 
y  
\end{bmatrix}=-2.\begin{bmatrix}
x \\ 
y  
\end{bmatrix} \]</span>
<span class="math display">\[x=-2x\]</span>
<span class="math display">\[x-2y=-2y \]</span>
<span class="math display">\[ \mbox{logo: } x=0\]</span></p>
<p>Portanto em <span class="math inline">\(\lambda_2=-2\)</span> será <span class="math inline">\(y=\begin{bmatrix}0\\1\end{bmatrix}\)</span> e <span class="math inline">\(x=0\)</span>.</p>
<p>Logo o primeiro autovetor será <span class="math inline">\(X=\begin{bmatrix}3\\1\end{bmatrix}\)</span> e o segundo <span class="math inline">\(y=\begin{bmatrix}0\\1\end{bmatrix}\)</span>.</p>
</div>
</div>
<div id="estatísticas" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Estatísticas</h3>
<p>Alguns conceitos de Estatísticas são fundamentais para que se entenda a ACP:</p>
<ul>
<li><strong>Covariância x Correlação:</strong> como apresentado em <a href="dicio.html#dicio">3</a>, a covariância é semelhante à correlação (ver <a href="preprocesso.html#normpadro">4.2.2</a>) entre duas variáveis, no entanto, elas diferem que os coeficientes de correlação são padronizados. Isso faz com que um relacionamento linear varie entre <span class="math inline">\(-1 \leq \rho \leq 1\)</span>. A correlação mede tanto a força como a direção da relação linear entre duas variáveis. Ao caso da covariância os valores não são padronizados. Assim, a covariância pode variar de <span class="math inline">\(-\infty \leq Cov (x,y) \leq \infty\)</span> demonstrando quanto <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> mudam juntas. Portanto o valor para uma relação linear ideal depende muito dos dados. Como os dados não são padronizados, é difícil determinar a força da relação entre as variáveis. Note que o coeficiente de correlação é uma função da covariância:</li>
</ul>
<p><span class="math display">\[\rho_{x,y}=\frac{cov(x,y)}{\sigma_x \sigma_y}\]</span></p>
<p>Uma covariância positiva sempre resulta em uma correlação positiva e uma covariância negativa sempre resulta em uma correlação negativa.</p>
<p>Quando temos um vetor de <span class="math inline">\(n\)</span> variáveis em vez de apenas duas, iremos obter uma matriz de covariâncias ou correlação. Contendo em sua diagonal a variância <span class="math inline">\(\sigma^2\)</span>, pois <span class="math inline">\(cov(x_i,x_i)=\sigma^2(x_i)\)</span>, por exemplo:</p>
<p><span class="math display">\[\begin{bmatrix}
cov_{1,1} &amp;cov_{1,2=2,1}  &amp;cov_{1,3=3,1} \\ 
cov_{1,1=2,1} &amp;cov_{2,2}  &amp;cov_{2,3=3,2} \\ 
cov_{3,1=1,3} &amp;cov_{2,3=3,2} &amp; cov_{3,3}
\end{bmatrix} = \begin{bmatrix}
var_{1} &amp;cov_{1,2=2,1}  &amp;cov_{1,3=3,1} \\ 
cov_{1,1=2,1} &amp;var_{2}  &amp;cov_{2,3=3,2} \\ 
cov_{3,1=1,3} &amp;cov_{2,3=3,2} &amp; var_{3}
\end{bmatrix}\]</span></p>
</div>
<div id="a-acp" class="section level3">
<h3><span class="header-section-number">7.3.3</span> A ACP</h3>
<p>Agora que compreendemos alguns conceitos importantes, podemos entender melhor a metodologia da ACP. Assumindo que os dados originais estão representados por variáveis correlacionadas (etapa de pré-processamento), ou seja, não independentes. Vamos ao objetivo de transformar essas <span class="math inline">\(p\)</span> variáveis em outras novas <span class="math inline">\(k\)</span> (com <span class="math inline">\(k&lt;p\)</span>) de ordem decrescente de variabilidade e que não sejam correlacionadas e que as primeiras novas variáveis retenham a maior parte da variação apresentadas pelas originais a fim de podermos classificá-los.</p>
<p>Dado um vetor <span class="math inline">\(X=(X_1,X_2,..., X_p)&#39;\)</span> aleatório de <span class="math inline">\(p\)</span> variáveis originais com vetor de médias <span class="math inline">\(\mu=(\mu_1,\mu_2,...,\mu_p)\)</span> e matriz de covariâncias <span class="math inline">\(A_{mxn}\)</span> e <span class="math inline">\(\lambda_1\geq\lambda_2\geq...\geq\lambda_p\)</span> os autovalores da matriz de covariâncias, com seus respectivos autovetores normalizados <span class="math inline">\(e_i&#39;=e_1,e_2,...,e_p\)</span>. O primeiro componente principal <span class="math inline">\(y_1\)</span>, como dito que deve ser ordem decrescente de variabilidade, será uma combinação linear do vetor aleatório <span class="math inline">\(X\)</span> de forma que a variância <span class="math inline">\(var(y_1)=\sigma^2_{y_{1}}\)</span> seja a máxima (maior possível), ou melhor, precisamos encontrar um vetor <span class="math inline">\(e_1\)</span> tal que <span class="math inline">\(y_1=(e_1)^T X\)</span> e <span class="math inline">\(var(y_1=(e_1)^T X\)</span> seja máxima. De mesmo modo para <span class="math inline">\(y_2\)</span> e um vetor <span class="math inline">\(e_2\)</span> e assim sucessivamente para <span class="math inline">\(p\)</span> variáveis em seu banco de dados.</p>
<p>Portanto a matriz dos autovetores normalizados da matriz de covariância <span class="math inline">\(A_{mxn}\)</span> é:</p>
<p><span class="math display" id="eq:autovetormatrix">\[\begin{equation}
    O_{mxn}= 
\begin{bmatrix}
e_{11} &amp;e_{21}  &amp;. &amp;.  &amp;. &amp;e_{p1} \\ 
e_{12} &amp;e_{22}  &amp;. &amp;.  &amp;. &amp;e_{p2} \\ 
.      &amp;.       &amp;. &amp;.  &amp;. &amp;. \\ 
.      &amp;.       &amp;. &amp;.  &amp;. &amp;. \\ 
.      &amp;.       &amp;. &amp;.  &amp;. &amp;. \\ 
e_{1p} &amp;e_{2p}  &amp;. &amp;.  &amp;. &amp;e_{pp} 
\end{bmatrix}
=[e_1,e_2,...,e_p]
\tag{7.26}
\end{equation}\]</span></p>
<p>E dos autovalores:
<span class="math display" id="eq:autovalormatrix">\[\begin{equation}
\Lambda_{mxn}=
\begin{bmatrix}
\lambda_1 &amp; &amp;0\\
&amp; \lambda_2 &amp; \\
0 &amp; &amp; \lambda_n
\tag{7.27}
\end{bmatrix}
\end{equation}\]</span></p>
<p>Portanto, as variáveis aleatórias que constituem o vetor <span class="math inline">\(Y\)</span> não são correlacionadas entre si. Com isso, a ideia de utilizar combinações lineares em <span class="math inline">\(Y\)</span> como forma de representar a estrutura de covariâncias do vetor <span class="math inline">\(X\)</span> torna-se interessante, a fim de reduzir o espaço de variáveis de <span class="math inline">\(p\)</span> para <span class="math inline">\(k&lt;p\)</span> dimensões. Os vetores <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> terão a mesma variância total e generalizada, com <span class="math inline">\(Y\)</span> de vantagem de não haver variáveis correlacionadas e facilitando na interpretação conjunta delas (análise multivariada). Portanto:</p>
<p><span class="math display" id="eq:cp">\[\begin{equation}
   Y_j=e_j&#39;X=e_{j1}X_1+e_{j2}X_2+...+e_{jp}X_p
   \tag{7.28}
\end{equation}\]</span></p>
<p>A esperança e variância:
<span class="math display">\[\begin{equation}
E[Y_j]=e_j&#39;\mu=e_{j1}\mu_1+e_{j2}\mu_2+...+e_{jp}\mu_p
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}     
Var[Y_j]=e_j&#39; A_{mxn}e_j=\lambda_j
\end{equation}\]</span></p>
<p>Lembrando que <span class="math inline">\(Cov[Y_j,Y_k]=0, \ j\neq k\)</span> e que cada autovalor <span class="math inline">\(\lambda_j\)</span> representa a variância de uma componente principal <span class="math inline">\(Y_j\)</span>. A primeira componente terá a maior variabilidade e a última menor.</p>
<p>Para calcularmos a correlação estimada entre a <span class="math inline">\(j\)</span>-ésima componente principal e a variável aleatória <span class="math inline">\(X\)</span>, podemos expressar:
<span class="math display">\[\begin{equation}   
r_{Y_j,X_i}=\frac{e_{ji}\sqrt{\lambda_j}}{\sqrt{\sigma}}
\end{equation}\]</span></p>
<p>De mesmo modo para tratarmos de amostras, são trabalhados com <span class="math inline">\(\hat{Y_j}\)</span> e <span class="math inline">\(\hat{X_j}\)</span> e seus respectivos, autovetores, autovalores, matriz de covariância amostral e correlação.</p>
<p>Os maiores autovalores são os que orientam o sinal, os demais podem ser descartados. Porém quantos componentes principais devemos utilizar? Precisamos verificar a proporção da variação total dos dados originais que uma componente pode explicar, a partir disso selecionarmos. Lembrando que cada autovalor <span class="math inline">\(\lambda_i\)</span> refere-se a <span class="math inline">\(var(y_i)\)</span>.</p>
<p>Para calcularmos a variação total, expressa-se pela somatória de todos os autovalores:
<span class="math display" id="eq:vartot">\[\begin{equation}
    \displaystyle \sum_j \lambda_j 
    \tag{7.29}
\end{equation}\]</span></p>
<p>Portanto, para analisar cada <span class="math inline">\(i\)</span> componente, ou seja, cada autovalor (variação “explicada” por cada componente):
<span class="math display" id="eq:varind">\[\begin{equation}
    p_i=\frac{\lambda_j}{\displaystyle \sum_j \lambda_j} 
    \tag{7.30}
\end{equation}\]</span></p>
<p>Sendo geralmente escolhido as componentes com seus respectivos autovalores que explicam entre 70%-90% segundo alguns pesquisadores. Outros como <span class="citation">Kaiser (<a href="#ref-kaiser1960application">1960</a>)</span>, propõe aceitar, observando diretamente, somente os autovalores iguais ou superiores à unidade.</p>
<p><strong>Importante:</strong> sobre utilizar matriz de covariância ou de correlação depende muito das fundamentações teóricas e recomendaçõesdos pesquisadores. Em geral, utiliza-se a matriz de correlação (quando padronizamos e elaboramos a matriz) ao caso de padronizar escalas distintas que podem viesar, como por exemplo, medidas de distância e de peso.</p>
<p>Caso esteja utilizando software para a análise, dependendo do software utilizado com seu determinado modelo de formulação de componentes principais, pode ocorrer essa troca de sinal que nada mais é do que uma reflexão em relação ao eixo, uma rotação em seu espaço vetorial n-dimensional em torno da origem, poderá ocasionar uma “rotação” em torno do eixo. Tratando de algebra linear e suas combinações lineares, a combinação poderá possuir soluções diferentes que diferem apenas o sinal.</p>
<p><strong>Resumo geral:</strong> É comum o pesquisador trabalhar com um volume de dados muito grande e que estão muitas vezes correlacionadas. A Análise de Componentes principais busca explicar a estrutura de variância e covariância de um vetor aleatório com <span class="math inline">\(p\)</span> variáveis, possibilitando por meio da combinação linear deste vetor aleatório novas componentes (denominada componente principal) com menos variáveis (<span class="math inline">\(k&lt;p\)</span>) que o conjunto de dados original e não correlacionadas de modo que estas componentes principais retenha a maior parte da variação apresentada pelas originais, possibilitando classificarmos o conjunto de dados e até mesmo descartar variáveis que podem ser redundantes ou não importantes. Utiliza-se de fundamentações teóricas de Autovetores e Autovalores para que se encontre um novo sistema de coordenadas com novos pontos a partir das originais, pode-se dizer que rotacionamos para que se visualize num “novo ângulo”, para descobrir e avaliar em ordem decrescente a variação (matriz de covariância do vetor aleatório) deste conjunto de dados. Os passos:</p>
<ol style="list-style-type: decimal">
<li><p>Calcular a Matriz de Correlação amostral <span class="math inline">\(R_{mxn}\)</span> ou Covariância amostral <span class="math inline">\(S_{mxn}\)</span> do vetor aleatório de <span class="math inline">\(p\)</span> variáveis.</p></li>
<li><p>Encontrar <span class="math inline">\(\lambda_i\)</span> autovalores da matriz.</p></li>
<li><p>Encontrar seus respectivos <span class="math inline">\(e_i\)</span> autovetores.</p></li>
<li><p>Aplicar outras análises caso necessite (como correlação da componente e a variável) , interpretar os dados e selecionar as novas variáveis.</p></li>
</ol>
</div>
<div id="exemplocp" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Exemplos</h3>
<p>Tomando como base exemplos de <span class="citation">Mingoti (<a href="#ref-mingoti2007analise">2007</a>)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Matriz de covariância amostral</strong></p>
<p>A Tabela apresenta dados relativos as 12 empresas no que se refere a 3 variáveis (medidas em unidades monetárias): ganho bruto (<span class="math inline">\(X1\)</span>), ganho líquido (<span class="math inline">\(X2\)</span>) e o patrimônio acumulado (<span class="math inline">\(X_3\)</span>):</p></li>
</ol>
<table>
<thead>
<tr class="header">
<th><strong>Empresas</strong></th>
<th align="center"><strong>Ganho Bruto(<span class="math inline">\(X_1\)</span>)</strong></th>
<th align="center"><strong>Ganho Líquido(<span class="math inline">\(X_2\)</span>)</strong></th>
<th align="center"><strong>Patrimônio Líquido(<span class="math inline">\(X_3\)</span>)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>E1</td>
<td align="center">9893</td>
<td align="center">564</td>
<td align="center">17689</td>
</tr>
<tr class="even">
<td>E2</td>
<td align="center">8776</td>
<td align="center">389</td>
<td align="center">17359</td>
</tr>
<tr class="odd">
<td>E3</td>
<td align="center">13572</td>
<td align="center">1103</td>
<td align="center">18597</td>
</tr>
<tr class="even">
<td>E4</td>
<td align="center">6455</td>
<td align="center">743</td>
<td align="center">8745</td>
</tr>
<tr class="odd">
<td>E5</td>
<td align="center">5129</td>
<td align="center">203</td>
<td align="center">14397</td>
</tr>
<tr class="even">
<td>E6</td>
<td align="center">5432</td>
<td align="center">215</td>
<td align="center">3467</td>
</tr>
<tr class="odd">
<td>E7</td>
<td align="center">3807</td>
<td align="center">385</td>
<td align="center">4679</td>
</tr>
<tr class="even">
<td>E8</td>
<td align="center">3423</td>
<td align="center">187</td>
<td align="center">6754</td>
</tr>
<tr class="odd">
<td>E9</td>
<td align="center">3708</td>
<td align="center">127</td>
<td align="center">2275</td>
</tr>
<tr class="even">
<td>E10</td>
<td align="center">3294</td>
<td align="center">297</td>
<td align="center">6754</td>
</tr>
<tr class="odd">
<td>E11</td>
<td align="center">5433</td>
<td align="center">432</td>
<td align="center">5589</td>
</tr>
<tr class="even">
<td>E12</td>
<td align="center">6287</td>
<td align="center">451</td>
<td align="center">8972</td>
</tr>
</tbody>
</table>
<p>Após calcularmos suas covariâncias (recomendo o leitor calcular e verificar e atentar que por ser exemplificação, passível de ocorrência de arrendondamento dos valores), obtemos a matriz de covariância amostral:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>Ganho Bruto (<span class="math inline">\(X_1\)</span>)</strong></th>
<th align="center"><strong>Ganho Líquido (<span class="math inline">\(X_2\)</span>)</strong></th>
<th align="center"><strong>Patrimônio Líquido (<span class="math inline">\(X_3\)</span>)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Ganho Bruto (<span class="math inline">\(X_1\)</span>)</strong></td>
<td align="center">9550608,6</td>
<td align="center">706121,1</td>
<td align="center">14978232,5</td>
</tr>
<tr class="even">
<td align="center"><strong>Ganho Líquido (<span class="math inline">\(X_2\)</span>)</strong></td>
<td align="center">706121,1</td>
<td align="center">76269,5</td>
<td align="center">933915,1</td>
</tr>
<tr class="odd">
<td align="center"><strong>Patrimônio Líquido (<span class="math inline">\(X_3\)</span>)</strong></td>
<td align="center">14978232,5</td>
<td align="center">933915,1</td>
<td align="center">34408113,0</td>
</tr>
</tbody>
</table>
<p>Para calcularmos os autovalores:</p>
<p><span class="math display">\[det(A_{mxn}-\lambda l)=0\]</span>
<span class="math display">\[\begin{bmatrix}
9550608,6 -\lambda &amp;706121,1 &amp;14978232,5\\ 
706121,1 &amp;76269,5-\lambda &amp; 933915,1 \\
14978232,5&amp;933915,1&amp;34408113,0-\lambda
\end{bmatrix}=0\]</span></p>
<p>Resolvendo o sistema, obtemos os seguintes autovalores das componentes principais:
<span class="math display">\[\lambda_1=38018192,2 \ \ \lambda_2=2327881,5 \ \ \lambda_3=19334,8\]</span>
Para encontrarmos a porcentagem da variância explicada por cada auto valor:
<span class="math display">\[\%\lambda_1=\frac{38018192,2}{38018192,2+2327881,5+19334,8}.100\%=94,2\% \]</span> <span class="math display">\[\%\lambda_2=\frac{2327881,5}{38018192,2+2327881,5+19334,8}.100\%=5,77\% \]</span> <span class="math display">\[\%\lambda_3=\frac{19334,8}{38018192,2+2327881,5+19334,8}.100\%=0,048\%\]</span>
Portanto, podemos descartar o segundo e o terceiro componente principal, pois o primeiro explica cerca de <span class="math inline">\(94,2\%\)</span>.</p>
<p>Por fim os autovetores podem sem calculados:</p>
<p><span class="math display">\[A_{mxn}\vec{u}=\lambda \vec{u}\]</span>
Com <span class="math inline">\(A_{mxn}\)</span> a matriz de covariância amostral, <span class="math inline">\(u\)</span> o autovetor e <span class="math inline">\(\lambda\)</span> os respectivos autovalores dos autovetores.</p>
<p><span class="math display">\[\begin{bmatrix}
\vec{u_1}\\ \vec{u_2} \\ \vec{u_3}
\end{bmatrix}
\begin{bmatrix}
9550608,6  &amp;706121,1 &amp;14978232,5\\ 
706121,1 &amp;76269,5 &amp; 933915,1 \\
14978232,5&amp;933915,1&amp;34408113,0
\end{bmatrix}  = \lambda_i \begin{bmatrix}
u_1\\ u_2 \\ u_3
\end{bmatrix} \]</span></p>
<p><span class="math display">\[\mbox{substituindo os autovalores:}\]</span>
<span class="math display">\[\begin{bmatrix}
u_1\\ u_2 \\ u_3
\end{bmatrix}
\begin{bmatrix}
9550608,6  &amp;706121,1 &amp;14978232,5\\ 
706121,1 &amp;76269,5 &amp; 933915,1 \\
14978232,5&amp;933915,1&amp;34408113,0
\end{bmatrix}  = \begin{bmatrix}0,942&amp;0&amp;0 \\ 0&amp;0,0577&amp;0 \\0&amp;0&amp; 0,0048 \end{bmatrix}\begin{bmatrix}
u_1\\ u_2 \\ u_3
\end{bmatrix}\]</span></p>
<p>Teremos os autovetores:</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>Autovetor Ganho Bruto (<span class="math inline">\(u_1\)</span>)</strong></th>
<th align="center"><strong>Autovetor Ganho Líquido (<span class="math inline">\(u_2\)</span>)</strong></th>
<th align="center"><strong>Autovetor Patrimônio Líquido (<span class="math inline">\(u_3\)</span>)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Autovetor Ganho Bruto (<span class="math inline">\(u_1\)</span>)</strong></td>
<td align="center">0,425</td>
<td align="center">0,900</td>
<td align="center">-0,099</td>
</tr>
<tr class="even">
<td align="center"><strong>Autovetor Ganho Líquido (<span class="math inline">\(u_2\)</span>)</strong></td>
<td align="center">0,028</td>
<td align="center">0,096</td>
<td align="center">0,995</td>
</tr>
<tr class="odd">
<td align="center"><strong>Autovetor Patrimônio Líquido (<span class="math inline">\(u_3\)</span>)</strong></td>
<td align="center">0,905</td>
<td align="center">-0,426</td>
<td align="center">0,016</td>
</tr>
</tbody>
</table>
<p>Com os autovetores, podemos elaborar as três componentes principais:</p>
<p><span class="math display">\[\hat{y_1}=0,425(Ganho Bruto)+0,028(GanhoLíquido)+0,905(PatrimônioLíquido)\]</span>
<span class="math display">\[\hat{y_2}=0,900(Ganho Bruto)+0,096(GanhoLíquido)-0,429(PatrimônioLíquido)\]</span>
<span class="math display">\[\hat{y_3}=-0,099(Ganho Bruto)+0,995(GanhoLíquido)+0,016(PatrimônioLíquido)\]</span></p>
<p>Determinada as componentes principais, podemos obter seus valores numéricos (<strong>escores</strong>) para cada elemento amostral. Basicamente substituímos os valores originais nas funções encontradas de componentes principais (<span class="math inline">\(\hat{y_1},\hat{y_2} \ \mbox{e}\  \hat{y_3}\)</span>):</p>
<table>
<thead>
<tr class="header">
<th><strong>Empresas</strong></th>
<th align="center"><strong><span class="math inline">\(CP_1\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(CP_2\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(CP_3\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>E1</td>
<td align="center">8857,59</td>
<td align="center">-165,27</td>
<td align="center">-90,18</td>
</tr>
<tr class="even">
<td>E2</td>
<td align="center">8079,36</td>
<td align="center">-1046,65</td>
<td align="center">-158,93</td>
</tr>
<tr class="odd">
<td>E3</td>
<td align="center">11257,93</td>
<td align="center">2810,25</td>
<td align="center">96,18</td>
</tr>
<tr class="even">
<td>E4</td>
<td align="center">-690,80</td>
<td align="center">566,19</td>
<td align="center">284,23</td>
</tr>
<tr class="odd">
<td>E5</td>
<td align="center">3844,09</td>
<td align="center">-3084,94</td>
<td align="center">-30,40</td>
</tr>
<tr class="even">
<td>E6</td>
<td align="center">-5915,42</td>
<td align="center">1841,62</td>
<td align="center">-224,93</td>
</tr>
<tr class="odd">
<td>E7</td>
<td align="center">-5504,97</td>
<td align="center">-119,93</td>
<td align="center">124,81</td>
</tr>
<tr class="even">
<td>E8</td>
<td align="center">-3796,38</td>
<td align="center">-1367,83</td>
<td align="center">-0,64</td>
</tr>
<tr class="odd">
<td>E9</td>
<td align="center">-7729,15</td>
<td align="center">789,46</td>
<td align="center">-160,88</td>
</tr>
<tr class="even">
<td>E10</td>
<td align="center">-3848,18</td>
<td align="center">-1473,28</td>
<td align="center">121,59</td>
</tr>
<tr class="odd">
<td>E11</td>
<td align="center">-3989,16</td>
<td align="center">960,15</td>
<td align="center">25,13</td>
</tr>
<tr class="even">
<td>E12</td>
<td align="center">-564,92</td>
<td align="center">290,23</td>
<td align="center">14,02</td>
</tr>
</tbody>
</table>
<p>Podemos observar que a empresa E9 possui o menor desempenho, e as E1, E2 e E3 os melhores. Entenda que não necessariamente o sinal de negativo é sempre ser um pior valor, isso depende da pesquisa e da interpretação do sinal ou como em caso de autovetores, indica a rotação. Para analisarmos por gráfico não é recomendável utilizar neste caso, devido que são valores bem grandes para serem inseridos. No caso de Matriz de correlação, que serão padronizados os dados, podemos visualizar melhor.</p>
<p>E a correlação entre as componentes principais e as variáives originais:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>CP 1</strong></th>
<th align="center"><strong>CP 2</strong></th>
<th align="center"><strong>CP 3</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Ganho Bruto (<span class="math inline">\(X_1\)</span>)</strong></td>
<td align="center">0,8859</td>
<td align="center">0,4639</td>
<td align="center">-0,0047</td>
</tr>
<tr class="even">
<td align="center"><strong>Ganho Líquido (<span class="math inline">\(X_2\)</span>)</strong></td>
<td align="center">0,6450</td>
<td align="center">0,5569</td>
<td align="center">0,5232</td>
</tr>
<tr class="odd">
<td align="center"><strong>Patrimônio Líquido (<span class="math inline">\(X_3)\)</span>)</strong></td>
<td align="center">0,9933</td>
<td align="center">-0,1156</td>
<td align="center">0,0004</td>
</tr>
</tbody>
</table>
<p>Por meio da observação de seus resultados podemos analisar que:</p>
<ul>
<li><p>A primeira componente possui alta correlação-positiva com todas as três variáveis, podemos analisar como um índice de desempenho global da empresa. Pelo autovetor, podemos ver que o patrimônio possui o maior peso e de menor o ganho líquido. Podemos verificar que quanto maior for os valores das variáveis, maior será dessa componente, ou melhor, maior será o desempenho global da empresa. Esta ocupa, observando pelos autovalores, 94,\20% de toda variação explicada, dependendo da pesquisa pode-se descartar as outras componentes.</p></li>
<li><p>A segunda componente que ocupa 5,77% de toda variação explicada (autovalor), possui o ganho bruto e patrimônio de maior variância amostral (analisando o tabela de covariância amostra). Pelos autovetores, podemos verificar que o ganho bruto é a variável dominante com segunda maior variância amostral. Com a componente próximo a zero, entende-se que haverá um certo equilíbrio entre ganho bruto e patrimônio acumulado, o que na verdade o aumento do ganho bruto eleva-se esta componente e o patrimônio contrário. Note que há correlação bem menor entre elas.</p></li>
<li><p>A terceira componente com pouca variância total explicada, referente ao ganho líquido de menor variância amostral, possui pouca importância. Apena o ganho líquido possui alta correlação, visto que às outras duas são próximas de zero.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Matriz de correlação</strong></li>
</ol>
<p>No exemplo anterior, vimos que as componentes principais foram obtidas a partir de matriz de covariâncias e que são influenciadas pelas variáives com maior variância. Porém em casos onde existe muita discrepância entre essas variâncias por motivos de unidades de medidas distintas entre as variáveis. Podemos amenizar essa discrepância por meio de transformação dos dados originais de modo a equilibrar as variâncias ou colocar todos os dados em mesma escala de medida. Uma muito usual é a padronização que gera novas variáveis centradas em zero e com variâncias iguais a 1. Em caso de dúvida, reveja em <a href="preprocesso.html#normpadro">4.2.2</a>. Tomando como base o mesm conjunto de dados do exercício anterior, padronizando e elaborando a matriz de correlação amostral, obtemos:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>Ganho Bruto (<span class="math inline">\(X_1\)</span>)</strong></th>
<th align="center"><strong>Ganho Líquido (<span class="math inline">\(X_2\)</span>)</strong></th>
<th align="center"><strong>Patrimônio Líquido (<span class="math inline">\(X_3\)</span>)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Ganho Bruto (<span class="math inline">\(X_1\)</span>)</strong></td>
<td align="center">1,00</td>
<td align="center">0,827</td>
<td align="center">0,826</td>
</tr>
<tr class="even">
<td align="center"><strong>Ganho Líquido (<span class="math inline">\(X_2\)</span>)</strong></td>
<td align="center">0,827</td>
<td align="center">1,00</td>
<td align="center">0,576</td>
</tr>
<tr class="odd">
<td align="center"><strong>Patrimônio Líquido (<span class="math inline">\(X_3\)</span>)</strong></td>
<td align="center">0,826</td>
<td align="center">0,576</td>
<td align="center">1,00</td>
</tr>
</tbody>
</table>
<p>Com o mesmo procedimento do exemplo anterior ao caso de matriz de covariância, obtemos os respectivos autovalores e autovetores:</p>
<p><span class="math display">\[\lambda_1=2,493 \ \ \lambda_2=0,423 \ \ \lambda_3=0,084 \]</span>
<span class="math display">\[\%\lambda_1=83,084\% \ \ \%\lambda_2=14,117\% \ \ \%\lambda_3=2,799\%\]</span></p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>Autovetor Ganho Bruto (<span class="math inline">\(u_1\)</span>)</strong></th>
<th align="center"><strong>Autovetor Ganho Líquido (<span class="math inline">\(u_2\)</span>)</strong></th>
<th align="center"><strong>Autovetor Patrimônio Líquido (<span class="math inline">\(u_3\)</span>)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Autovetor Ganho Bruto (<span class="math inline">\(u_1\)</span>)</strong></td>
<td align="center">0,617</td>
<td align="center">-0,001</td>
<td align="center">-0,787</td>
</tr>
<tr class="even">
<td align="center"><strong>Autovetor Ganho Líquido (<span class="math inline">\(u_2\)</span>)</strong></td>
<td align="center">0,557</td>
<td align="center">-0,706</td>
<td align="center">0,437</td>
</tr>
<tr class="odd">
<td align="center"><strong>Autovetor Patrimônio Líquido (<span class="math inline">\(u_3\)</span>)</strong></td>
<td align="center">0,556</td>
<td align="center">0,708</td>
<td align="center">0,435</td>
</tr>
</tbody>
</table>
<p>Por meio dos autovalores, podemos verificar que a variância total explicada pela primeira componente é aproximadamente 83,1%, pela segunda 14,1% e pela terceira 2,8%. As duas primeiras componentes explicam juntas 97,2% aproximadamente da variância total do vetor original padronizado. Note que o processo de análise é da mesma forma que o exemplo anterior. A primeira componente é um índice de desempenho global padronizado da empresa. A segunda componente representa uma comparação entre ganho líquido e patrimônio padronizados (verifique pelo autovetor da segunda componente que o ganho bruto possui um valor de coeficiente muito pequeno em relação aos outros). Por fim, a terceira componente compara-se o ganho bruto com às outras duas variáveis.</p>
<p>Suas componentes principais são:</p>
<p><span class="math display">\[\hat{y_1}=0,617(Ganho Bruto)+0,557(GanhoLíquido)+0,556(PatrimônioLíquido)\]</span>
<span class="math display">\[\hat{y_2}=-0,001(Ganho Bruto)-0,706(GanhoLíquido)+0,708(PatrimônioLíquido)\]</span>
<span class="math display">\[\hat{y_3}=-0,787(Ganho Bruto)+0,437(GanhoLíquido)+0,435(PatrimônioLíquido)\]</span></p>
<p>Note que em relação ao exemplo anterior, seus coeficientes de ponderação estão numericamente mais equilibrados que no caso de matriz de covariâncias amostral. Todas as variâncias iguais a um, sem dominância direta de nenhuma variável.</p>
<p>Determinada as componentes principais, podemos obter seus valores numéricos (escores) para cada elemento amostral. Podendo ser obtidas com técnicas estatísticas usuais como análise de variância e análise de regressão, entre outras. Usando dados nas três componentes principais, obtemos:</p>
<table>
<thead>
<tr class="header">
<th><strong>Empresas</strong></th>
<th align="center"><strong><span class="math inline">\(CP_1\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(CP_2\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(CP_3\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>E1</td>
<td align="center">1,85</td>
<td align="center">0,65</td>
<td align="center">-0,11</td>
</tr>
<tr class="even">
<td>E2</td>
<td align="center">1,22</td>
<td align="center">1,07</td>
<td align="center">-0,13</td>
</tr>
<tr class="odd">
<td>E3</td>
<td align="center">3,84</td>
<td align="center">-0,68</td>
<td align="center">-0,13</td>
</tr>
<tr class="even">
<td>E4</td>
<td align="center">0,62</td>
<td align="center">-0,96</td>
<td align="center">0,41</td>
</tr>
<tr class="odd">
<td>E5</td>
<td align="center">-0,23</td>
<td align="center">1,20</td>
<td align="center">0,31</td>
</tr>
<tr class="even">
<td>E6</td>
<td align="center">-1,22</td>
<td align="center">-0,21</td>
<td align="center">-0,60</td>
</tr>
<tr class="odd">
<td>E7</td>
<td align="center">-1,08</td>
<td align="center">-0,51</td>
<td align="center">0,21</td>
</tr>
<tr class="even">
<td>E8</td>
<td align="center">-1,38</td>
<td align="center">0,28</td>
<td align="center">0,14</td>
</tr>
<tr class="odd">
<td>E9</td>
<td align="center">-1,89</td>
<td align="center">-0,13</td>
<td align="center">-0,38</td>
</tr>
<tr class="even">
<td>E10</td>
<td align="center">-1,17</td>
<td align="center">-0,02</td>
<td align="center">0,36</td>
</tr>
<tr class="odd">
<td>E11</td>
<td align="center">-0,56</td>
<td align="center">-0,53</td>
<td align="center">0,08</td>
</tr>
<tr class="even">
<td>E12</td>
<td align="center">0,00</td>
<td align="center">0,15</td>
<td align="center">-0.01</td>
</tr>
</tbody>
</table>
<p>Da mesma forma que o exemplo anterior é importante reforçar novamente que pode haver troca de sinal de acordo com a formulação pelo software e a sua rotação em torno do eixo, visto que tratando de combinações lineares pode haver soluções com sinais diferentes. Pode resultar em valores numéricos diferentes e o pesquisador deve atentar em sua interpretação dos resultados obtidos. Pelos resultados nos leva a verificar que a empresa E9 possui o menor desempenho, e as E1, E2 e E3 os melhores.</p>
<p>Note que se compararmos os Escores do primeiro exemplo com matriz de covariância amostral e os Escores do exemplo com matriz de correlação amostral, foram concordantes a indicação das três empresas com melhor desempenho global e na de pior desempenho. Porém, algumas discordaram em algumas posições. Houve 5 concordância em 12 classificações (41,7%). Como as componentes principais foram obtidas pela decomposição espectral de matrizes diferentes, houve esta diferença. E que a matriz de correlação amostral leva em consideração a média do conjunto de 12 empresas de cada variável original, a matriz de covariância amostral não. Importante notar que neste caso, as primeiras componentes obtidas possuem a mesma interpretação, verificando consistência nas análises de ambos os métodos.</p>
<p>Com valores padronizados, fica mais fácil a visualizaçãor e interpretação da ACP por um gráfico, que denominamos de biplot. Basicamente colocamos no eixo X a primeira componente e no eixo Y a segunda componente principal (as que mais explicam a variância total). No qual as observações em análise são os Escores das observações (no caso as empresas) e os respectivos vetores das variáveis (Ganho Bruto, Ganho Líquido e Patrimônio Líquido).</p>
<div class="figure" style="text-align: center"><span id="fig:biplotpadro"></span>
<img src="Figuras/biplotpadro.png" alt="Gráfico de biplot, em X a primeira componente principal e Y a segunda componente." width="70%" />
<p class="caption">
Figura 7.18: Gráfico de biplot, em X a primeira componente principal e Y a segunda componente.
</p>
</div>

<p>A primeira componente principal (Dim1), as variáveis que se referem ao patrimônio, ganho bruto e ganho líquido possuem cargas positivas (todas tendendo fortemente à direita), reafirmando a análise anterior de que a primeira componente refere-se ao desempenho global padronizado da empresa. Com E1, E2 e E3 os maiores em desempenho.
Para a segunda componente principal (Dim2), note que Patrimônio e Ganho Líquido estão em sentidos opostos, o que reafirma nossa análise anterior de ser uma comparação entre elas.</p>

</div>
</div>
<div id="análise-de-agrupamentos---clusters" class="section level2">
<h2><span class="header-section-number">7.4</span> Análise de Agrupamentos - <em>Clusters</em></h2>
<p>A Análise de Agrupamentos, também conhecido como Análise de Conglomerados, classificação ou <em>Clusters</em>, tem como propósito dividir os elementos de uma amostra (ou população) em grupos de modo que os elementos pertencentes a estes grupos tenham características similares entre si e heterôgenos com os outros grupos <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>. Esta classificação vai de acordo com a medida e o método de classificação. Este tipo de análise é muito comum seu uso em diversas áreas como segmentação de clientes de acordo com perfis de consumo <span class="citation">(Punj and Stewart <a href="#ref-punj1983cluster">1983</a>)</span>, perfis de personalidade em psicologia <span class="citation">(Speece, McKinney, and Appelbaum <a href="#ref-speece1985classification">1985</a>)</span>, classificação de cidades, etc. Para o agrupamento de <em>clusters</em>, tomaremos como base a literatura de <span class="citation">Mingoti (<a href="#ref-mingoti2007analise">2007</a>)</span>.</p>
<p>É muito importante o critério que o pesquisador utilizará para delimitar até que ponto os elementos podem ser considerados semelhantes em suas características ou não, por isso precisa-se de medidas apropriadas para classificar. Cada elemento amostral têm informações de <span class="math inline">\(p\)</span> variáveis dentro de um vetor e por meio de medidas matemáticas, como as medidas de distância pode ser possível compararmos as observações dentro de seu banco de dados. Calculando a distância entre os vetores das observações da amostra e agrupando de acordo com suas distância (agrupar os de menores distâncias entre si). Aqui entramos com a aplicação de Medida de Distância, em <a href="dicio.html#meddist">3.3.3</a>. Quaquer medida de distância pode ser utilizada em variáveis quantitativas pode ser transformada num coeficiente de similaridade <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</p>
<p>A Análise de Clusters são frequentemente classificadas em <strong>Hierárquicas (aglomerativas e divisivas)</strong> que comumente utilizada para identificar possíveis agrupamentos e um provável valor da quantitade de grupos <span class="math inline">\(g\)</span> e <strong>Não hierárquicas</strong> que necessita um número de grupos pré-estabelecido pelo pesquisados que a aplica.</p>
<div class="figure" style="text-align: center"><span id="fig:hierarq"></span>
<img src="Figuras/hierarq.PNG" alt="Esquema geral de procedimentos hierárquicos aglomerativos e divisivos (Mingoti 2007)." width="70%" />
<p class="caption">
Figura 7.19: Esquema geral de procedimentos hierárquicos aglomerativos e divisivos <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.
</p>
</div>

<div id="técnicas-hierárquicas-aglomerativas" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Técnicas Hierárquicas Aglomerativas</h3>
<p>Nesta técnica, inicia-se com <span class="math inline">\(n\)</span> conglomerados como se cada elemento do banco e dados fosse um conglomerado isolado. No algoritmo, a cada passo os elementos amostrais vão sendo agrupados, formando novos conglomerados até que todos os elementos considerados estejam num único grupo. No início, tratando-se de variabilidade, tem-se a partição de menor dispersão interna ja que todos os conglomerados possui amenas um único elemento (variância <span class="math inline">\(\sigma^2\)</span> zero). Ao final dos estágios, encontra-se a maior dispersão interna possível, pois todos os elementos amostrais estão num único <em>cluster</em> <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>. Conforme <span class="citation">Mingoti (<a href="#ref-mingoti2007analise">2007</a>)</span>, os passos fundamentais são:</p>
<ol style="list-style-type: decimal">
<li><p>Cada elemento possui um <em>cluster</em> de tamanho 1, logo <span class="math inline">\(n\)</span> <em>clusters</em>;</p></li>
<li><p>Em cada estágio do algoritmo de agrupamento, os pares de conglomerados mais “similares” vão combinando-se passando a constituir um único conglomerado. Apenas um novo conglomerado pode ser formada a cada passo, ou seja, a cada etapa o número de conglomerados irá diminuir;</p></li>
<li><p>Como mostra-se na Figura <a href="ptII.html#fig:hierarq">7.19</a>, em cada estágio do algoritmo, cada novo conglomerado formado é um agrupamento de conglomerados de estágios anteriores. Se dois elementos amostrais aparecem juntos num mesmo <em>cluster</em> em alguma etapa, estarão juntos em todos os outros;</p></li>
<li><p>Por estarmos trabalhando com conglomerados em hierarquia, podemos construir um gráfico denominado Dendograma, ou Dendrograma, que representa a história do agrupamento. É um gráfico em forma de árvore tal que a escala vertical indica o nível de similaridade (ou dissimilaridade) e na horizontal os elementos amostrais em ordem relacionada à história do agrupamento. Sua altura representa ao nível em que os elementos foram considerados semelhantes entre si (distância do agrupamento ou nível de similaridade).</p></li>
</ol>
<p>Existe alguns métodos para que se escolha o número final dos grupos <span class="math inline">\(g\)</span>, mas em geral é subjetivo com base em fundamentações empíricas. Vamos agora para os métodos mais comuns e utilizados em muitos <em>softwares</em> estatísticos.</p>
<div id="método-de-ligaçao-simples-simple-linkage" class="section level4">
<h4><span class="header-section-number">7.4.1.1</span> Método de Ligaçao Simples (<em>Simple Linkage</em>)</h4>
<p>A similaridade entre dois conglomerados é definida pelos dois elementos mais parecidos entre si <span class="citation">(Sneath <a href="#ref-sneath1957application">1957</a>)</span>. Por exemplo, num determinado momento do algoritmo, encontra-se dois grupos: <span class="math inline">\(C_1=\{X_1,X_3,X_7\}\)</span> e <span class="math inline">\(C_2=\{X_2,X_6\}\)</span>. A distância entre esses dois grupos será definida por:</p>
<p><span class="math display" id="eq:ligsimples">\[\begin{equation}
    d(C_1,C+2)=min\{d(X_l,X_k, l\neq k, l=1,3,7 \ \ \mbox{e} \ \ k=2,6\}
    \tag{7.31}
\end{equation}\]</span></p>
<p>é a distância entre os vizinhos mais próximos (elementos mais parecidos com os conglomerados. Em cada estágio, os dois conglomerados que são mais similares com relação à distância são combinados em um únicos <em>cluster</em>. Como ilustrado abaixo, a distância entre 6 e 1 caracteriza a distância entre os grupos, pelo método de ligação simples.</p>
<div class="figure" style="text-align: center"><span id="fig:ligsimples"></span>
<img src="Figuras/ligsimples.png" alt="Método de ligação simples, adaptado de Mingoti (2007)." width="70%" />
<p class="caption">
Figura 7.20: Método de ligação simples, adaptado de <span class="citation">Mingoti (<a href="#ref-mingoti2007analise">2007</a>)</span>.
</p>
</div>

<p>Vamos aproveitar o exemplo que utilizamos de distância Euclidiana, em <a href="dicio.html#disteuclidana">3.3.3.1</a>. Com o seguinte banco de dados:</p>
<table>
<caption><span id="tab:dadossrenda">Tabela 7.2: </span> Renda e Idade de 6 indíviduos, abordado em <a href="dicio.html#disteuclidana">3.3.3.1</a> sobre distância Euclidiana <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span></caption>
<thead>
<tr class="header">
<th align="center"><strong>Renda</strong></th>
<th align="center">9,6</th>
<th align="center">8,4</th>
<th align="center">2,4</th>
<th align="center">18,2</th>
<th align="center">3,9</th>
<th align="center">6,4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Idade</strong></td>
<td align="center">28</td>
<td align="center">31</td>
<td align="center">42</td>
<td align="center">38</td>
<td align="center">25</td>
<td align="center">41</td>
</tr>
</tbody>
</table>
<p>A matriz de distância calculada entre os seis elementos amostrais é dada por:
<span class="math display">\[D_{6x6}=\begin{bmatrix}\\
 &amp;A&amp;B&amp;C&amp;D&amp;E&amp;F \\
 A&amp;0&amp;&amp;&amp;&amp;&amp;\\
 B&amp;3,23&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 15,74&amp; 12,53&amp;0&amp;&amp;&amp;\\
 D&amp; 13,19&amp; 12,04&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,44&amp; 7,50&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 13,39&amp; 10,19&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Sabemos que o menor valor observado na Matriz é 3,23 (distância entre os elementos A e B) nas duas variáveis medidas. Portanto este dois indivíduos são aglomerados fazendo com que a amostra de seis elementos passe a ser cinco. Lembrando que permanece as distâncias mínimas dos elementos com o novo conglomerado <span class="math inline">\(\{A,B\}\)</span> pois queremos os mais próximos.
<span class="math display">\[d(\{A,B\},\{C\})=min(d\{A,C\},\{B,C\})=min(\{15,74\},\{12,53\})=12,53\]</span>
<span class="math display">\[d(\{A,B\},\{D\})=min(d\{A,D\},\{B,D\})=min(\{13,19\},\{12,04\})=12,04\]</span>
<span class="math display">\[d(\{A,B\},\{E\})=min(d\{A,E\},\{B,E\})=min(\{6,44\},\{7,50\})=6,44\]</span>
<span class="math display">\[d(\{A,B\},\{F\})=min(d\{A,F\},\{B,F\})=min(\{13,39\},\{10,19\})=10,19\]</span></p>
<p><span class="math display">\[D_{5x5}=\begin{bmatrix}\\
 &amp;\{A,B\}&amp;C&amp;D&amp;E&amp;F \\
 \{A,B\}&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 12,53&amp;0&amp;&amp;&amp;\\
 D&amp; 12,04&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,44&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 10,19&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Nesta nova matriz, será a distância entre C e F (4,12), da mesma forma que anteriormente, fazendo com que fique quatro grupos:</p>
<p><span class="math display">\[D_{4x4}=\begin{bmatrix}\\
 &amp;\{A,B\}&amp; \{C,F\}&amp;\{D\}&amp;\{E\} \\
 \{A,B\}&amp;0&amp;&amp;\\
 \{C,F\} &amp; 10,19&amp;0&amp;&amp;\\
 \{D\}&amp; 12,04&amp; 12,18&amp;0&amp;\\
 \{E\}&amp; 6,44&amp; 16,19&amp; 19,33&amp;0\\
\end{bmatrix}\]</span></p>
<p>Agora a menor distância encontra-se entre <span class="math inline">\(\{A,B\}\)</span> e <span class="math inline">\(\{E\}\)</span> com 6,44:
<span class="math display">\[D_{3x3}=\begin{bmatrix}\\
 &amp;\{A,B,E\}&amp; \{C,F\}&amp;\{D\} \\
 \{A,B,E\}&amp;0&amp;\\
 \{C,F\} &amp; 10,19&amp;0&amp;\\
 \{D\}&amp; 12,04&amp; 12,18&amp;0\\
\end{bmatrix}\]</span>
O próximo valor mínimo será 10,19 sobrando <span class="math inline">\(C_1=\{A,B,E,C,F\}\)</span> e <span class="math inline">\(C_2=\{D\}\)</span> e por fim reduz-se um único <em>cluster</em> <span class="math inline">\(C_1=\{A,B,C,D,E,F\}\)</span> com o nível de junção igual a 12,04.</p>
<p>Portanto, o o histórico do agrupamento e seu respectivo dendograma será:</p>
<table>
<caption><span id="tab:ligsimple">Tabela 7.3: </span> Histórico do agrupamento por meio da Ligação Simples.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Passo</strong></th>
<th align="center"><strong>Número de Grupos</strong></th>
<th align="center"><strong>Fusão</strong></th>
<th align="center"><strong>Distância</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">5</td>
<td align="center">{A} e {B}</td>
<td align="center">3,23</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">{C} e {F}</td>
<td align="center">4,12</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">3</td>
<td align="center">{A,B} e {E}</td>
<td align="center">6,44</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">2</td>
<td align="center">{A,B,E} e {C,F}</td>
<td align="center">10,19</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">1</td>
<td align="center">{A,B,E,C,F} e {D}</td>
<td align="center">12,04</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:dendsimples"></span>
<img src="Figuras/dendsimples.png" alt="Dendograma do agrupamento. Método de ligação simples." width="70%" />
<p class="caption">
Figura 7.21: Dendograma do agrupamento. Método de ligação simples.
</p>
</div>

</div>
<div id="método-de-ligaçao-completa-complete-linkage" class="section level4">
<h4><span class="header-section-number">7.4.1.2</span> Método de Ligaçao Completa (<em>Complete Linkage</em>)</h4>
<p>A similaridade entre dois conglomerados é definida pelos elementos que são menos semelhantes entre si <span class="citation">(Sneath <a href="#ref-sneath1957application">1957</a>)</span>. Por exemplo, vamos considerar os conjuntos <span class="math inline">\(C_1=\{X_1,X_3,X_7\}\)</span> e <span class="math inline">\(C_2=\{X_2,X_6\}\)</span>. A distância entre eles então será:</p>
<p><span class="math display" id="eq:ligcompleta">\[\begin{equation}
d(C_1,C_2)=max \{d(X_l,X_k, l\neq k,l=1,3,7 \ \ \mbox{e} \ \ k=2,6\}
 \tag{7.32}
\end{equation}\]</span></p>
<p>Em cada estágio calcula-se para os pares de grupos, combinando num único aqueles que estiverem com o menor valor da distância. Segue abaixo uma ilustração.</p>
<div class="figure" style="text-align: center"><span id="fig:ligcompleta"></span>
<img src="Figuras/ligcompleta.png" alt="Método de ligação completa, adaptado de Mingoti (2007)." width="70%" />
<p class="caption">
Figura 7.22: Método de ligação completa, adaptado de <span class="citation">Mingoti (<a href="#ref-mingoti2007analise">2007</a>)</span>.
</p>
</div>

<p>Aproveitando o mesmo exemplo que utilizamos no método anterior, sobre a distância Euclidiana, em <a href="dicio.html#disteuclidana">3.3.3.1</a>. A matriz de distância calculada entre os seis elementos amostrais é dada por:
<span class="math display">\[D_{6x6}=\begin{bmatrix}\\
 &amp;A&amp;B&amp;C&amp;D&amp;E&amp;F \\
 A&amp;0&amp;&amp;&amp;&amp;&amp;\\
 B&amp;3,23&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 15,74&amp; 12,53&amp;0&amp;&amp;&amp;\\
 D&amp; 13,19&amp; 12,04&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,44&amp; 7,50&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 13,39&amp; 10,19&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Sabemos que o menor valor observado na Matriz é 3,23 (distância entre os elementos A e B) nas duas variáveis medidas. Portanto este dois indivíduos são aglomerados fazendo com que a amostra de seis elementos passe a ser cinco. Lembrando que permanece as distâncias máximas dos elementos com o novo conglomerado <span class="math inline">\(\{A,B\}\)</span> pois queremos os mais afastados.
<span class="math display">\[d(\{A,B\},\{C\})=max(d\{A,C\},\{B,C\})=min(\{15,74\},\{12,53\})=15,74\]</span>
<span class="math display">\[d(\{A,B\},\{D\})=max(d\{A,D\},\{B,D\})=max(\{13,19\},\{12,04\})=13,19\]</span>
<span class="math display">\[d(\{A,B\},\{E\})=max(d\{A,E\},\{B,E\})=max(\{6,44\},\{7,50\})=7,50\]</span>
<span class="math display">\[d(\{A,B\},\{F\})=max(d\{A,F\},\{B,F\})=max(\{13,39\},\{10,19\})=13,39\]</span></p>
<p><span class="math display">\[D_{5x5}=\begin{bmatrix}\\
 &amp;\{A,B\}&amp;C&amp;D&amp;E&amp;F \\
 \{A,B\}&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 15,74&amp;0&amp;&amp;&amp;\\
 D&amp; 13,19&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 7,50&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 13,39&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Nesta nova matriz, será a distância entre C e F (4,12), da mesma forma que anteriormente, fazendo com que fique quatro grupos e analisando pela máxima:</p>
<p><span class="math display">\[D_{4x4}=\begin{bmatrix}\\
 &amp;\{A,B\}&amp; \{C,F\}&amp;\{D\}&amp;\{E\} \\
 \{A,B\}&amp;0&amp;&amp;\\
 \{C,F\} &amp; 15,74&amp;0&amp;&amp;\\
 \{D\}&amp; 13,19&amp; 16,29&amp;0&amp;\\
 \{E\}&amp; 7,50&amp; 17,06&amp; 19,33&amp;0\\
\end{bmatrix}\]</span></p>
<p>Agora a menor distância encontra-se entre <span class="math inline">\(\{A,B\}\)</span> e <span class="math inline">\(\{E\}\)</span> com 7,50:
<span class="math display">\[D_{3x3}=\begin{bmatrix}\\
 &amp;\{A,B,E\}&amp; \{C,F\}&amp;\{D\} \\
 \{A,B\}&amp;0&amp;\\
 \{C,F\} &amp; 17,06&amp;0&amp;\\
 \{D\}&amp; 19,33&amp; 16,29&amp;0\\
\end{bmatrix}\]</span></p>
<p>O próximo valor mínimo será 16,29, unindo os grupos <span class="math inline">\(\{C,F\}\)</span> e <span class="math inline">\(\{D\}\)</span>, tornando os conglomerados <span class="math inline">\(C_1=\{A,B,E\}\)</span> e <span class="math inline">\(C_2=\{C,F,D\}\)</span> e por fim a distância máxima entre os dois grupos é 19,33.</p>
<p>Portanto, o o histórico do agrupamento por meio da Ligação Completa e seu respectivo dendograma será:</p>
<table>
<caption><span id="tab:ligcomplet">Tabela 7.4: </span> Histórico do agrupamento por meio da Ligação Completa.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Passo</strong></th>
<th align="center"><strong>Número de Grupos</strong></th>
<th align="center"><strong>Fusão</strong></th>
<th align="center"><strong>Distância</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">5</td>
<td align="center">{A} e {B}</td>
<td align="center">3,23</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">{C} e {F}</td>
<td align="center">4,12</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">3</td>
<td align="center">{A,B} e {E}</td>
<td align="center">7,5</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">2</td>
<td align="center">{C,F} e {D}</td>
<td align="center">16,29</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">1</td>
<td align="center">{A,B,E} e {C,F}</td>
<td align="center">19,33</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:dendcomplete"></span>
<img src="Figuras/dendcomplete.png" alt="Dendograma do agrupamento. Método de ligação completa." width="70%" />
<p class="caption">
Figura 7.23: Dendograma do agrupamento. Método de ligação completa.
</p>
</div>

</div>
<div id="método-da-média-das-distâncias-average-linkage" class="section level4">
<h4><span class="header-section-number">7.4.1.3</span> Método da Média das Distâncias (<em>Average Linkage</em>)</h4>
<p>Neste caso a distância entre os conglomerados é com base nas médias. Se <span class="math inline">\(C_1\)</span> tem <span class="math inline">\(n_1\)</span> elementos e <span class="math inline">\(C_2\)</span> tem <span class="math inline">\(n_2\)</span> elementos, a distância será expressa como:</p>
<p><span class="math display" id="eq:ligmedia">\[\begin{equation}
d(C_1,C_2)=\displaystyle \sum_{l\epsilon C_1} \sum_{k\epsilon C_2} \frac{1}{n_1 n_2} d(X_l,X_k)
 \tag{7.33}
\end{equation}\]</span></p>
<p>Portanto, a distância entre <span class="math inline">\(C_1=\{X_1,X_3,X_7\}\)</span> e <span class="math inline">\(C_2=\{X_2,X_6\}\)</span> será:</p>
<p><span class="math display">\[d(C_1,C_2=\frac{1}{6}[d(X_1,X_2)+d(X_l,X_6)+d(X_3,X_2)+d(X_3,X_6)+d(X_7,X_2)+d(X_7,X_6)]\]</span>
Vamo considerar novamente a matriz inicial como exemplificação:</p>
<p><span class="math display">\[D_{6x6}=\begin{bmatrix}\\
 &amp;A&amp;B&amp;C&amp;D&amp;E&amp;F \\
 A&amp;0&amp;&amp;&amp;&amp;&amp;\\
 B&amp;3,23&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 15,74&amp; 12,53&amp;0&amp;&amp;&amp;\\
 D&amp; 13,19&amp; 12,04&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,44&amp; 7,50&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 13,39&amp; 10,19&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Sabemos que o menor valor observado na Matriz é 3,23 (distância entre os elementos A e B) nas duas variáveis medidas. Portanto este dois indivíduos são aglomerados fazendo com que a amostra de seis elementos passe a ser cinco. Dessa vez são calculados em relação às médias das distância dos conglomerados <span class="math inline">\(\{A,B\}\)</span> com os outros.
<span class="math display">\[d(\{A,B\},\{C\})=[d\{A,C\}+\{B,C\}]/2=[\{15,74\}+\{12,53\}]/2=14,13\]</span>
<span class="math display">\[d(\{A,B\},\{D\})=[d\{A,D\}+\{B,D\}]/2=[\{13,19\}+\{12,04\}]/2=12,62\]</span>
<span class="math display">\[d(\{A,B\},\{E\})=[d\{A,E\}+\{B,E\}]/2=[\{6,44\}+\{7,50\}]/2=6,97\]</span>
<span class="math display">\[d(\{A,B\},\{F\})=[d\{A,F\}+\{B,F\}]/2=[\{13,39\}+\{10,19\}]/2=11,79\]</span></p>
<p><span class="math display">\[D_{5x5}=\begin{bmatrix}\\
 &amp;\{A,B\}&amp;C&amp;D&amp;E&amp;F \\
 \{A,B\}&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 14,13&amp;0&amp;&amp;&amp;\\
 D&amp; 16,62&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,97&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 11,79&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>A próxima distância será entre C e F (4,12) novamente, fazendo com que fique quatro grupos. Repetindo o processo pela média.:</p>
<p><span class="math display">\[D_{4x4}=\begin{bmatrix}\\
 &amp;\{A,B\}&amp; \{C,F\}&amp;\{D\}&amp;\{E\} \\
 \{A,B\}&amp;0&amp;&amp;\\
 \{C,F\} &amp; 12,96&amp;0&amp;&amp;\\
 \{D\}&amp; 12,62&amp; 14,24&amp;0&amp;\\
 \{E\}&amp; 6,97&amp; 16,62&amp; 16,19&amp;0\\
\end{bmatrix}\]</span></p>
<p>Atente-se ao cálculo das distâncias médias, como por exemplo <span class="math inline">\(\{A,B\}\)</span> e <span class="math inline">\(\{C,F\}\)</span> foram quatros valores: <span class="math inline">\(d(\{A,B\},\{C,F\})=[d(A,C)+d(A,F)+d(B,C)+d(B,F)]/4\)</span>.</p>
<p>Agora, teremos <span class="math inline">\(\{A,B\}\)</span> e <span class="math inline">\(\{E\}\)</span> com 6,97:</p>
<p><span class="math display">\[D_{3x3}=\begin{bmatrix}\\
 &amp;\{A,B,E\}&amp; \{D\}&amp;\{C,F\} \\
 \{A,B\}&amp;0&amp;\\
 \{D\} &amp; 14,85&amp;0&amp;\\
 \{E\}&amp; 14,18&amp; 14,24&amp;0\\
\end{bmatrix}\]</span></p>
<p>O próximo valor mínimo será 14,18, unindo os grupos <span class="math inline">\(\{A,B,E\}\)</span> e <span class="math inline">\(\{C,F\}\)</span>, tornando os conglomerados <span class="math inline">\(C_1=\{A,B,C,E,F\}\)</span> e <span class="math inline">\(C_2=\{D\}\)</span> e por final será 14,61 a distância entre eles.</p>
<p>Portanto, o o histórico do agrupamento e seu respectivo dendograma será:</p>
<table>
<caption><span id="tab:ligmedia">Tabela 7.5: </span> Histórico do agrupamento por meio da Ligação Média.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Passo</strong></th>
<th align="center"><strong>Número de Grupos</strong></th>
<th align="center"><strong>Fusão</strong></th>
<th align="center"><strong>Distância</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">5</td>
<td align="center">{A} e {B}</td>
<td align="center">3,23</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">{C} e {F}</td>
<td align="center">4,12</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">3</td>
<td align="center">{A,B} e {E}</td>
<td align="center">6,97</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">2</td>
<td align="center">{A,B,E} e {C,F}</td>
<td align="center">14,19</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">1</td>
<td align="center">{A,B,E,C,F} e {D}</td>
<td align="center">14,61</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:dendaverage"></span>
<img src="Figuras/dendaverage.png" alt="Dendograma do agrupamento. Método da média das distâncias." width="70%" />
<p class="caption">
Figura 7.24: Dendograma do agrupamento. Método da média das distâncias.
</p>
</div>

</div>
<div id="método-do-centróide-centroid-method" class="section level4">
<h4><span class="header-section-number">7.4.1.4</span> Método do Centróide (<em>Centroid Method</em>)</h4>
<p>A distância entre dois grupos é medida com a distância entre os vetores de médias, também denominado como centróides. Com <span class="math inline">\(C_1=\{X_1,X_3,X_7\}\)</span> e <span class="math inline">\(C_2=\{X_2,X_6\}\)</span>, pode-se calcular:</p>
<p><span class="math display">\[\mbox{vetor de médias de} C_1=\overline{X_1}=\frac{1}{3}[X_1+X_3+X_7] \]</span>
<span class="math display">\[\mbox{vetor de médias de} C_2=\overline{X_2}=\frac{1}{2}[X_2+X_6]\]</span>
E a distância entre <span class="math inline">\(C_1\)</span> e <span class="math inline">\(C_2\)</span>, é a distância Euclidiana ao quadrado entre os vetores de médias amostral (também pode ser usado com a usual entre os vetores):</p>
<p><span class="math display" id="eq:distcentroide">\[\begin{equation}
d(C_1,C_2)= (\overline{X_1}-\overline{X_2})&#39;(\overline{X_1}-\overline{X_2})
 \tag{7.34}
\end{equation}\]</span></p>
<p>Para cada passo do algoritmo, os conglomerados que possuem o menor valor de distância são agrupados. Vamos manter a matriz euclidiana (podemos manter a usual ou utilizar calcular as distâncias euclidianas ao quadrado)</p>
<p><span class="math display">\[D_{6x6}=\begin{bmatrix}\\
 &amp;A&amp;B&amp;C&amp;D&amp;E&amp;F \\
 A&amp;0&amp;&amp;&amp;&amp;&amp;\\
 B&amp;3,23&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 15,74&amp; 12,53&amp;0&amp;&amp;&amp;\\
 D&amp; 13,19&amp; 12,04&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,44&amp; 7,50&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 13,39&amp; 10,19&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>Lembrando que os valores originais das variáveis são:</p>
<table>
<caption><span id="tab:dadosrenda">Tabela 7.6: </span> Renda e Idade de 6 indíviduos, abordado em <a href="dicio.html#disteuclidana">3.3.3.1</a> sobre distância Euclidiana <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Renda</strong></th>
<th align="center">9,6</th>
<th align="center">8,4</th>
<th align="center">2,4</th>
<th align="center">18,2</th>
<th align="center">3,9</th>
<th align="center">6,4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Idade</strong></td>
<td align="center">28</td>
<td align="center">31</td>
<td align="center">42</td>
<td align="center">38</td>
<td align="center">25</td>
<td align="center">41</td>
</tr>
</tbody>
</table>
<p>Temos o menor o valor de 3,23 (distância entre os elementos A e B) nas duas variáveis medidas. Portanto este dois indivíduos são aglomerados fazendo com que a amostra de seis elementos passe a ser cinco. Lembrando que agora calcula-se a média entre os vetores, já que queremos o centróide. Em <span class="math inline">\(\{A,B\}\)</span> obtemos:</p>
<p><span class="math display">\[\{A,B\}=(\frac{9,6+8,4}{2});(\frac{28+31}{2})=(9;29,5)\]</span>
Portanto calculando a distância com a nova coordenada <span class="math inline">\(\{A,B\}\)</span>:</p>
<p><span class="math display">\[d(\{A,B\},\{C\})=\sqrt{(9-2,4)^2+(29,5-42)^2}=14,135\]</span>
<span class="math display">\[d(\{A,B\},\{D\})=\sqrt{(9-18,2)^2+(29,5-38)^2}=12,525\]</span>
<span class="math display">\[d(\{A,B\},\{E\})=\sqrt{(9-3.9)^2+(29,5-25)^2}=6,800\]</span>
<span class="math display">\[d(\{A,B\},\{F\})=\sqrt{(9-6,4)^2+(29,5-41)^2}=11,700\]</span></p>
<p><span class="math display">\[D_{5x5}=\begin{bmatrix}\\
 &amp;\{A,B\}&amp;C&amp;D&amp;E&amp;F \\
 \{A,B\}&amp;0&amp;&amp;&amp;&amp;\\
 C &amp; 14,13&amp;0&amp;&amp;&amp;\\
 D&amp; 12,52&amp; 16,29&amp;0&amp;&amp;\\
 E&amp; 6,80&amp; 17,06&amp; 19,33&amp;0&amp;\\
 F&amp; 11,70&amp; 4,12&amp; 12,18&amp; 16,19&amp;0 \\
\end{bmatrix}\]</span></p>
<p>A próxima distância será entre C e F (4,12) novamente, fazendo com que fique quatro grupos. Repetindo o processo sucessivamente de retornar aos dados originais, recalcular identificar o novo menor valor (o que dependendo do banco de dados exige tempo computacional) chegaremos aos seguintes resultados:</p>
<table>
<caption><span id="tab:ligcent">Tabela 7.7: </span> Histórico do agrupamento pelo Método de Centróide.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Passo</strong></th>
<th align="center"><strong>Número de Grupos</strong></th>
<th align="center"><strong>Fusão</strong></th>
<th align="center"><strong>Distância</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">5</td>
<td align="center">{A} e {B}</td>
<td align="center">3,2</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">{C} e {F}</td>
<td align="center">4,1</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">3</td>
<td align="center">{A,B} e {E}</td>
<td align="center">6,8</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">2</td>
<td align="center">{A,B,E} e {C,F}</td>
<td align="center">13,8</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">1</td>
<td align="center">{A,B,E,C,F} e {D}</td>
<td align="center">12,9</td>
</tr>
</tbody>
</table>
<p>Lembrando que dependendo do arredondamento, pode haver pequena variação e que é possível fazer com o quadrado da distância euclidiana. Note também que o nível de fusão no passo 5 foi menor que o do passo 4. É possível esta ocorrência no método de centróide pois em algum passo do algoritmo de agrupamento houver empates entre valores da matriz de distâncias, quanto maior for o número de elementos amostrais, menor será a chance dessa ocorrência <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>. A partição do dendograma será muito parecida com os anteriores, não será necessário apresentar.</p>
</div>
<div id="método-de-ward-wards-method" class="section level4">
<h4><span class="header-section-number">7.4.1.5</span> Método de Ward (<em>Ward’s Method</em>)</h4>
<p>Vimos nos métodos anteriores que ao aumentarmos o estágio <span class="math inline">\(k\)</span> para <span class="math inline">\(k+1\)</span>, a qualidade a partição decresce (com excessão de centróide) pois o nivel de fusão e portanto o nível de similaridade também. Então percebe-se que a variação entre grupos diminui e a variação dentro dos grupos aumenta. <span class="citation">Ward Jr (<a href="#ref-ward1963hierarchical">1963</a>)</span> propôs um método fundamental na mudança de variação entre os grupos em formação e entre cada passo do processo de agrupamento. É também conhecido como “mínima variância” por ter como objetivo a minimização da soma de quadrados dentro dos grupos.</p>
<p>Inicialmente cada elemento é considerado como um único elemento conglomerado e em cada passo do algoritmo de agrupamento é calculada a soma de quadrados dentro de cada conglomerado. Portanto, o agrupamento é feito a partir das somas de quadrados dos desvios entre acessos ou do quadrado da distância Euclidiana.</p>
<p><span class="math display" id="eq:sumquadraeuclid">\[\begin{equation}
 SS_i=\displaystyle \sum^{n_i}_{j=1} (X_{ij}-\overline{X_i})&#39;(X_{ij}-\overline{X_i})
 \tag{7.35}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(n_i\)</span> é o número de elementos no conglomerado <span class="math inline">\(C_i\)</span>, quando se está no passo <span class="math inline">\(k\)</span>; <span class="math inline">\(X_i\)</span> é o vetor de observações do <span class="math inline">\(j\)</span>-ésimo elemento amostral e pertence ao <span class="math inline">\(i\)</span>-ésimo conglomerado; <span class="math inline">\(\overline{X_i}\)</span>, o centróide do conglomerado; e <span class="math inline">\(SS_i\)</span>, a soma de quadrados correspondente do conglomerado <span class="math inline">\(C_i\)</span>. No passo <span class="math inline">\(k\)</span> então, a soma de quadrados total é expressa como:</p>
<p><span class="math display" id="eq:sumquadrak">\[\begin{equation}
 SS_i=\displaystyle \sum^{g_k}_{i=1}SS_i
 \tag{7.36}
\end{equation}\]</span></p>
<p>onde <span class="math inline">\(g_k\)</span> é o valor de grupos existentes quando se está no passo <span class="math inline">\(k\)</span>.
A Distância entre os conglomerados <span class="math inline">\(C_l\)</span> e <span class="math inline">\(C_i\)</span> é definida pela soma dos quadrados entre os <em>clusters</em> <span class="math inline">\(C_l\)</span> e <span class="math inline">\(C_i\)</span>:</p>
<p><span class="math display" id="eq:distward">\[\begin{equation}
 d(C_l,C_i)=[\frac{n_l n_i}{n_l+n_i}] (X_{l}-\overline{X_i})&#39;(X_{l}-\overline{X_i})
 \tag{7.37}
\end{equation}\]</span></p>
<p>em que cada passo do algoritmo de agrupamento, os dois conglomerados que minimizam a distância são combinados.</p>
<p>Note que é muito semelhante com o método de centróide, porém o método de Ward leva em consideração a diferença dos tamanhos dos conglomerados que estão sendo comparados, ele possui como fator de ponderação <span class="math inline">\([\frac{n_l n_i}{n_l+n_i}]\)</span> que quanto maior forem os valores de <span class="math inline">\(n_l\)</span> e <span class="math inline">\(n_i\)</span> e a discrepância entre eles, maior será o fator e portanto, a distância entre os centróides dos conglomerados comparados <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>.</p>
<p>O processo de calcular é bem simples, semelhante ao método anterior, porém para as distâncias entre os conglomerados utiliza-se a equação <a href="ptII.html#eq:distward">(7.37)</a> e acaba que sendo bem trabalhosa e consumindo muito tempo do pesquisador, por isso temos atualmente diversos <em>softwares</em> que possam auxiliar neste processo. Com o mesmo conjunto de dados de Renda e Idade do exemplos anteriores podemos chegar no seguinte histórico de agrupamento:</p>
<table>
<caption><span id="tab:ligward">Tabela 7.8: </span> Histórico do agrupamento pelo Método de Ward.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Passo</strong></th>
<th align="center"><strong>Número de Grupos</strong></th>
<th align="center"><strong>Fusão</strong></th>
<th align="center"><strong>Distância</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">5</td>
<td align="center">{A} e {B}</td>
<td align="center">10,44</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">{C} e {F}</td>
<td align="center">17,00</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">3</td>
<td align="center">{A,B} e {E}</td>
<td align="center">61,68</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">2</td>
<td align="center">{A,B,E} e {C,F}</td>
<td align="center">270,25</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">1</td>
<td align="center">{A,B,E,C,F} e {D}</td>
<td align="center">465,00</td>
</tr>
</tbody>
</table>
<p>Importante lembrar que de mesmo modo em outras literaturas, os métodos descritos fazem o agrupamento de elementos amostrais com base em algum critério pré-estabelecido, então nem sempre segue a divisão dos dados amostrais de ordem “natural” entre os <span class="math inline">\(n\)</span> elementos amostrais ou populacional. Pode variar um pouco dependente do <em>Software</em> e sua simulação.</p>
<p>Atente-se pois com os exemplos utilizados, deram resultados bem próximos. Dependendo do tamanho do conjunto de dados pode nem sempre ocorrer assim, mas claro, esperamos uma consistência entre os diferentes métodos.</p>
</div>
</div>
<div id="número-final-de-grupos" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Número final de grupos</h3>
<p>Como escolher o número final de grupos <span class="math inline">\(g\)</span>? Em qual passo <span class="math inline">\(k\)</span>? Isso varia muito com sua pesquisa, sua fundamentação teórica e até mesmo a experiência. Existe alguns critérios que pesquisadores utilizam para avaliar e tomar a decisão, as principais são:</p>
<ol style="list-style-type: decimal">
<li><strong>Análise do comportamento do nível de fusão:</strong> sabemos que a medida que o passo aumenta, a similaridade entre os conglomerados vai decrescendo. Portanto muitos elaboram um gráfico de passo pelo nível de distância. Se há “pontos de salto” grandes em relações aos outros pontos de distância, pode indicar parar. Ao caso do exemplo Método de Ligação Simples ficaria:</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:steps"></span>
<img src="Figuras/steps.png" alt="Gráfico de Passo a partir do exemplo de Método de Ligação Simples. Passos versus Distância." width="70%" />
<p class="caption">
Figura 7.25: Gráfico de Passo a partir do exemplo de Método de Ligação Simples. Passos <em>versus</em> Distância.
</p>
</div>

<ol start="2" style="list-style-type: decimal">
<li><strong>Análise do Comportamento do nível de similaridade:</strong> em vez de observarmos o comportamento da distância em cada estágio, como no critério anterior. utiliza-se o seguinte cálculo para <span class="math inline">\(C_i\)</span> e <span class="math inline">\(C_l\)</span> unidos em certa etapa:</li>
</ol>
<p><span class="math display" id="eq:critsimi">\[\begin{equation}
S_{il}=(1-\frac{d_{il}}{max\{d_{jk},j,k-1,2,...,n\}}).100
 \tag{7.38}
\end{equation}\]</span></p>
<p>sendo <span class="math inline">\(d_{il}\{max\{d_{jk},j,k-1,2,...,n\}\}\)</span> a maior distância entre os <span class="math inline">\(n\)</span> elementos amostrais na matriz do primeiro estágio. Tem como objetivo encontrar pontos onde há decrescimento acentuado na similaridade dos conglomerados unidos (ao encontrar, finaliza o algoritmo). Segundo <span class="citation">Felix (<a href="#ref-felix2004">2004</a>)</span>, geralmente valores acima de 90% resulta em quantidade de grupos muito elevado.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Análise da soma de quadrados entre grupos, o coeficiente <span class="math inline">\(R^2\)</span>:</strong> Em cada <span class="math inline">\(k\)</span> passo, podemos calcular a soma de quadrados entre os grupos e dentro dos grupos.</li>
</ol>
<p>Para <span class="math inline">\(X_{ij}\)</span> vetor de medidas observadas para o <span class="math inline">\(j\)</span>-ésimo elemento amostral do <span class="math inline">\(i\)</span>-ésimo grupo <span class="math inline">\(\overline{X}\)</span> e partição dos dados amostrais em <span class="math inline">\(g\)</span> grupos. A Soma de Quadrados Total corrigida para a média global em cada variável:</p>
<p><span class="math display" id="eq:sstc">\[\begin{equation}
SST_c=\displaystyle \sum^{g}_{i=1} \sum^{n_i}_{i=1} (X_{ij}-\overline{X})&#39;(X_{ij}-\overline{X})
 \tag{7.39}
\end{equation}\]</span></p>
<p>A Soma dos Quadrados Total dentro dos grupos da partição, que equivale ao residual:</p>
<p><span class="math display" id="eq:ssrc">\[\begin{equation}
SSR=\displaystyle \sum^{g}_{i=1} \sum^{n_i}_{i=1} = \sum^{g}_{i=1}SS_i
 \tag{7.40}
\end{equation}\]</span></p>
<p>E a Soma de Quadrados Total entre os <span class="math inline">\(g\)</span> grupos da partição:</p>
<p><span class="math display" id="eq:ssbc">\[\begin{equation}
SSB=\displaystyle \sum^{g}_{i=1} n_i (\overline{X_{i}}-\overline{X})&#39;((\overline{X_{i}}-\overline{X})
 \tag{7.41}
\end{equation}\]</span></p>
<p>Por fim, o coeficiente <span class="math inline">\(R^2\)</span> é expresso por:</p>
<p><span class="math display" id="eq:rsquarec">\[\begin{equation}
R^2=\frac{SSB}{SST_c}
 \tag{7.42}
\end{equation}\]</span></p>
<p>Quanto maior for seu valor, maior será a soma de quadrados SSB e menor o residual SSR. Elaborando um gráfico com os passos do agrupamento e o <span class="math inline">\(R^2\)</span>. E parar o algoritmo no “ponto de salto” grande em relação aos demais.</p>
<p><strong>Observação:</strong> Há diversos critérios para o leitor se aprofundar, como <strong>Estatística Pseudo F</strong>, <strong>Estatística Pseudo T</strong>, <strong>Correlação Semiparcial</strong> que pode-se aplicar em método de Ward, <strong>Estatísica <em>Cubic Clustering Criterium</em></strong>, etc. Cabe o leitor interessado se aprofundar em seus estudos de acordo com sua pretensão. Os métodos hierárquicos são muito utilizados nas pesquisas atuais e ainda estão em constante desenvolvimento e combinações com outros modelos para aperfeiçoar suas pesquisas. Muitos usam o método hierárquico também, da mesma forma que Análise de Componentes Principais e pré-processamento, para selecionar variáveis que possar ser utilizadas em sua pesquisa.</p>
</div>
<div id="técnicas-não-hierárquicas" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Técnicas Não Hierárquicas</h3>
<p>As Técnicas Não Hierárquicas são técnicas que têm como propósito identificar diretamente uma partição de <span class="math inline">\(n\)</span> elementos em <span class="math inline">\(k\)</span> <em>clusters</em>, de modo que a partição satisfaça: coesão/semelhança interna e isolamento/separação dos <em>clusters</em> formados. Há muitas partições possíveis de ordem <span class="math inline">\(k\)</span> e não é plausível criar todas possíveis (provávelmente nem será possível). Portanto deve-se utilizar alguns meios que possam investigar algumas partições viáveis e próxima da ótima.</p>
<p>Diferentemente do Hierárquico, esta técnica necessita que o pesquisador específique previamente o número de <em>clusters</em> desejado. Em cada passo do agrupamento, os novos grupos podem ser formados através da divisão ou junção de grupos formados em etapas anteriores, ou seja, não necessariamente os mesmos elementos num mesmo conglomerado estarão juntos no final. Então, não será possível o uso de dendogramas e geralmente são processos iterativos com maior capacidade de dados. Vamos observar alguns métodos utilizados.</p>
<div id="método-da-k-médias-k-means" class="section level4">
<h4><span class="header-section-number">7.4.3.1</span> Método da K-Médias (<em>K-Means</em>)</h4>
<p>Por <span class="citation">Hartigan and Wong (<a href="#ref-hartigan1979algorithm">1979</a>)</span>, o método k-Médias é um dos mais conhecidos e utilizados em Ánalise de <em>Clusters</em>. Nele, cada elemento amostral é alocado ao <em>cluster</em> mp qual o centróide (vetor de médias amostral) é o mais próximo de valores observados para o respectivo elemento. É composto pelos passos <span class="citation">(Mingoti <a href="#ref-mingoti2007analise">2007</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Selecione <span class="math inline">\(k\)</span> centróides (conhecido como sementes ou protótipos), para iniciar a etapa de partição. Para selecionar varia também pelo método aplicado: pode-se selecionar de forma aleatória simples sem reposição; utilizar técnicas hierárquicas aglomerativas para se obter os grupos iniciais e calcular o vetor de médias; escolher a partir de uma variável aleatória de maior variância; selecionar por análise estatística elementos discrepantes no conjunto de dados; escolhar prefixada ou os primeiros valores do <em>dataset</em>, etc;</p></li>
<li><p>Cada elemento do conjunto de dados é comparado com cada centróide inicial, por meio de alguma medida de distância (geralmente Euclidiana). O de menor distância é alocado ao grupo;</p></li>
<li><p>Após aplicar em cada <span class="math inline">\(n\)</span> elemento amostral, recalcula-se os valores dos centróides para cada novo grupo formado e repte-se a etapa 2, considerando os centróides desse novo grupo;</p></li>
<li><p>Os passos 2 e 3 serão repetidos até que nenhuma realocação de elementos seja necessária e o pesquisador verificar e analisar de acordo com sua demanda.</p></li>
</ol>
<p>Não é recomendável para o experimento quando <span class="math inline">\(k\)</span> primeiros elementos amostrais são similares entre si.</p>
<p>Vamos a um exemplo:</p>
<table>
<caption><span id="tab:desenv">Tabela 7.9: </span> Valores dos índices de desenvolvimento de países.</caption>
<thead>
<tr class="header">
<th align="center">Países</th>
<th align="center">Expectativa de Vida</th>
<th align="center">Educação</th>
<th align="center">PIB</th>
<th align="center">Estabilidade Política</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Reino Unido</td>
<td align="center">0,88</td>
<td align="center">0,99</td>
<td align="center">0,91</td>
<td align="center">1,10</td>
</tr>
<tr class="even">
<td align="center">Austrália</td>
<td align="center">0,90</td>
<td align="center">0,99</td>
<td align="center">0,93</td>
<td align="center">1,26</td>
</tr>
<tr class="odd">
<td align="center">Canadá</td>
<td align="center">0,90</td>
<td align="center">0,98</td>
<td align="center">0,94</td>
<td align="center">1,24</td>
</tr>
<tr class="even">
<td align="center">Estados Unidos</td>
<td align="center">0,87</td>
<td align="center">0,98</td>
<td align="center">0,97</td>
<td align="center">1,18</td>
</tr>
<tr class="odd">
<td align="center">Japão</td>
<td align="center">0,93</td>
<td align="center">0,93</td>
<td align="center">0,93</td>
<td align="center">1,20</td>
</tr>
<tr class="even">
<td align="center">França</td>
<td align="center">0,89</td>
<td align="center">0,97</td>
<td align="center">0,92</td>
<td align="center">1,04</td>
</tr>
<tr class="odd">
<td align="center">Cingapura</td>
<td align="center">0,88</td>
<td align="center">0,87</td>
<td align="center">0,91</td>
<td align="center">1,41</td>
</tr>
<tr class="even">
<td align="center">Argentina</td>
<td align="center">0,81</td>
<td align="center">0,92</td>
<td align="center">0,80</td>
<td align="center">0,55</td>
</tr>
<tr class="odd">
<td align="center">Uruguai</td>
<td align="center">0,82</td>
<td align="center">0,92</td>
<td align="center">0,75</td>
<td align="center">1,05</td>
</tr>
<tr class="even">
<td align="center">Cuba</td>
<td align="center">0,85</td>
<td align="center">0,90</td>
<td align="center">0,64</td>
<td align="center">0,07</td>
</tr>
<tr class="odd">
<td align="center">Colômbia</td>
<td align="center">0,77</td>
<td align="center">0,85</td>
<td align="center">0,69</td>
<td align="center">-1,36</td>
</tr>
<tr class="even">
<td align="center">Brasil</td>
<td align="center">0,71</td>
<td align="center">0,83</td>
<td align="center">0,72</td>
<td align="center">0,47</td>
</tr>
<tr class="odd">
<td align="center">Paraguai</td>
<td align="center">0,75</td>
<td align="center">0,83</td>
<td align="center">0,63</td>
<td align="center">-0,87</td>
</tr>
<tr class="even">
<td align="center">Egito</td>
<td align="center">0,70</td>
<td align="center">0,62</td>
<td align="center">0,60</td>
<td align="center">0,21</td>
</tr>
<tr class="odd">
<td align="center">Nigéria</td>
<td align="center">0,44</td>
<td align="center">0,58</td>
<td align="center">0,37</td>
<td align="center">-1,36</td>
</tr>
<tr class="even">
<td align="center">Senegal</td>
<td align="center">0,47</td>
<td align="center">0,37</td>
<td align="center">0,45</td>
<td align="center">-0,68</td>
</tr>
<tr class="odd">
<td align="center">Serra Leoa</td>
<td align="center">0,23</td>
<td align="center">0,33</td>
<td align="center">0,27</td>
<td align="center">-1,26</td>
</tr>
<tr class="even">
<td align="center">Angola</td>
<td align="center">0,34</td>
<td align="center">0,36</td>
<td align="center">0,51</td>
<td align="center">-1,98</td>
</tr>
<tr class="odd">
<td align="center">Etiópia</td>
<td align="center">0,31</td>
<td align="center">0,35</td>
<td align="center">0,32</td>
<td align="center">-0,55</td>
</tr>
<tr class="even">
<td align="center">Moçambique</td>
<td align="center">0,24</td>
<td align="center">0,37</td>
<td align="center">0,36</td>
<td align="center">0,20</td>
</tr>
<tr class="odd">
<td align="center">China</td>
<td align="center">0,76</td>
<td align="center">0,80</td>
<td align="center">0,61</td>
<td align="center">0,39</td>
</tr>
</tbody>
</table>
<p><strong>Fonte:</strong> ONU, 2002, site: www.undp.org/hdro. Relatório de Desenvolvimento Humano.</p>
<p>Como dito, este método é muito dispendioso ao pesquisador calcular manualmente, portanto recomendo-o utilizar algum <em>software</em> estatístico para o seu cálculo e verificar o algoritmo do mesmo. Particularmente, utilizo para a escolha das sementes iniciais a seleção aleatória ou alguma técnica Hierárquica, pois ela evita com que há influências pessoais na seleção.</p>
<p>Após aplicarmos o método <em>k-Médias</em> com a escolha aleatória das sementes iniciais obtemos:</p>
<table>
<caption><span id="tab:exkmeans">Tabela 7.10: </span> Resultado descritivo dos <em>clusters</em> formados.</caption>
<colgroup>
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="33%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Grupos</strong></th>
<th><strong>SQ</strong></th>
<th><strong>Países</strong></th>
<th align="center"><strong>Média Expectativa de Vida</strong></th>
<th><strong>Média Educação</strong></th>
<th><strong>Média PIB</strong></th>
<th><strong>Média Estabilidade Política</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(n_1=3\)</span></td>
<td>0,0257</td>
<td>Reino Unido, França, Uruguai</td>
<td align="center">0,8633</td>
<td>0,960</td>
<td>0,860</td>
<td>1,063</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_2=7\)</span></td>
<td>2,187</td>
<td>Colômbia, Paraguai, Nigéria, Senegal, Serra Leoa, Angola, Etiópia</td>
<td align="center">0,473</td>
<td>0,524</td>
<td>0,463</td>
<td>-1,154</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_3=6\)</span></td>
<td>0,748</td>
<td>Argentina, Cuba, Brasil, Egito, Moçambique, China</td>
<td align="center">0,678</td>
<td>0,740</td>
<td>0,622</td>
<td>0,315</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_4=5\)</span></td>
<td>0,047</td>
<td>Austrália, Canadá, Estados Unidos, Japão, Cingapura</td>
<td align="center">0,896</td>
<td>0,950</td>
<td>0,936</td>
<td>1,258</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:exkmeans3">Tabela 7.11: </span> Análise da qualidade dos grupos formados.</caption>
<thead>
<tr class="header">
<th align="center"><strong>SSR (Soma de Quadrados Residual, soma dos grupos)</strong></th>
<th align="center">3,008</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>SSB (Soma de Quadrados entre os <span class="math inline">\(g\)</span> grupos)</strong></td>
<td align="center">22,757</td>
</tr>
<tr class="even">
<td align="center"><strong>SST (Soma de Quadrados Total)</strong></td>
<td align="center">25,765</td>
</tr>
<tr class="odd">
<td align="center"><strong><span class="math inline">\(R^2=SSB/SST\)</span></strong></td>
<td align="center">88,3%</td>
</tr>
</tbody>
</table>
<p>Podemos avaliar um bom modelo pelo seu <span class="math inline">\(R^2\)</span>, vale lembrar que o algoritmo foi aplicado para a escolha aleatória de centróides. O que pode variar o resultado de acordo com o processo. Por isso muitas vezes, os pesquisadores utilizam mais de um método para classificações para que se possa comparar e ter consistência em sua pesquisa. Caso o pesquisador verifique e valide estas classificações, pode-se aplicar novos estudos por exemplo, para cada conjunto de países, desde análise de regressão à diversos outros métodos.</p>
</div>
<div id="método-de-fuzzy-c-médias-c-means" class="section level4">
<h4><span class="header-section-number">7.4.3.2</span> Método de Fuzzy C-Médias (<em>C-Means</em>)</h4>
<p>O método de Fuzzy <span class="citation">(Bezdek <a href="#ref-bezdek1981objective">1981</a>)</span> também é um método iterativo que requer do pesquisador pré-estabelecer do número de grupos, como <em>K-means</em>. Este método procura a partição que minimiza a função objetivo, expressa por:</p>
<p><span class="math display" id="eq:cmeansobjetivo">\[\begin{equation}
J=\displaystyle \sum^c_{i=l} \sum^n_{j=l} (u_{ij}^m d(X_j,V_i))
 \tag{7.43}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(V_i\)</span> é a semente (protótipo ou centróide ponderado) do conglomerado <span class="math inline">\(i=1,2,...,c\)</span>, <span class="math inline">\(m&gt;1\)</span> é o parâmetro Fuzzy, quanto mais alto for, mais difuso será o cluster no final (geralmente usam-se <span class="math inline">\(m=2\)</span>); <span class="math inline">\(u_{ij}\)</span> é a probabilidade de que o elemento <span class="math inline">\(X_j\)</span> pertença ao congomerado com a semente <span class="math inline">\(V_i\)</span> e <span class="math inline">\(d\)</span> é a distância (método escolhido pelo pesquisador, geralmente Euclidiana).</p>
<p>A função <span class="math inline">\(J\)</span> <a href="ptII.html#eq:cmeansobjetivo">(7.43)</a> é minimizada quando as probabilidades <span class="math inline">\(u_{ij}\)</span> e a semente <span class="math inline">\(V_i\)</span> são definidas como:</p>
<p><span class="math display" id="eq:cmeansprob">\[\begin{equation}
u_{ij}=\Big[\displaystyle \sum^c_{k=1}\Big(\frac{d(X_j,V_i)}{d(X_j,V_k)}\Big)^{2/(m-1)}\Big]^{-1}
 \tag{7.44}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:cmeanssemente">\[\begin{equation}
V_{i}=\displaystyle \frac{\sum^n_{j=1}(u_{ij})^m X_j}{\sum^n_{j=1}(u_{ij})^m} 
\tag{7.45}
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(i=1,2...,c\)</span> e <span class="math inline">\(j=1,2,...n\)</span>. Com <span class="math inline">\(u_{ij}\)</span> seguindo uma distribuição entre 0 e 1 e os protótipos vão se modificando a cada iteração. O algoritmo é interrompido quando a distância entre os protótipos de uma iteração em relação à anterior é menor ou igual a um erro <span class="math inline">\(\mu\)</span> estabelicido pelo pesquisador, ou seja, os vetores <span class="math inline">\(V_t\)</span> e <span class="math inline">\(V_{t+1}\)</span> da iterações <span class="math inline">\(t\)</span> e <span class="math inline">\(t+1\)</span> que guardam as sementes precisam que: <span class="math inline">\(d(V_t,V_{t+1})&lt;\mu\)</span>.</p>
<p>Para cada elemento amostral, este método estima uma probabilidade de que o este elemento pertença a cada um dos <em>clusters</em> <span class="math inline">\(c\)</span> da partição. Podemos então encontrar elementos amostrais que se assemelham a mais de um dos <span class="math inline">\(c\)</span> grupos. Alguns pesquisadores utilizam como critério de seleção para qual <em>cluster</em> irá pertencer de acordo com o que tenha a maior probabilidade.</p>
<p>Utilizando o mesmo conjunto de dados utilizado no método K-Médias e aplicarmos o método de Fuzzy C-Médias, supondo <span class="math inline">\(m=2\)</span> e com <em>cluster</em> pré-estabelecido, obtemos seus resultados e alocando-os com base no critério de maior probabildade.</p>
<table>
<caption><span id="tab:excmeans">Tabela 7.12: </span> Resultado obtido pelo método de C-Médias. Utilizando como critério de alocar ao grupo pela maior probabilidade.</caption>
<thead>
<tr class="header">
<th>Países</th>
<th>Prob. <span class="math inline">\(C_1\)</span></th>
<th>Prob. <span class="math inline">\(C_2\)</span></th>
<th>Prob. <span class="math inline">\(C_3\)</span></th>
<th>Prob. <span class="math inline">\(C_4\)</span></th>
<th>Prob. <span class="math inline">\(C_5\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Reino Unido</td>
<td>0,864</td>
<td>0,026</td>
<td>0,063</td>
<td>0,027</td>
<td>0,020</td>
</tr>
<tr class="even">
<td>Austrália</td>
<td>0,776</td>
<td>0,044</td>
<td>0,098</td>
<td>0,046</td>
<td>0,035</td>
</tr>
<tr class="odd">
<td>Canadá</td>
<td>0,802</td>
<td>0,039</td>
<td>0,087</td>
<td>0,041</td>
<td>0,031</td>
</tr>
<tr class="even">
<td>EstadosUnidos</td>
<td>0,842</td>
<td>0,031</td>
<td>0,071</td>
<td>0,032</td>
<td>0,024</td>
</tr>
<tr class="odd">
<td>Japão</td>
<td>0,836</td>
<td>0,032</td>
<td>0,073</td>
<td>0,033</td>
<td>0,025</td>
</tr>
<tr class="even">
<td>França</td>
<td>0,767</td>
<td>0,043</td>
<td>0,110</td>
<td>0,046</td>
<td>0,034</td>
</tr>
<tr class="odd">
<td>Cingapura</td>
<td>0,625</td>
<td>0,076</td>
<td>0,158</td>
<td>0,080</td>
<td>0,061</td>
</tr>
<tr class="even">
<td>Argentina</td>
<td>0,228</td>
<td>0,098</td>
<td>0,500</td>
<td>0,103</td>
<td>0,071</td>
</tr>
<tr class="odd">
<td>Uruguai</td>
<td>0,636</td>
<td>0,066</td>
<td>0,177</td>
<td>0,070</td>
<td>0,051</td>
</tr>
<tr class="even">
<td>Cuba</td>
<td>0,135</td>
<td>0,158</td>
<td>0,447</td>
<td>0,160</td>
<td>0,100</td>
</tr>
<tr class="odd">
<td>Colômbia</td>
<td>0,071</td>
<td>0,310</td>
<td>0,103</td>
<td>0,184</td>
<td>0,332</td>
</tr>
<tr class="even">
<td>Brasil</td>
<td>0,123</td>
<td>0,068</td>
<td>0,685</td>
<td>0,075</td>
<td>0,049</td>
</tr>
<tr class="odd">
<td>Paraguai</td>
<td>0,048</td>
<td>0,557</td>
<td>0,080</td>
<td>0,162</td>
<td>0,152</td>
</tr>
<tr class="even">
<td>Egito</td>
<td>0,120</td>
<td>0,121</td>
<td>0,533</td>
<td>0,144</td>
<td>0,082</td>
</tr>
<tr class="odd">
<td>Nigéria</td>
<td>0,024</td>
<td>0,103</td>
<td>0,035</td>
<td>0,081</td>
<td>0,757</td>
</tr>
<tr class="even">
<td>Senegal</td>
<td>0,035</td>
<td>0,165</td>
<td>0,060</td>
<td>0,631</td>
<td>0,108</td>
</tr>
<tr class="odd">
<td>Serra Leoa</td>
<td>0,057</td>
<td>0,196</td>
<td>0,083</td>
<td>0,207</td>
<td>0,457</td>
</tr>
<tr class="even">
<td>Angola</td>
<td>0,084</td>
<td>0,221</td>
<td>0,113</td>
<td>0,196</td>
<td>0,386</td>
</tr>
<tr class="odd">
<td>Etiópia</td>
<td>0,054</td>
<td>0,172</td>
<td>0,093</td>
<td>0,547</td>
<td>0,134</td>
</tr>
<tr class="even">
<td>Moçambique</td>
<td>0,149</td>
<td>0,177</td>
<td>0,285</td>
<td>0,253</td>
<td>0,136</td>
</tr>
<tr class="odd">
<td>China</td>
<td>0,061</td>
<td>0,041</td>
<td>0,823</td>
<td>0,046</td>
<td>0,029</td>
</tr>
</tbody>
</table>
<p>Portanto ficará:</p>
<table>
<caption><span id="tab:excmeans2">Tabela 7.13: </span> Quantidade de Países por grupo e Soma de Quadrados por grupo.</caption>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Grupos</strong></th>
<th><strong>Países</strong></th>
<th><strong>SQ</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(n_1=8\)</span></td>
<td>Reino Unido, Austrália, Canadá, Estados Unidos, Japão, França, Cingapura, Uruguai</td>
<td>0,157</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_2=1\)</span></td>
<td>Paraguai</td>
<td>0,000</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_3=6\)</span></td>
<td>Argentina,Cuba, Brasil, Egito, Moçambique, China</td>
<td>0,748</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_4=2\)</span></td>
<td>Senegal, Etiópia</td>
<td>0,030</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_5=4\)</span></td>
<td>Colômbia, Nigéria, Serra Leoa, Angola</td>
<td>0,763</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:exkmeans2">Tabela 7.14: </span> Análise da qualidade dos grupos formados.</caption>
<thead>
<tr class="header">
<th align="center"><strong>SSR (Soma de Quadrados Residual, soma dos grupos)</strong></th>
<th align="center">1,698</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>SSB (Soma de Quadrados entre os <span class="math inline">\(g\)</span> grupos)</strong></td>
<td align="center">20,983</td>
</tr>
<tr class="even">
<td align="center"><strong>SST (Soma de Quadrados Total)</strong></td>
<td align="center">22,681</td>
</tr>
<tr class="odd">
<td align="center"><strong><span class="math inline">\(R^2=SSB/SST\)</span></strong></td>
<td align="center">0,925%</td>
</tr>
</tbody>
</table>
<p>Tivemos um resultado interessante com um bom valor de <span class="math inline">\(R^2\)</span> e próximo ao do exemplo anterior, entretanto seria importante dar uma atenção maior em alguns países, visto que suas probabilidades para a seleção de <em>cluster</em> são bem semelhantes para Colômbia em <span class="math inline">\(n_2 (0,310)\)</span> e <span class="math inline">\(n_5 (0,332)\)</span> e Moçambique <span class="math inline">\(n_3=0,285\)</span> e <span class="math inline">\(n_4=0,253\)</span>. Poderíamos testar com novas estratégias, novas quantidades de <em>clusters</em> ou alguns métodos de avaliação para que se avalie e torne mais consistente a análise. Em caso de variáveis com alta probabilidade não teremos dúvidas sobre sua alocação. O início da seleção de centróides foi formulado de forma aleatória para este exemplo. Recomendo-o o leitor retornar ao exemplo de K-médias e comparar a este ou até mesmo com Análise de Componentes Principais. Entenda que são metodologias diferentes com combinações de estratégias diferentes (desde medidas de distância como Euclidiana, método de análise multivariada, tipo de seleção de centróides, etc), podemos combinar e comparar todas estas técnicas para termos consistências em nossas pesquisas.</p>
<p>Em <a href="valid.html#valid">5</a> serão apresentados outros métodos para medir o desempenho e validar seu modelo.</p>

</div>
</div>
</div>
<div id="knn-k-vizinhos-mais-próximos-k-nearest-neighbors" class="section level2">
<h2><span class="header-section-number">7.5</span> KNN: K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</h2>
<p>Uma metodologia muito conhecida e utilizada, referida na literatura de <span class="citation">(Bhattacharya, Poulsen, and Toussaint <a href="#ref-bhattacharya1981application">1981</a>)</span> e <span class="citation">(Bhattacharya, Mukherjee, and Toussaint <a href="#ref-bhattacharya2005geometric">2005</a>)</span>. O <strong>K-Vizinhos Mais Próximos</strong>, do inglês <strong><em>K-Nearest Neighbors</em> (KNN)</strong>, é um classificador simples. O conjunto de treinamento é formado por vetores com <span class="math inline">\(n\)</span>-dimensões no qual cada elemento deste conjunto retrata um ponto no espaço <span class="math inline">\(n\)</span>-dimensional.</p>
<p>Vamos supor que temos um conjunto de dados repartido em duas classes: doentes e não doentes. Com a entrada de mais um paciente para a análise, temos uma nova observação que ainda não está classificada. Dentro do conjunto de treinamento, o classificador KNN procura <span class="math inline">\(K\)</span> elementos que estejam mais próximos deste elemento de classe desconhecida, ou seja, que tenham a menor distância. Após verificar quais são as classes desses <span class="math inline">\(K\)</span> vizinhos e a classe mais frequente das observações próximas, será atribuída a classe deste elemento desconhecido. Por isso é denominado por K-Vizinhos Mais Próximos, onde <span class="math inline">\(K\)</span> indica a quantidade de vizinhos próximos à observação nova no conjunto de dados.</p>
<p>Como já vimos, existe diversos métodos de calcular a distância entre as observações, algumas delas estão apresentadas em <a href="dicio.html#meddist">3.3.3</a>. Muitas vezes por ser um exaustivo processo computacional para calcular todas as distâncias entre as observações com a observação de classe desconhecida, é comum para que identifique vizinhos mais próximos, elaborar uma hiper-esfera de raio <span class="math inline">\(R\)</span> que será decidido pelo pesquisador e selecionar os elementos que estão dentro desta hiper-esfera. Este processo torna muito mais rápido e barato para o pesquisador, porém como desvantagem de haver possibilidade de não ter pontos dentro da hiper-esfera.</p>
<p>Na figura <a href="ptII.html#fig:knn1">7.26</a> temos como exemplo uma situação onde queremos saber se o novo paciente de um hospital está doente ou não. De acordo com pacientes anteriores, podemos reprentar graficamente com características <span class="math inline">\(X_1\)</span> E <span class="math inline">\(X_2\)</span> e identificar em qual classe cada um pertence. Após calcularmos as distância entre as observações e este elemento de classificação desconhecida, podemos identificar os vizinhos mais próximos. Supondo que temos três vizinhos (<span class="math inline">\(k=3\)</span>) mais próximo do elemento de classe desconhecida, note que das três observações (duas azuis e uma vermelha) próximas do elemento, duas são consideradas “doentes” (cor azul) e uma “não doente” (vermelho).</p>
<div class="figure" style="text-align: center"><span id="fig:knn1"></span>
<img src="Figuras/knn1.png" alt="Exemplo gráfico de KNN com \(k=3\)." width="70%" />
<p class="caption">
Figura 7.26: Exemplo gráfico de KNN com <span class="math inline">\(k=3\)</span>.
</p>
</div>

<p>Por voto de maioria, com duas azuis e uma vermelha (2x1), este elemento será classificado pelo algoritmo como “doente”.</p>
<div class="figure" style="text-align: center"><span id="fig:knn2"></span>
<img src="Figuras/knn2.png" alt="Exemplo gráfico de KNN com \(k=3\). Como a maioria é classificada como “doente” entre os vizinhos próximos do elemento, o elemento desconhecido também será." width="70%" />
<p class="caption">
Figura 7.27: Exemplo gráfico de KNN com <span class="math inline">\(k=3\)</span>. Como a maioria é classificada como “doente” entre os vizinhos próximos do elemento, o elemento desconhecido também será.
</p>
</div>

<div id="exknn" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Exemplo</h3>
<p>1 - A EmprestaX, uma empresa de empréstimos nova em uma cidade, possui apenas alguns dias de serviço. Com intuito de melhorar na identificação de seu público alvo, a empresa pretende utilizar o algoritmo de KNN com base no histórico de seus primeiros clientes: Renda mensal, a quantidade de Contas atrasadas e sua classificação de ser ou não um possível cliente.</p>
<table>
<caption><span id="tab:dadossrendaaknn">Tabela 7.15: </span> Dados históricos de clientes da EmprestaX com as variáveis Renda Mensal (R$), Contas Atrasadas e Possível Cliente.</caption>
<thead>
<tr class="header">
<th><strong>Cliente</strong></th>
<th><strong>A</strong></th>
<th><strong>B</strong></th>
<th><strong>C</strong></th>
<th><strong>D</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Renda Mensal</strong></td>
<td>R$ 4000,00</td>
<td>R$ 1000,00</td>
<td>R$ 2500,00</td>
<td>R$ 2800,00</td>
</tr>
<tr class="even">
<td><strong>Contas Atrasadas</strong></td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr class="odd">
<td><strong>Possível Cliente</strong></td>
<td>Não</td>
<td>Sim</td>
<td>Sim</td>
<td>Não</td>
</tr>
</tbody>
</table>
<p>Supondo um indivíduo E com uma renda mensal de R$ 3300,00 , duas contas atrasadas. Pelo método de KNN, com k=3 vizinhos, qual será sua classificação de ser ou não um possível cliente para a EmprestaX?</p>
<p>Primeiramente, vamos observar os pontos em um gráfico:</p>
<div class="figure" style="text-align: center"><span id="fig:knnex1"></span>
<img src="Figuras/knnex1.png" alt="Gráfico gerado com base na tabela 7.15 e classificação para Possível Cliente em: Sim, Não, Não Classificado." width="70%" />
<p class="caption">
Figura 7.28: Gráfico gerado com base na tabela <a href="ptII.html#tab:dadossrendaaknn">7.15</a> e classificação para Possível Cliente em: Sim, Não, Não Classificado.
</p>
</div>

<p>Agora que observamos o gráfico, lembrando que pode haver diversos métodos para calcular a distância ou considerando uma hiper-esfera, vamos calcular a Distância Euclidiana entre as observações:
<span class="math display">\[\mbox{Distância entre E e A=}\sqrt{(4000-3300)^2+(1-2)^2}=700,0007\]</span>
Repetindo o mesmo processo para todas observações, obtemos:</p>
<p><span class="math display">\[D_{5x5}=\begin{bmatrix}\\
 &amp;A&amp;B&amp;C&amp;D&amp;E \\
 A&amp;0&amp;&amp;&amp;&amp;\\
 B&amp;3000,0007&amp;0&amp;&amp;&amp;\\
 C &amp; 1500,0003&amp; 1500,0003&amp;0&amp;&amp;\\
 D&amp; 1200,0004&amp; 1800,0003&amp; 300,0000&amp;0&amp;\\
 E&amp; 700,0007&amp; 2300,0002&amp; 800,0000&amp; 500,0000&amp;0\\
\end{bmatrix}\]</span></p>
<p>Como queremos <span class="math inline">\(k=3\)</span> vizinhos mais próximos do indivíduo E para que se possa classificá-lo, temos então:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th><strong>A</strong></th>
<th><strong>C</strong></th>
<th><strong>D</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>E</strong></td>
<td>700,0007</td>
<td>800,0000</td>
<td>500,0000</td>
</tr>
<tr class="even">
<td><strong>Possível Cliente</strong></td>
<td>Não</td>
<td>Sim</td>
<td>Não</td>
</tr>
</tbody>
</table>
<p>Portanto, pela maioria, temos que o indivído E não será considerado um possível cliente pelo algoritmo de KNN.</p>
<div class="figure" style="text-align: center"><span id="fig:knnex2"></span>
<img src="Figuras/knnex2.png" alt="Gráfico gerado com base na tabela @tab:dadossrendaaknn e classificado conforme o algoritmo de KNN com \(k=3\) vizinhos próximos do elemento novo na amostra." width="70%" />
<p class="caption">
Figura 7.29: Gráfico gerado com base na tabela @tab:dadossrendaaknn e classificado conforme o algoritmo de KNN com <span class="math inline">\(k=3\)</span> vizinhos próximos do elemento novo na amostra.
</p>
</div>

<p>Note que neste exemplo, caso fosse escolhido com <span class="math inline">\(k=4\)</span> ou <span class="math inline">\(k=2\)</span> vizinhos, haveria empate e necessitaria de alterarmos o valor de <span class="math inline">\(k\)</span> ou utilizar algum outro método de classificação. Aumentar o número de observações para a amostra também é muito importante para que se treine seu modelo de Aprendizado de Máquina.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-analytics">
<p>ANALYTICS VIDHYA. 2016. “Tree Based Algorithms: A Complete Tutorial from Scratch (in R &amp; Python).” In. <a href="https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/">https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/</a>.</p>
</div>
<div id="ref-bezdek1981objective">
<p>Bezdek, James C. 1981. “Objective Function Clustering.” In <em>Pattern Recognition with Fuzzy Objective Function Algorithms</em>, 43–93. Springer.</p>
</div>
<div id="ref-bhattacharya1981application">
<p>Bhattacharya, Binay K, Ronald S Poulsen, and Godfried T Toussaint. 1981. “Application of Proximity Graphs to Editing Nearest Neighbor Decision Rule.” In <em>International Symposium on Information Theory, Santa Monica</em>.</p>
</div>
<div id="ref-bhattacharya2005geometric">
<p>Bhattacharya, Binay, Kaustav Mukherjee, and Godfried Toussaint. 2005. “Geometric Decision Rules for Instance-Based Learning Problems.” In <em>International Conference on Pattern Recognition and Machine Intelligence</em>, 60–69. Springer.</p>
</div>
<div id="ref-breiman1984classification">
<p>Breiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. <em>Classification and Regression Trees</em>. CRC press.</p>
</div>
<div id="ref-ding2001multi">
<p>Ding, Chris HQ, and Inna Dubchak. 2001. “Multi-Class Protein Fold Recognition Using Support Vector Machines and Neural Networks.” <em>Bioinformatics</em> 17 (4): 349–58.</p>
</div>
<div id="ref-felix2004">
<p>Felix, F. N. 2004. “Aplicando Bootstrap Para Determinação de Intervalos de Confiança Para O Número de Grupos No Procedimento Hierárquico Aglomerativo de Ward.” <em>Dissertação-Mestrado</em>.</p>
</div>
<div id="ref-gonccalves2015maquina">
<p>Gonçalves, André Ricardo. 2008. “Máquina de Vetores Suporte.” <em>Universidade Estadual de Londrina</em> 21.</p>
</div>
<div id="ref-hartigan1979algorithm">
<p>Hartigan, John A, and Manchek A Wong. 1979. “Algorithm as 136: A K-Means Clustering Algorithm.” <em>Journal of the Royal Statistical Society. Series c (Applied Statistics)</em> 28 (1): 100–108.</p>
</div>
<div id="ref-holsheimer1994data">
<p>Holsheimer, Marcel, and Arno PJM Siebes. 1994. <em>Data Mining: The Search for Knowledge in Databases</em>. Centrum voor Wiskunde en Informatica.</p>
</div>
<div id="ref-hongyu2016analise">
<p>Hongyu, Kuang, Vera Lúcia Martins Sandanielo, and Gilmar Jorge de Oliveira Junior. 2016. “Análise de Componentes Principais: Resumo Teórico, Aplicação E Interpretação.” <em>E&amp;S Engineering and Science</em> 5 (1): 83–90.</p>
</div>
<div id="ref-hotelling1933analysis">
<p>Hotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” <em>Journal of Educational Psychology</em> 24 (6): 417.</p>
</div>
<div id="ref-ingargiola1996building">
<p>Ingargiola, Giorgio. 1996. “Building Classification Models: ID3 and C4. 5.” <em>Disponı́vel Por WWW Em: Http://Www. Cis. Temple. Edu/~ Ingargio/Cis587/Readings/Id3-C45. Html</em>.</p>
</div>
<div id="ref-kaiser1960application">
<p>Kaiser, Henry F. 1960. “The Application of Electronic Computers to Factor Analysis.” <em>Educational and Psychological Measurement</em> 20 (1): 141–51.</p>
</div>
<div id="ref-light1971analysis">
<p>Light, Richard J, and Barry H Margolin. 1971. “An Analysis of Variance for Categorical Data.” <em>Journal of the American Statistical Association</em> 66 (335): 534–44.</p>
</div>
<div id="ref-lima2002maquinas">
<p>Lima, Allan Reffson Granja. 2002. “Máquinas de Vetores Suporte Na Classificaçao de Impressoes Digitais.” <em>Universidade Federal Do Ceará, Departamento de Computação, Fortaleza-Ceará</em>.</p>
</div>
<div id="ref-lorena2003introduccaoas">
<p>Lorena, Ana Carolina, and André CPLF de Carvalho. 2003. “Introduçaoas Máquinas de Vetores Suporte.” <em>Relatório Técnico Do Instituto de Ciências Matemáticas E de Computaçao (USP/Sao Carlos)</em> 192: 11.</p>
</div>
<div id="ref-mingoti2007analise">
<p>Mingoti, Sueli Aparecida. 2007. “Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada.” In <em>Análise de Dados Através de Métodos Estatı́stica Multivariada: Uma Abordagem Aplicada</em>, 295–95.</p>
</div>
<div id="ref-pearson1901liii">
<p>Pearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> 2 (11): 559–72.</p>
</div>
<div id="ref-powellpca">
<p>Powell, Victor and Lehe, Lewis. 2014. “Análise Do Componente Principal.” In. <a href="https://setosa.io/ev/principal-component-analysis/">https://setosa.io/ev/principal-component-analysis/</a>.</p>
</div>
<div id="ref-punj1983cluster">
<p>Punj, Girish, and David W Stewart. 1983. “Cluster Analysis in Marketing Research: Review and Suggestions for Application.” <em>Journal of Marketing Research</em> 20 (2): 134–48.</p>
</div>
<div id="ref-shiba2005classificaccao">
<p>Shiba, Marcelo Hiroshi, Rosangela Leal Santos, José Alberto Quintanilha, and Hae Yong Kim. 2005. “Classificação de Imagens de Sensoriamento Remoto Pela Aprendizagem Por árvore de Decisão: Uma Avaliação de Desempenho.” <em>Simpósio Brasileiro de Sensoriamento Remoto</em> 12: 4319–26.</p>
</div>
<div id="ref-da2009classificaccao">
<p>Silva Meloni, Raphael Belo da. 2009. “Classificação de Imagens de Sensoriamento Remoto Usando Svm.” PhD thesis, PUC-Rio.</p>
</div>
<div id="ref-smola2000introduction">
<p>Smola, Alexander J, Peter Bartlett, Bernhard Schölkopf, and Dale Schuurmans. 2000. “Introduction to Large Margin Classifiers.”</p>
</div>
<div id="ref-sneath1957application">
<p>Sneath, Peter HA. 1957. “The Application of Computers to Taxonomy.” <em>Microbiology</em> 17 (1): 201–26.</p>
</div>
<div id="ref-speece1985classification">
<p>Speece, Deborah L, James D McKinney, and Mark I Appelbaum. 1985. “Classification and Validation of Behavioral Subtypes of Learning-Disabled Children.” <em>Journal of Educational Psychology</em> 77 (1): 67.</p>
</div>
<div id="ref-sung2003identifying">
<p>Sung, Andrew H, and Srinivas Mukkamala. 2003. “Identifying Important Features for Intrusion Detection Using Support Vector Machines and Neural Networks.” In <em>2003 Symposium on Applications and the Internet, 2003. Proceedings.</em>, 209–16. IEEE.</p>
</div>
<div id="ref-vapnik2013nature">
<p>Vapnik, Vladimir. 2013. <em>The Nature of Statistical Learning Theory</em>. Springer science &amp; business media.</p>
</div>
<div id="ref-ward1963hierarchical">
<p>Ward Jr, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective Function.” <em>Journal of the American Statistical Association</em> 58 (301): 236–44.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Algoritmosaprendizagem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ptIII.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "github", "instagram"]
},
"fontsettings": ["white", "sepia", "night"],
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08.1-ModeloIIFilho.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
