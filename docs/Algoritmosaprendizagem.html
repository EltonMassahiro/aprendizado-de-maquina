<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 Algoritmos de Aprendizagem I | Machine Learning</title>
  <meta name="description" content="Tutorial de Machine Learning." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 Algoritmos de Aprendizagem I | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Tutorial de Machine Learning." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 Algoritmos de Aprendizagem I | Machine Learning" />
  
  <meta name="twitter:description" content="Tutorial de Machine Learning." />
  

<meta name="author" content="Elton Massahiro Saito Loures" />


<meta name="date" content="2021-01-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="valid.html"/>
<link rel="next" href="ptII.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#por-que-ler-esse-livro"><i class="fa fa-check"></i><b>0.1</b> Por que ler esse livro?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#estrutura"><i class="fa fa-check"></i><b>0.2</b> Estrutura</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#informações-a-respeito-do-conteúdo"><i class="fa fa-check"></i><b>0.3</b> Informações a respeito do conteúdo</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#agradecimentos"><i class="fa fa-check"></i><b>0.4</b> Agradecimentos</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#dicas-de-estudo"><i class="fa fa-check"></i><b>1.1</b> Dicas de estudo</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="i-a.html"><a href="i-a.html"><i class="fa fa-check"></i><b>2</b> Inteligência Artificial (IA)</a><ul>
<li class="chapter" data-level="2.1" data-path="i-a.html"><a href="i-a.html#o-que-é-ia-de-onde-veio-esse-conceito"><i class="fa fa-check"></i><b>2.1</b> O que é IA? De onde veio esse conceito?</a></li>
<li class="chapter" data-level="2.2" data-path="i-a.html"><a href="i-a.html#a-arte-de-uma-ia"><i class="fa fa-check"></i><b>2.2</b> A arte de uma IA</a></li>
<li class="chapter" data-level="2.3" data-path="i-a.html"><a href="i-a.html#vertentes-de-uma-ia-e-fundamentação-filosófica"><i class="fa fa-check"></i><b>2.3</b> Vertentes de uma IA e fundamentação filosófica</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>3</b> O Aprendizado de Máquina</a><ul>
<li class="chapter" data-level="3.1" data-path="machinelearning.html"><a href="machinelearning.html#como-a-máquina-aprende"><i class="fa fa-check"></i><b>3.1</b> Como a máquina aprende?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dicio.html"><a href="dicio.html"><i class="fa fa-check"></i><b>4</b> Um pouco de revisão</a><ul>
<li class="chapter" data-level="4.1" data-path="dicio.html"><a href="dicio.html#um-pouco-de-álgebra-linear"><i class="fa fa-check"></i><b>4.1</b> Um pouco de Álgebra Linear</a></li>
<li class="chapter" data-level="4.2" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística.-parte-i"><i class="fa fa-check"></i><b>4.2</b> Um pouco de Estatística. Parte I</a></li>
<li class="chapter" data-level="4.3" data-path="dicio.html"><a href="dicio.html#um-pouco-de-estatística.-parte-ii"><i class="fa fa-check"></i><b>4.3</b> Um pouco de Estatística. Parte II</a></li>
<li class="chapter" data-level="4.4" data-path="dicio.html"><a href="dicio.html#multlagrange"><i class="fa fa-check"></i><b>4.4</b> Multiplicadores de Lagrange</a></li>
<li class="chapter" data-level="4.5" data-path="dicio.html"><a href="dicio.html#kkt"><i class="fa fa-check"></i><b>4.5</b> Karush-Kuhn-Tucker (KKT)</a></li>
<li class="chapter" data-level="4.6" data-path="dicio.html"><a href="dicio.html#bias"><i class="fa fa-check"></i><b>4.6</b> Bias</a></li>
<li class="chapter" data-level="4.7" data-path="dicio.html"><a href="dicio.html#medidasimport"><i class="fa fa-check"></i><b>4.7</b> Medidas de Importância</a><ul>
<li class="chapter" data-level="4.7.1" data-path="dicio.html"><a href="dicio.html#medidasdep"><i class="fa fa-check"></i><b>4.7.1</b> Medidas de Dependência</a></li>
<li class="chapter" data-level="4.7.2" data-path="dicio.html"><a href="dicio.html#medinfo"><i class="fa fa-check"></i><b>4.7.2</b> Medidas de Informação</a></li>
<li class="chapter" data-level="4.7.3" data-path="dicio.html"><a href="dicio.html#meddist"><i class="fa fa-check"></i><b>4.7.3</b> Medidas de Similaridade e Dissimilaridade</a></li>
<li class="chapter" data-level="4.7.4" data-path="dicio.html"><a href="dicio.html#medidas-de-precisão"><i class="fa fa-check"></i><b>4.7.4</b> Medidas de Precisão</a></li>
<li class="chapter" data-level="4.7.5" data-path="dicio.html"><a href="dicio.html#medidas-de-consistência"><i class="fa fa-check"></i><b>4.7.5</b> Medidas de consistência</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocesso.html"><a href="preprocesso.html"><i class="fa fa-check"></i><b>5</b> Pré-processamento</a><ul>
<li class="chapter" data-level="5.1" data-path="preprocesso.html"><a href="preprocesso.html#dados-faltantes-e-a-limpeza-de-dados"><i class="fa fa-check"></i><b>5.1</b> Dados faltantes e a Limpeza de dados</a><ul>
<li class="chapter" data-level="5.1.1" data-path="preprocesso.html"><a href="preprocesso.html#tratamento-de-dados-faltantes"><i class="fa fa-check"></i><b>5.1.1</b> Tratamento de dados faltantes</a></li>
<li class="chapter" data-level="5.1.2" data-path="preprocesso.html"><a href="preprocesso.html#outlier"><i class="fa fa-check"></i><b>5.1.2</b> <em>Outlier</em></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="preprocesso.html"><a href="preprocesso.html#transformação-de-dados"><i class="fa fa-check"></i><b>5.2</b> Transformação de dados</a><ul>
<li class="chapter" data-level="5.2.1" data-path="preprocesso.html"><a href="preprocesso.html#tipos-de-datasets"><i class="fa fa-check"></i><b>5.2.1</b> Tipos de <em>datasets</em></a></li>
<li class="chapter" data-level="5.2.2" data-path="preprocesso.html"><a href="preprocesso.html#normpadro"><i class="fa fa-check"></i><b>5.2.2</b> Normalização e padronização</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="preprocesso.html"><a href="preprocesso.html#features-selection---seleção-de-atributos-sa"><i class="fa fa-check"></i><b>5.3</b> Features Selection - Seleção de atributos (SA)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="valid.html"><a href="valid.html"><i class="fa fa-check"></i><b>6</b> Validação de um modelo</a><ul>
<li class="chapter" data-level="6.1" data-path="valid.html"><a href="valid.html#fitt"><i class="fa fa-check"></i><b>6.1</b> <em>Overfitting, Underfitting</em></a><ul>
<li class="chapter" data-level="6.1.1" data-path="valid.html"><a href="valid.html#underfitting-no-cenário-underfitting-o-desempenho-já-é-ruim-no-próprio-treinamento-de-seu-algoritmo."><i class="fa fa-check"></i><b>6.1.1</b> <strong>Underfitting</strong>: No cenário underfitting, o desempenho já é ruim no próprio treinamento de seu algoritmo.</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="valid.html"><a href="valid.html#validação-cruzada"><i class="fa fa-check"></i><b>6.2</b> Validação Cruzada</a></li>
<li class="chapter" data-level="6.3" data-path="valid.html"><a href="valid.html#como-escolher-um-bom-modelo"><i class="fa fa-check"></i><b>6.3</b> Como escolher um bom modelo?</a></li>
<li class="chapter" data-level="6.4" data-path="valid.html"><a href="valid.html#aocroc"><i class="fa fa-check"></i><b>6.4</b> AOC e ROC</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html"><i class="fa fa-check"></i><b>7</b> Algoritmos de Aprendizagem I</a><ul>
<li class="chapter" data-level="7.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#naive-bayes"><i class="fa fa-check"></i><b>7.1</b> Naive Bayes</a><ul>
<li class="chapter" data-level="7.1.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exbayes"><i class="fa fa-check"></i><b>7.1.1</b> Exemplo</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reg"><i class="fa fa-check"></i><b>7.2</b> Regressão</a><ul>
<li class="chapter" data-level="7.2.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#reglin"><i class="fa fa-check"></i><b>7.2.1</b> Análise de Regressão Linear Simples</a></li>
<li class="chapter" data-level="7.2.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regmult"><i class="fa fa-check"></i><b>7.2.2</b> Regressão Linear Múltipla</a></li>
<li class="chapter" data-level="7.2.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#mpl"><i class="fa fa-check"></i><b>7.2.3</b> Modelo de Probabilidade Linear (MPL)</a></li>
<li class="chapter" data-level="7.2.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplo1reg"><i class="fa fa-check"></i><b>7.2.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#GD"><i class="fa fa-check"></i><b>7.3</b> Gradiente Descendente (GD)</a><ul>
<li class="chapter" data-level="7.3.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#exemplos"><i class="fa fa-check"></i><b>7.3.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#regularizacao"><i class="fa fa-check"></i><b>7.4</b> Regularização</a><ul>
<li class="chapter" data-level="7.4.1" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#penalizacoes"><i class="fa fa-check"></i><b>7.4.1</b> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></a></li>
<li class="chapter" data-level="7.4.2" data-path="Algoritmosaprendizagem.html"><a href="Algoritmosaprendizagem.html#elasticnet"><i class="fa fa-check"></i><b>7.4.2</b> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ptII.html"><a href="ptII.html"><i class="fa fa-check"></i><b>8</b> Algoritmos de Aprendizagem II</a><ul>
<li class="chapter" data-level="8.1" data-path="ptII.html"><a href="ptII.html#svm"><i class="fa fa-check"></i><b>8.1</b> Máquina de Vetores Suporte - <em>Support Vectors Machine</em></a><ul>
<li class="chapter" data-level="8.1.1" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.1</b> Classificação de Padrões Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.2" data-path="ptII.html"><a href="ptII.html#margmax"><i class="fa fa-check"></i><b>8.1.2</b> Hiperplano de Separação Ótima / Margem Máxima</a></li>
<li class="chapter" data-level="8.1.3" data-path="ptII.html"><a href="ptII.html#classificação-de-padrões-não-linearmente-separáveis"><i class="fa fa-check"></i><b>8.1.3</b> Classificação de Padrões Não-Linearmente Separáveis</a></li>
<li class="chapter" data-level="8.1.4" data-path="ptII.html"><a href="ptII.html#exemplosvm"><i class="fa fa-check"></i><b>8.1.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ptII.html"><a href="ptII.html#decisiontree"><i class="fa fa-check"></i><b>8.2</b> Árvore de Decisão (<em>Decision Tree</em>)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ptII.html"><a href="ptII.html#extree"><i class="fa fa-check"></i><b>8.2.1</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ptII.html"><a href="ptII.html#AC"><i class="fa fa-check"></i><b>8.3</b> Análise de Componentes Principais</a><ul>
<li class="chapter" data-level="8.3.1" data-path="ptII.html"><a href="ptII.html#autovalores-e-autovetores"><i class="fa fa-check"></i><b>8.3.1</b> Autovalores e Autovetores</a></li>
<li class="chapter" data-level="8.3.2" data-path="ptII.html"><a href="ptII.html#estatísticas"><i class="fa fa-check"></i><b>8.3.2</b> Estatísticas</a></li>
<li class="chapter" data-level="8.3.3" data-path="ptII.html"><a href="ptII.html#a-acp"><i class="fa fa-check"></i><b>8.3.3</b> A ACP</a></li>
<li class="chapter" data-level="8.3.4" data-path="ptII.html"><a href="ptII.html#exemplocp"><i class="fa fa-check"></i><b>8.3.4</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ptII.html"><a href="ptII.html#análise-de-agrupamentos---clusters"><i class="fa fa-check"></i><b>8.4</b> Análise de Agrupamentos - <em>Clusters</em></a><ul>
<li class="chapter" data-level="8.4.1" data-path="ptII.html"><a href="ptII.html#técnicas-hierárquicas-aglomerativas"><i class="fa fa-check"></i><b>8.4.1</b> Técnicas Hierárquicas Aglomerativas</a></li>
<li class="chapter" data-level="8.4.2" data-path="ptII.html"><a href="ptII.html#número-final-de-grupos"><i class="fa fa-check"></i><b>8.4.2</b> Número final de grupos</a></li>
<li class="chapter" data-level="8.4.3" data-path="ptII.html"><a href="ptII.html#técnicas-não-hierárquicas"><i class="fa fa-check"></i><b>8.4.3</b> Técnicas Não Hierárquicas</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ptII.html"><a href="ptII.html#knn-k-vizinhos-mais-próximos-k-nearest-neighbors"><i class="fa fa-check"></i><b>8.5</b> KNN: K-Vizinhos Mais Próximos (<em>K-Nearest Neighbors</em>)</a><ul>
<li class="chapter" data-level="8.5.1" data-path="ptII.html"><a href="ptII.html#exknn"><i class="fa fa-check"></i><b>8.5.1</b> Exemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ptIII.html"><a href="ptIII.html"><i class="fa fa-check"></i><b>9</b> Os métodos <em>Ensemble</em></a><ul>
<li class="chapter" data-level="9.1" data-path="ptIII.html"><a href="ptIII.html#bagging"><i class="fa fa-check"></i><b>9.1</b> Bagging</a></li>
<li class="chapter" data-level="9.2" data-path="ptIII.html"><a href="ptIII.html#rf"><i class="fa fa-check"></i><b>9.2</b> Floresta Aleatória - <em>Random Forest</em></a></li>
<li class="chapter" data-level="9.3" data-path="ptIII.html"><a href="ptIII.html#boosting"><i class="fa fa-check"></i><b>9.3</b> grad boosting</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="redesneurais.html"><a href="redesneurais.html"><i class="fa fa-check"></i><b>10</b> Redes Neurais</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado com bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Algoritmosaprendizagem" class="section level1">
<h1><span class="header-section-number">Capítulo 7</span> Algoritmos de Aprendizagem I</h1>
<p><em>Existe uma infinidade de algoritmos utilizados em machine learning, cada um com uma finalidade específica. Há também características que podem inviabilizar a escolha do modelo mais preciso para determinado problema, como a utilização alto poder computacional.</em></p>
<div id="naive-bayes" class="section level2">
<h2><span class="header-section-number">7.1</span> Naive Bayes</h2>
<p>Antes de falarmos sobre este algoritmo, vamos para o conceito matemático. Em (<a href="dicio.html#dicio">4</a>) tratamos do Teorema de Bayes para <span class="math inline">\(n\)</span> atributos. Colocando-o como probabilidade condicional:
<span class="math display">\[p(A|B_{1},...,B_{n}) = \]</span>
<span class="math display" id="eq:bayescond">\[\begin{equation} 
  p(A)p(B_{1}|A)p(B_{2}|A,B_{1}),p(B_{3}|A,B_{1},B_{2})...p(B_{n}|A,B_{1},B_{2},...,B_{n−1})
  \tag{7.1}
\end{equation}\]</span></p>
<p>Assumindo que cada atributo <span class="math inline">\(B_i\)</span> é condicionalmente independente de todos os outros <span class="math inline">\(B_j\)</span> para <span class="math inline">\(j\neq i\)</span> e <span class="math inline">\(p(B_i|A,B_j)=p(B_i|A)\)</span> o modelo poderá ser expresso como:</p>
<p><span class="math display" id="eq:bayesprodutorio">\[\begin{equation} 
  p(A_k|B_1,...,B_n)=p(A_k)p(B_1|A_k)p(B_2|A_k),...=p(A_k)\prod_i^n p(B_i|A_k) \ k ∈{1,...,k}
  \tag{7.2}
\end{equation}\]</span></p>
<p>Por fim para podermos classificar, aplicamos argumento de máxima para otimizarmos a função, assim obtém-se o classificador de Naive Bayes:</p>
<p><span class="math display" id="eq:naivebayes">\[\begin{equation} 
  \mbox{classificador} \ \hat{y}=argmax \ p(A_k)\displaystyle \prod_{i=1}^n p(B_i|A_k) \ \ k ∈{1,...,k}
  \tag{7.3}
\end{equation}\]</span></p>
<p>Lembrando que para cada atributo, a sua distribuição de probabilidades é assumida como normal.</p>
<p>O Naive Bayes é uma técnica de classificação baseado no teorema de Bayes com uma suposição de independência entre os preditores, ou seja, este classificador assume que a presença de uma característica particular em uma classe não está relacionada com a presença de qualquer outro fator. Por exemplo, uma fruta verde, redonda e com um tamanho de diâmetro X pode ser uma melancia, porém mesmo que estas variáveis dependam uns dos outros e de outras características, todas estas propriedades contribuem de forma independente para a probabilidade de que seja uma melancia. Este modelo é muito utilizado devido que é fácil de construir e particularmente útil para grandes volumes de dados. Porém a própria independência entre os preditores a torna desvantajosa na prática e caso haja variáveis categóricas num conjunto de dados de teste que não forem treinadas, o modelo não irá estimar estas novas variáveis.</p>
<div id="exbayes" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Exemplo</h3>
<p>No diagnóstico de uma nova doença e que foi feito testes em 100 pessoas aleatórias (exemplo de <span class="citation">Orgânica Digital (<a href="#ref-organica">2019</a>)</span>).</p>
<p>Após coletarmos a análise, descobrimos que das 100 pessoas, 20 possuíam a doença (20%) e 80 pessoas estavam saudáveis (80%), sendo que das pessoas que possuíam a doença, 90% receberam o resultado positivo no teste da doença, e 30% das pessoas que não possuíam a doença também receberam o teste positivo. Caso uma nova pessoa realizar o teste e receber um resultado positivo, qual a probabilidade de ela realmente possuir a doença?</p>
<div class="figure" style="text-align: center"><span id="fig:bayes"></span>
<img src="Figuras/bayes.png" alt="Dados coletados de uma amostra de 100 pessoas aleatórias." width="70%" />
<p class="caption">
Figura 7.1: Dados coletados de uma amostra de 100 pessoas aleatórias.
</p>
</div>

<p>Com o algoritmo de Naive Bayes, buscamos encontrar uma probabilidade da pessoa possuir a doença dado que ela recebeu um resultado positivo, multiplicando a probabilidade de possuir a doença pela probabilidade de “receber um resultado positivo, dado que tem a doença”. De mesmo modo verificar a probabilidade de não possuir a doença dado que recebeu um resultado positivo.</p>
<p>Ou seja, ao caso de ter a doença dado que o resultado deu positivo:
<span class="math display">\[P(doença|positivo) = 20\% . 90\% \]</span> <span class="math display">\[P(doença|positivo) = 0,2 * 0,9 \]</span> <span class="math display">\[P(doença|positivo) = 0,18\]</span>
Para o caso de não ter a doença, dado que deu positivo:
<span class="math display">\[P(não \ doença|positivo) = 80\%.30\%\]</span>
<span class="math display">\[P(não \ doença|positivo) = 0,8 * 0,3\]</span>
<span class="math display">\[P(não\ doença|positivo) = 0,24\]</span>
Após isso precisamos normalizar os dados, para que a soma das duas probabilidades resulte 1 (100%). Como vimos em pré-processamento <a href="preprocesso.html#preprocesso">5</a>, a <strong>Normalização por reescala</strong> por meio de um valor mínimio e um máximo, gera um novo intervalo onde os valores de um atributo estão contidos. Um intervalo entre 0 e 1. Portanto, dividimos o resultado pela soma das duas probabilidades.</p>
<p><span class="math display">\[P(doença|positivo) = 0,18/(0,18+0,24) = 0,4285\]</span>
<span class="math display">\[P(não doença|positivo) = 0,24/(0,18+0,24) = 0,5714\]</span>
Logo, podemos concluir que se o resultado do teste da nova pessoa for positivo, ela possui aproximadamente 43% (0,4285) de chance de estar doente.</p>
<p>Naive Bayes é uma técnica de classificação baseado no teorema de Bayes com uma <strong>suposição de independência entre os preditores</strong> diferentemente do caso em <a href="dicio.html#dicio">4</a> (Teorema de Bayes), ou seja, O Naive Bayes assume que a presença de uma característica particular em uma classe não está relacionada com a presença de qualquer outro fator. Ao caso da melancia, uma fruta verde, redonda e com um tamanho de diâmetro X é possível ser ela, porém mesmo que estas variáveis dependam uma das outras e de outras características, elas contribuem de forma independente para a probabilidade de que seja uma melancia. É um modelo simples de construir e útil para grandes volumes de dados. Porém a própria independência entre os preditores a torna desvantajosa para apliação prática e que variáveis categóricas num conjunto de dados de teste que não foram treinadas, não irá estimar essa nova variável.</p>
<p>Por isso <em>Naive</em> vem do significado “ingênuo”, pois como a Figura <a href="Algoritmosaprendizagem.html#fig:naive">7.2</a> demonstra, os atributos contribuem de forma independente para a probabilidade de A.</p>
<div class="figure" style="text-align: center"><span id="fig:naive"></span>
<img src="Figuras/naive.png" alt="Gráfico de Probabilidade x Entropia." width="70%" />
<p class="caption">
Figura 7.2: Gráfico de Probabilidade x Entropia.
</p>
</div>


</div>
</div>
<div id="reg" class="section level2">
<h2><span class="header-section-number">7.2</span> Regressão</h2>
<p>A análise de variância, pressupõe a independência dos efeitos dos diversos tratamentos utilizados no experimento. Quando a hipótese não é verificada, necessitamos refletir a dependência entre os efeitos dos tratamentos. No caso de experimentos quantitativos, frequentemente justifica a existência da equação de regressão, que une os valores dos tratamentos aos analisados. Em grande parte, trata de estimação e/ou previsão do valor médio (para população) da variável dependente com base nos valores conhecidos da variável explanatória. É uma análise supervisionada.</p>
<div id="reglin" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Análise de Regressão Linear Simples</h3>
<p>Como na prática não conseguimos análisar uma população, trabalhamos em cima de amostras e estimamos para o todo, para que possamos fazer uma aproximação. Partimos da ideia de estimarmos uma função com dados amostrais com o menor erro possível. Portanto, o <span class="math inline">\(Y_i\)</span> (população) observado pode ser expresso como:</p>
<p><span class="math display" id="eq:frp">\[\begin{equation}
    Y_i=\hat{Y_i}+\hat{\mu_i}
    \tag{7.4}
\end{equation}\]</span></p>
<p>E o modelo para função de regressão amostral:
<span class="math display" id="eq:fra">\[\begin{equation}
    Y_i=\hat{\beta_0}+\hat{\beta_1}X_i+\hat{\mu_i}
    \tag{7.5}
\end{equation}\]</span></p>
<p>em que:</p>
<p><span class="math inline">\(\hat{Y_i}\)</span> é o valor observado com <span class="math inline">\(i\)</span> níveis de <span class="math inline">\(X\)</span> (estimador da esperança <span class="math inline">\(E(Y|Xi)\)</span>), <span class="math inline">\(\hat{\beta_0}\)</span> a constante de regressão estimado e intercepto de <span class="math inline">\(\hat{Y}\)</span>, <span class="math inline">\(\hat{\beta_1}\)</span> o coeficiente de regressão estimado que seria a variação de <span class="math inline">\(\hat{Y}\)</span> em função da variação de cada unidade de <span class="math inline">\(X\)</span>, <span class="math inline">\(X_i\)</span> com <span class="math inline">\(i\)</span> níveis da variável independente e <span class="math inline">\(\hat{\mu_i}\)</span> é o erro associado à distância entre o valor observado e o correspondente ponto na curva. Note que os “chapéis” em cima das variáveis é utilizado quando referimos a estimações, ou seja, são variáveis de dados amostrais e não a população.</p>
<p>Mas como estimaremos os parâmetros da função de forma que fique mais próxima possível e com o menor erro? Com o método dos <strong>Mínimos Quadrados Ordinários (MMQ)</strong> atribuído ao Carl Friedrich Gauss - matemático alemão - torna-se possível estimar os melhores <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> que minimizam os erros.</p>
<p>Como não podemos observar a função de regressão populacional (FRP), precisamos estimá-lo por meio da função de regressão amostral:
<span class="math display">\[Y_i=\hat{\beta_0}+\hat{\beta_1}X_i+\hat{\mu_i} \]</span>
<span class="math display">\[Y_i=\hat{Y_i}+\hat{\mu_i}\]</span>
<span class="math display">\[\mbox{Logo temos que} \rightarrow \ \hat{\mu_i}=Y_i-\hat{\beta_0}-\hat{\beta_1} X_i\]</span></p>
<p>Podemos ver que os erros <span class="math inline">\(\hat{\mu_i}\)</span> (resíduos) são basicamente as diferenças entre os valores observados e estimados de <span class="math inline">\(Y\)</span>. Ao caso de dados com <span class="math inline">\(n\)</span> pares de observações de <span class="math inline">\(Y\)</span> e <span class="math inline">\(X\)</span>, queremos encontrar a FRA que se encontra o mais próximo possível do <span class="math inline">\(Y\)</span> observado, ou seja, escolher a
FRA de modo que a soma dos resíduos <span class="math inline">\(\sum \hat{\mu}_i=\sum(Y_i-\hat{Y_i})\)</span> seja a menor possível. Porém, como se pode ver pelo diagrama de dispersão na Figura <a href="Algoritmosaprendizagem.html#fig:mmq">7.3</a>, os erros possuem a mesma importância com variações entre sinais positivos e negativos e sua somatória será zero. Isso dificultaria a possibilidade de minimizarmos.</p>
<div class="figure" style="text-align: center"><span id="fig:mmq"></span>
<img src="Figuras/mmq.PNG" alt="Critério do minímos quadrados Gujarati and Porter (2011)." width="70%" />
<p class="caption">
Figura 7.3: Critério do minímos quadrados <span class="citation">Gujarati and Porter (<a href="#ref-gujarati2011econometria">2011</a>)</span>.
</p>
</div>

<p>Para evitarmos isso, utilizamos o critério dos mínimos quadrados, de modo que elevamos os resíduos ao quadrado. Fazendo isso, o método dá mais peso aos resíduos (não irão mais se anular), podendo visualizar melhor o “tamanho” do erro total e obter propriedades estatísticas mais desejáveis.
<span class="math display">\[\sum \hat{\mu}^2_i=\sum(Y_i-\hat{Y_i})^2 \]</span>
<span class="math display" id="eq:mmqeq">\[\begin{equation}
    = \sum (Y_i-\hat{\beta_0}-\hat{\beta_1})X_i^2 
    \tag{7.6}
\end{equation}\]</span></p>
<p>O método dos mínimos quadrados nos oferece estimativas únicas de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> que proporcionam o menor valor possível (encontrando <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span>) de <span class="math inline">\(\sum \hat{\mu}_i\)</span>. Por meio de cálculo diferenciável (recomendo o leitor interessado em se aprofundar na definição matemática buscar literaturas em foco estatístico ler, como por exemplo de <span class="citation">Gujarati and Porter (<a href="#ref-gujarati2011econometria">2011</a>)</span>) é possível obter:</p>
<p><span class="math display" id="eq:sumyi">\[\begin{equation}
    \sum Y_i=n\hat{\beta_0} + \hat{\beta_1} \sum X_i
    \tag{7.7}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:sumyixi">\[\begin{equation}
    \sum Y_i X_i=\hat{\beta_0} \sum X_i + \hat{\beta_1} \sum X_i^2
    \tag{7.8}
\end{equation}\]</span></p>
<p>Para encontrarmos os valores dos <span class="math inline">\(\beta&#39;s\)</span> a fim de minimizar. Precisamos aplicar a derivada parcial:</p>
<p><span class="math display">\[\mbox{temos que:  }\ \sum\hat{\mu}_i^2=\sum(Y_i-\hat{\beta_0}-\hat{\beta}_1X_i)^2\]</span>
<span class="math display">\[\frac{\partial(\sum\hat{\mu})}{\partial \hat{\beta}_0}\rightarrow-2\sum\hat{\mu}=-2\sum(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)\]</span>
<span class="math display">\[\frac{\partial(\sum\hat{\mu})}{\partial \hat{\beta}_1}\rightarrow-2\sum\hat{\mu}X_i=-2\sum(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)X_i\]</span>
<span class="math display">\[(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0 \ \ \ (I)\]</span>
<span class="math display">\[(Y_i X_i-\hat{\beta}_0X_i-\hat{\beta}_1X_i^2)=0 \ \ \ (II)\]</span>
Expandindo o somatório de (I):
<span class="math display">\[-\sum Y_i+ n\hat{\beta}_0+\hat{\beta}_1\sum X_i=0\]</span>
Note que se isolarmos <span class="math inline">\(\sum Y_i\)</span>, obtém-se a equação <a href="Algoritmosaprendizagem.html#eq:sumyi">(7.7)</a>. Ao isolarmos <span class="math inline">\(\hat{\beta}_0\)</span> obtemos:
<span class="math display">\[\hat{\beta}_0=\frac{\sum Y_i}{n}-\hat{\beta}_1\frac{X_i}{n} \ \ \ (III)\]</span>
<span class="math display" id="eq:betazero">\[\begin{equation}
\hat{\beta}_0=\overline{Y}_i-\hat{\beta}_1\overline{X}_i
\tag{7.9}
\end{equation}\]</span></p>
<p>Expandindo (II), obtemos:
<span class="math display">\[-\sum X_iY_i+\hat{\beta}_0\sum X_i+\hat{\beta}_1\sum X_i^2=0 \ \ \ (IV)\]</span>
Note também que isolando <span class="math inline">\(\sum X_i Y_i\)</span> teremos a equação <a href="Algoritmosaprendizagem.html#eq:sumyixi">(7.8)</a>.</p>
<p>Por fim, substituindo (III) em (IV) e manipulando algebricamente, também temos <span class="math inline">\(\hat{beta}_1\)</span>:
<span class="math display">\[\hat{\beta}_1=\frac{n\sum X_iY_i-\sum X_i \sum Y_i}{n\sum X_i^2-(\sum X_i)^2}=\frac{\sum (X_i-\overline{X})(Y_i-\overline{Y})}{\sum(X_i-\overline{X})^2}\]</span>
<span class="math display" id="eq:betaum">\[\begin{equation}
\hat{\beta_1}=\frac{\sum x_i y_i}{\sum x_i^2}
\tag{7.10}
\end{equation}\]</span>
em que <span class="math inline">\(\overline{X}\)</span> e <span class="math inline">\(\overline{Y}\)</span> são as médias amostrais de <span class="math inline">\(X\)</span> e de <span class="math inline">\(Y\)</span> e que <span class="math inline">\(x_i=(X_i-\overline{X})\)</span> e <span class="math inline">\(y_i=(Y_i-\overline{Y})\)</span>.</p>
<p>Os estimadores são conhecidos como <strong>estimadores de mínimos quadrados</strong>. Cada estimador proporciona um único valor do parâmetro populacional relevante e que, após obtê-los, torna-se possível elaborar a linha de regressão amostral que passa pelas médias amostrais de <span class="math inline">\(X\)</span> e de <span class="math inline">\(Y\)</span>.</p>
<p>Para obtermos o erro padrão da estimativa como uma medida resumida da “qualidade do ajustamento” da linha de regressão, podemos pela expressão:</p>
<p><span class="math display" id="eq:desvioreg">\[\begin{equation}
\hat{\sigma}=\sqrt{\frac{\sum \hat{\mu}_i^2}{n-2}}
\tag{7.11}
\end{equation}\]</span></p>
<p>Lembrando que para obtermos a variância, basta elevarmos ao quadrado.</p>
<p>Para que seja feito o modelo de regressão, ela depende das premissas: independência das variáveis erro, homogeneidade das variâncias, normalidade e relação linear entre as variáveis <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Para a independência do termo de erro, os valores assumidos pelo regressor <span class="math inline">\(X\)</span> podem ser fixos ou mudar de acordo com variável dependente <span class="math inline">\(Y\)</span>. Para o o caso de não serem fixos, a covariância entre a variável e a o termo erro precisa ser zero (independentes). <span class="math inline">\(cov(X_i,\mu_i)=0\)</span>.</p></li>
<li><p>O valor médio do erro <span class="math inline">\(\mu_i\)</span> é zero. Ou seja: <span class="math inline">\(E(\mu_i|X_i)=0\)</span> e se <span class="math inline">\(X\)</span> não aleatório <span class="math inline">\(E(\mu)i=0\)</span>. Isto implica de que não haja viés de especificação do modelo diante da análise empírica, os fatores não inclusos específicamente no modelo agrupados em <span class="math inline">\(\mu_i\)</span> não afetam sistematicamente o valor médio de <span class="math inline">\(Y\)</span> <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>)</span>.</p></li>
<li><p>Variância constante de <span class="math inline">\(\mu_i\)</span> (<strong>Homocedasticidade</strong>). A variância do termo de erro será a mesma independente do valor de <span class="math inline">\(X\)</span>.</p></li>
<li><p>Entre os termos de erro, dados qualquer valor de <span class="math inline">\(X\)</span>, não há autocorrelação. Ou seja <span class="math inline">\(cov(\mu_i,\mu_j|X_i \ \mbox{e} \ X_j)=0\)</span>, em que <span class="math inline">\(i\)</span> e <span class="math inline">\(j\)</span> são duas observações diferentes <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>)</span>. Para entender melhor, na figura a seguir, não queremos (a) e (b).</p></li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:autocorrmu"></span>
<img src="Figuras/autocorrmu.png" alt="Padrões de correlação entre os termos de erro. (a) correlação serial positiva; (b) correlação serial negativa; (c) correlação zero Gujarati and Porter (2011)." width="70%" />
<p class="caption">
Figura 7.4: Padrões de correlação entre os termos de erro. (a) correlação serial positiva; (b) correlação serial negativa; (c) correlação zero <span class="citation">Gujarati and Porter (<a href="#ref-gujarati2011econometria">2011</a>)</span>.
</p>
</div>

<p>Para o caso das observações <span class="math inline">\(X&#39;s\)</span> com <span class="math inline">\(Y\)</span> precisa-se haver correlações entre si.</p>
<ol start="5" style="list-style-type: decimal">
<li><p>O número de observações <span class="math inline">\(n\)</span> deve ser maior que o número de parâmetros a serem estimados e que os valores de <span class="math inline">\(X\)</span> em uma amostra não devem ser os mesmos ou muito discrepantes (poderá haver problemas de <em>outliers</em> que será apresentado em <a href="preprocesso.html#outlier">5.1.2</a>).</p></li>
<li><p>Uma das propriedades da distribuição normal é que qualquer função linear que possui variáveis com distribuição normal também é normalmente distribuída; as variáveis com distribuição normal, covariância ou correlações iguais a zero, indicam que há independência das variáveis presentes na amostra. Por isso é importante a etapa de pré-processamento. Aos interessados, recomendo-os buscar em algumas literaturas alguns testes de normalidade, como a de <span class="citation">(Shapiro and Wilk <a href="#ref-shapiro1965analysis">1965</a>)</span>, para verificar o comportamento do conjunto de dados.</p></li>
</ol>
<p>Segundo o <strong>Teorema de Gauss-Markov</strong>, dadas as premissas do modelo clássico de regressão linear, os estimadores de mínimos quadrados dos estimadores não viesados possuem variância mínima. Podemos dizer que são o melhor estimador linear não viesado <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>)</span>.</p>
<ul>
<li><strong>Coeficiente de determinação <span class="math inline">\(r^2\)</span>: medir a qualidade de seu ajuste</strong></li>
</ul>
<p>Estimamos os parâmetros e o erro da função, agora precisamos considerar a <strong>qualidade do ajuste</strong> da linha de regressão ajustada a um conjunto de dados, ou seja, vamos descobrir quão “bom” o ajuste dessa linha de regressão
amostral é adequada aos dados. Se todas as observações estivessem exatamente em cima da linha de regressão, seria “perfeito”, o que raramente acontece e provávelmente seria um problema de <strong>Overfitting</strong> (será apresentado em <a href="valid.html#valid">6</a> para verificarmos a validade do modelo). O coeficiente de terminação <span class="math inline">\(r^2\)</span> é um medida que diz quanto a linha de regressão
amostral ajusta-se aos dados.</p>
<p>Para entendermos melhor, vamos visualizar por Diagrama de Venn <span class="citation">(Kennedy <a href="#ref-kennedy1981ballentine">1981</a>)</span>. O círculo <span class="math inline">\(Y\)</span> representa a variação da variável dependente <span class="math inline">\(Y\)</span> e o círculo <span class="math inline">\(X\)</span>, a variação da variável explanatória <span class="math inline">\(X\)</span> como vimos em regressão linear. A área sombreada indica o quanto em que a variação de <span class="math inline">\(Y\)</span> é explicada pela variação de <span class="math inline">\(X\)</span>. Quanto maior a área sobreposta, maior a parte da variação de <span class="math inline">\(Y\)</span> é explicada por <span class="math inline">\(X\)</span>. O coefiente de determinação <span class="math inline">\(r^2\)</span> é apenas a medida numérica dessa sobreposição. Na Figura <a href="Algoritmosaprendizagem.html#fig:ballentine">7.5</a>, conforme move-se da esquerda para a direita, a sobreposição aumenta, ou seja, uma proporção cada vez maior da variação de <span class="math inline">\(Y\)</span>
é explicada por <span class="math inline">\(X\)</span> (o <span class="math inline">\(r^2\)</span> aumenta). Sem sobreposição, <span class="math inline">\(r^2=0\)</span> e com total sobreposição, <span class="math inline">\(r^2=1\)</span>, pois 100% da variação de <span class="math inline">\(Y\)</span> é explicada por <span class="math inline">\(X\)</span>. Portanto o coefienciente situa-se no intervalo entre 0 e 1.</p>
<div class="figure" style="text-align: center"><span id="fig:ballentine"></span>
<img src="Figuras/ballentine.png" alt="Coeficiente de determinação, prossegue a sobreposição de (a): \(r^2=0\) até (f) \(r^2=1\) (Gujarati and Porter 2011)." width="70%" />
<p class="caption">
Figura 7.5: Coeficiente de determinação, prossegue a sobreposição de (a): <span class="math inline">\(r^2=0\)</span> até (f) <span class="math inline">\(r^2=1\)</span> <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>)</span>.
</p>
</div>

<p>Podemos chegar ao coeficiente de determinação apenas por manipulação algébrica:</p>
<p><span class="math display">\[\mbox{sabemos que:} \ y_i=\hat{y}_i+\hat{\mu}_i\]</span>
<span class="math display">\[\mbox{elevando ao quadrado e somando a amostra:} \ \sum y^2_i=\sum \hat{y}^2_i+\sum \hat{\mu}^2_i+2\sum \hat{y}_i \hat{\mu}_i \]</span>
<span class="math display">\[\mbox{como} \ \sum \hat{\mu}_i=0, \ \mbox{temos que:}\ \sum y^2_i= \hat{y}^2_i+\sum \hat{\mu}^2_i \]</span>
<span class="math display">\[\sum y^2_i=\hat{\beta}^2_1 \sum x_i^2+\sum \hat{\mu}^2_i \]</span></p>
<p><span class="math display" id="eq:sqt">\[\begin{equation}
    \mbox{podemos dizer} \ SQT=SQE+SQR
    \tag{7.12}
\end{equation}\]</span></p>
<p>sendo SQT a soma total dos quadrados, SQE a soma do quadrados explicados e SQR soma dos quadrados dos resíduos.</p>
<p><span class="math display">\[\mbox{dividindo a equação anterior por SQT:}\]</span>
<span class="math display">\[1=\frac{SQE}{SQT}+\frac{SQR}{SQT} \]</span>
<span class="math display">\[\mbox{definindo}\ r^2 \ \mbox{como:} \ \frac{SQE}{SQT} \]</span></p>
<p><span class="math display" id="eq:coefdet">\[\begin{equation}
    \mbox{obtemos:} \ r^2=1-\frac{SQR}{SQT} \rightarrow 1 - \frac{\sum \hat{\mu}_i}{\sum (Y_i - \overline{Y}_i)^2}
    \tag{7.13}
\end{equation}\]</span></p>
<p><span class="math inline">\(r^2\)</span> portanto, mede a proporção ou percentual da variação total de Y explicada pelo modelo de regressão. Por manipulação algébrica, podemos verificar também que <span class="math inline">\(r^2=\hat{\beta}^2_1(\frac{S^2_x}{S^2_y})\)</span>, sendo <span class="math inline">\(S^2_x\ \mbox{e} \ S^2_y\)</span> as respectivas variâncias amostrais de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<p>Note que ao aplicarmos a raiz quadrada no coeficiente de determinação obtemos o coeficiente de correlação visto em <a href="dicio.html#medidasdep">4.7.1</a>, que mede o grau de associação entre duas variáveis.</p>
<p><span class="math display">\[r=\pm \sqrt{r^2}\]</span>
- <strong>Análise de Variância na Regressão</strong></p>
<p>Anteriormente vimos que <span class="math inline">\(\sum y^2_i=\hat{\beta}^2_1 \sum x_i^2+\sum \hat{\mu}^2_i\)</span> podem ser expressas como <span class="math inline">\(STQ=SQE+SQR\)</span>, a soma total de quadrados (STQ) composta pela soma dos quadrados expicados pela regressão (SQE) e a soma do quadrado dos resíduos (SQR). Como sabe-se sobre análise de variância, associados a eles encontra-se os graus de liberdade, onde <span class="math inline">\(STQ\)</span> possui <span class="math inline">\(n-1\)</span> g.l ao calcular a média da amostra <span class="math inline">\(\overline{Y}\)</span>. A <span class="math inline">\(SQR\)</span> tem <span class="math inline">\(n-2\)</span> g.l (ao caso de duas variáveis com o intercepto presente) e a <span class="math inline">\(SQE\)</span> possui 1 g.l (ao caso de duas variáveis) pela função <span class="math inline">\(SQE=\hat{\beta_1^2}\sum x^2_i\)</span>.</p>
<p>A tabela <strong>ANOVA</strong> com a hipótese nula <span class="math inline">\(H_0: \beta_1=0\)</span> para verificar a existência da relação e a influência de <span class="math inline">\(X\)</span> em <span class="math inline">\(Y\)</span> ficará como:</p>
<table>
<caption><span id="tab:anovareg">Tabela 7.1: </span> Tabela ANOVA para um modelo de regressão de duas variáveis.</caption>
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th>C.V</th>
<th>G.L</th>
<th>S.Q</th>
<th>Q.M</th>
<th>F.C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regressão (SQE)</td>
<td>1</td>
<td><span class="math inline">\(\sum \hat{y}_i^2=\hat{\beta}_1^2 \sum x_i^2\)</span></td>
<td><span class="math inline">\(\hat{\beta}_1^2 \sum x_i^2\)</span></td>
<td><span class="math inline">\(\frac{Q.M.SQE}{Q.M.SQR}\)</span></td>
</tr>
<tr class="even">
<td>Erro (SQR)</td>
<td>n-2</td>
<td><span class="math inline">\(\sum \hat{\mu}_i^2\)</span></td>
<td><span class="math inline">\(\frac{\sum \mu_i^2}{n-2}=\hat{\sigma}^2\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total (STQ)</td>
<td>n-1</td>
<td><span class="math inline">\(\sum \hat{y}_i^2\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Não esqueça:</strong> dependendo das variáveis em estudo é possível que haja comportamento polinomial ao observarmos no gráfico, podendo ser quadrática, cúbica, etc. Os procedimentos são os mesmos de que linear, mas basicamente incluímos a variável e seu respectivo grau. Dependendo do comportamento muitas vezes é mais fácil ao invés e manter em exponencial (não linear), linearizarmos a função por meio dos logaritmos, semi-logaritmicos entre outros. Isso faz com que temos menos trabalho para tratarmos e estimarmos os parâmetros da função exponencial.</p>
<div class="figure" style="text-align: center"><span id="fig:explog"></span>
<img src="Figuras/explog.PNG" alt="Em (a) curva de função exponencial e (b) após aplicarmos o logaritmo (Gujarati and Porter 2011)." width="70%" />
<p class="caption">
Figura 7.6: Em (a) curva de função exponencial e (b) após aplicarmos o logaritmo <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>)</span>.
</p>
</div>

<p>Atualmente é bem comum utilizarmos o modelo <strong>log-log</strong>, pois seu coeficiente angular <span class="math inline">\(\beta_i\)</span> mede a <strong>elasticidade</strong> de <span class="math inline">\(Y\)</span> em relação a <span class="math inline">\(X\)</span>, ou seja, a variação percentual de <span class="math inline">\(Y\)</span> correspondente a uma variação percentual em <span class="math inline">\(X\)</span>. Por exemplo: na Figura <a href="Algoritmosaprendizagem.html#fig:explog">7.6</a> se <span class="math inline">\(Y\)</span> representa a quantidade demandada de camisetas e <span class="math inline">\(X\)</span> seu preço unitário. Em (a) temos a relação da quantidade de demanda por camisetas e o preço, mas com a transformação logaritmica teremos a estimação de <span class="math inline">\(-\beta_2\)</span> (pois é uma reta descendente) que indica a elasticidade preço (variação em <span class="math inline">\(ln(Y)\)</span> por unidade de variação em <span class="math inline">\(ln(X)\)</span>). Portanto teríamos a variação percentual da quantidade demandada de camisetas dada uma variação percentual do preço. Atente-se: <strong>porcentagem</strong> <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>)</span>.</p>
</div>
<div id="regmult" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Regressão Linear Múltipla</h3>
<p>Na prática deparamos com muitas outros fatores que podem influenciar em sua variável dependente <span class="math inline">\(Y\)</span>. Portanto são acrescentadas dentro de seu modelo de regressão mais variáveis, o que é conhecido como <strong>Regressão Linear Múltipla</strong>, nada mais do que uma ampliação da regressão linear simples. Num modelo, por exemplo, com três variáveis (caso mais simples) pode ser expressa para a amostra como:</p>
<p><span class="math display" id="eq:regmult">\[\begin{equation}
    Y_i=\hat{\beta_0}+\hat{\beta_{1}}X_{1i}+\hat{\beta_{2}}X_{2i}+\mu_i
    \tag{7.14}
\end{equation}\]</span></p>
<p>Da mesma forma, <span class="math inline">\(Y_i\)</span> a variável dependente, <span class="math inline">\(X_{2}\)</span> e <span class="math inline">\(X_{3}\)</span> as independentes explanatórias (explicativa), <span class="math inline">\(\mu_i\)</span> o erro estocático e <span class="math inline">\(i\)</span> para indicar <span class="math inline">\(i\)</span>-ésima observação. Ao caso dos parâmetros, <span class="math inline">\(\beta_0\)</span> como intercepto, <span class="math inline">\(\beta_1\)</span> e <span class="math inline">\(\beta_2\)</span> os <strong>coeficientes parciais de regressão/angulares</strong>. <span class="math inline">\(\beta_2\)</span> mede a variação no valor médio de <span class="math inline">\(Y\)</span> (esperança de <span class="math inline">\(Y\)</span>), por unidade de variação em <span class="math inline">\(X_2\)</span>, mantendo <span class="math inline">\(X_3\)</span> constante, ou seja, traz o efeito “direto” de uma unidade de variação em <span class="math inline">\(X_2\)</span> sobre o valor médio de <span class="math inline">\(Y\)</span>, excluindo o efeito de <span class="math inline">\(X_3\)</span> na média de <span class="math inline">\(Y\)</span>. De mesmo modo, <span class="math inline">\(X_3\)</span> com <span class="math inline">\(X_2\)</span> constante.</p>
<p>A regressão múltipla pressupõe as mesma hipóteses de que a regressão linear simples, porém como acréscimo - e muito importante- que as variáveis independentes devem estar <strong>ausentes de multicolinearidade</strong>, ou seja, não devem haver relação linear entre si. Se essa relação linear existir entre <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_3\)</span> <strong>são colineares</strong> ou <strong>linearmente dependentes</strong>, do contrário <strong>linearmente independentes</strong>. Caso a multicolinearidade for perfeita, os coeficientes de regressão das variáveis <span class="math inline">\(X\)</span> serão indeterminados e seus erros padrão, infinitos. Se a multicolinearidade for menos que perfeita, serão determinado mas com grandes erros padrão (em relação aos próprios
coeficientes), o que trará um modelo ruim para sua estimação.</p>
<p>Para medirmos a multicolinearidade é comum a análise de <strong>correlação de pearson</strong> entre todas as variáveis, como mencionada em <strong>Medidas de Dependência <a href="dicio.html#medidasdep">4.7.1</a></strong>, ou analisar a ocorrência de intervalo de confiança mais amplo, verificação de razões “t” insignificantes mesmo que seu <span class="math inline">\(R^2\)</span> esteja alto, parâmetros estimados muitos sensíveis a qualquer alteração de dados e comumente utilizado para verificar o <strong>fator de inflação de variância (FIV)</strong> <span class="citation">(Montgomery, Peck, and Vining <a href="#ref-montgomery2012introduction">2012</a>)</span>, que pode ser expressa como:</p>
<p><span class="math display" id="eq:vif">\[\begin{equation}
    VIF_j=\frac{1}{1-r^2_j} \ \ j=1,2,...,p
    \tag{7.15}
\end{equation}\]</span></p>
<p>sendo <span class="math inline">\(r^2\)</span> o coeficiente de correlação ao quadrado e <span class="math inline">\(j\)</span> para referir as variáveis. Por exemplo, se <span class="math inline">\(r^2_{23}\)</span>, refe-se ao coeficiente de correlação entre as variáveis <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_3\)</span>. Segundo ,quando este indicador apresenta o valor acima de cinco, é possível a existência de multicolinearidade <span class="citation">(Maroco <a href="#ref-maroco2014analise">2014</a>)</span>.</p>
<p>De mesmo modo que em regressão linear simples, são estimados os MQO, Máxima verossimilhança e o <strong>coeficiente de determinação múltiplo <span class="math inline">\(R^2\)</span></strong> (mesma interpratação para regressão linear simples <span class="math inline">\(r^2\)</span>) para que se obtenha a melhor aproximação possível.</p>
</div>
<div id="mpl" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Modelo de Probabilidade Linear (MPL)</h3>
<p>Considerando um modelo típico de regressão linear simples:
<span class="math display">\[Y_i=\beta_0+\beta_1 X_i+\mu_i \]</span></p>
<p>em que <span class="math inline">\(X =\)</span>sua renda e <span class="math inline">\(Y=1\)</span> de que você compre um celular e <span class="math inline">\(0\)</span> não compre. Como o regressando é binário, ou dicotômico, chamamos de probabilidade linear (MPL). Pode ser interpretada como probabilidade condicional de que o evento ocorra dado <span class="math inline">\(X_i\)</span>, isto é, Pr <span class="math inline">\((Yi = 1 | Xi)\)</span>. Neste caso, é a probabilidade de você comprar um celular e cuja renda é dado por <span class="math inline">\(X_i\)</span>.</p>
<p>Para entender este modelo, vamos supor <span class="math inline">\(E(\hat{\mu}_i)=0\)</span> para evitarmos estimadores tendenciosos (erros). Portanto:</p>
<p><span class="math display" id="eq:regcond">\[\begin{equation}
    E(Y_i|X_i)=\beta_0+\beta_1 X_i
    \tag{7.16}
\end{equation}\]</span></p>
<p>Com <span class="math inline">\(P_i=\)</span>probabilidade de que <span class="math inline">\(Y_i=1\)</span>(ocorrência do evento) e <span class="math inline">\((1-P_i)\)</span>=probabilidade de <span class="math inline">\(Y_i=0\)</span>(não ocorrência do evento). <span class="math inline">\(Y_i\)</span> possui a seguinte <strong>distribuição de probabilidade de Bernoulli</strong>:</p>
<table>
<caption><span id="tab:bernoulireg">Tabela 7.2: </span></caption>
<thead>
<tr class="header">
<th align="center"><strong><span class="math inline">\(Y_i\)</span></strong></th>
<th align="center"><strong>Probabilidade</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(1-P_i\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(P_i\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>Total</strong></td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>Aplicando a esperança, obtemos:</p>
<p><span class="math display" id="eq:esperbernoulli">\[\begin{equation}
    E(Y_i)=0(1-P_i)+1(P_i)=P_i
    \tag{7.17}
\end{equation}\]</span></p>
<p>Igualando <a href="Algoritmosaprendizagem.html#eq:esperbernoulli">(7.17)</a> com <a href="Algoritmosaprendizagem.html#eq:regcond">(7.16)</a>, obtemos:</p>
<p><span class="math display" id="eq:regprob">\[\begin{equation}
    E(Y_i|X_i)=\beta_0+\beta_1 X_i
    \tag{7.18}
\end{equation}\]</span></p>
<p>Isso verifica que a esperança condicional do modelo de regressão <a href="Algoritmosaprendizagem.html#eq:frp">(7.4)</a> pode ser interpretada como a probabilidade
condicional de <span class="math inline">\(Yi\)</span>. Note que, como explicado em <a href="dicio.html#dicio">4</a> sobre <strong>Distribuição Bernoulli</strong> e <strong>Distribuição Binominal</strong>, caso haja <span class="math inline">\(n\)</span> observações independentes, cada um com uma probabilidade <span class="math inline">\(p\)</span> (sucesso) e probabilidade <span class="math inline">\((1 - p)\)</span> (fracasso) e <span class="math inline">\(X\)</span> dessas observações representarem o número de sucessos, <span class="math inline">\(X\)</span> então segue a distribuição binomial (com médi <span class="math inline">\(np\)</span> e variância <span class="math inline">\(np(1-p)\)</span>. Lembrando que a probabilidade <span class="math inline">\(P_i\)</span> situa-se entre 0 e 1 <span class="math inline">\(\rightarrow 0 \leq E(Y_i|X_i) \leq 1\)</span>.</p>
<p>Alguns detalhes importantes:</p>
<ul>
<li><p>A hipótese de normalidade de <span class="math inline">\(\mu_i\)</span> não se verifica no caso dos modelos de probabilidade linear, pois os termos de erro assumem também apenas dois valores, seguindo a distribuição de Bernoulli. Se objetivo for a estimação pontual, a hipótese de normalidade deixa de ser necessária <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>)</span> e que conforme aumentamos o tamanho da amostra indefinidamente, os estimadores de MQO tendem geralmente a distribuir-se normalmente.</p></li>
<li><p>Como sabe-se, a média e variância de uma distribuição Bernoulli possuem respectivamente <span class="math inline">\(p\)</span> e <span class="math inline">\(p(1-p)\)</span>. Logo a variância é heterocedástica <span class="math inline">\(var(\mu_i)=P_i(1-P_i)\)</span> e portanto os estimadores de MQO não são eficientes (não possuem variância mínima). Podemos fazer a transformação para que seja homocedástico:
<span class="math display">\[\sqrt{E(Y_i|X_i)-[1-E(Y_i|X_i)]}=\sqrt{P_i(1-P_i)=\sqrt{w_i}}\]</span>
<span class="math display" id="eq:probhomecedastico">\[\begin{equation}
  \frac{Y_i}{\sqrt{w)i}} = \frac{\beta_0}{\sqrt{w)i}}+\frac{\beta_1 X_i}{\sqrt{w)i}}+\frac{\mu_i}{\sqrt{w)i}}
  \tag{7.19}
\end{equation}\]</span></p></li>
</ul>
<p>Com a transformação, pode-se calcular por MQO (ponderados).</p>
<p><strong>Alternativas para o MPL:</strong></p>
<ul>
<li><p>Como mencionado, a probabilidade condicional situa-se entre <span class="math inline">\(0\)</span> e <span class="math inline">\(1\)</span>, porém por MQO não levarem em conta esta restrição. Pode-se verificar os valores que constam entre o intervalo, considerando os valores negativos como <span class="math inline">\(0\)</span> e maiores que <span class="math inline">\(1\)</span> como iguais a <span class="math inline">\(1\)</span> ou aplicar algum outro modelo para garanti-los dentro dos intervalos.</p></li>
<li><p>O <span class="math inline">\(R^2\)</span> costuma-se situar muito abaixo de 1. Por ser limitado em caso de modelos binários, muitos pesquisadores buscam evitar seu uso.</p></li>
</ul>
<p>Os modelos mais comuns para ser utilizado como alternativa ao MPL são o <strong>logit</strong> e o <strong>probit</strong> para evitar estes problemas.</p>
<div id="logit" class="section level4">
<h4><span class="header-section-number">7.2.3.1</span> Logit</h4>
<p>A fim de fazer com que <span class="math inline">\(P_i\)</span> varie entre 0 e 1 e relacione-se linearmente a <span class="math inline">\(X_i\)</span>, a <strong>função de distribuição logística</strong> pode ser expressa como:</p>
<p><span class="math display" id="eq:logitpi">\[\begin{equation}
    P_i=\frac{1}{1+e^{-Z_i}}=\frac{e^Z_i}{1+e^Z_i}
    \tag{7.20}
\end{equation}\]</span></p>
<p>e <span class="math inline">\((1-P_i)\)</span> da probabilidade fracasso:</p>
<p><span class="math display" id="eq:logitmenospi">\[\begin{equation}
    1-P_i=\frac{1}{1+e^{Z_i}}\rightarrow e^{Z_i}
    \tag{7.21}
\end{equation}\]</span></p>
<p>onde <span class="math inline">\(Z_i=\beta_0+\beta_1X_i\)</span>. Assim <span class="math inline">\(Z_i\)</span> varia de <span class="math inline">\(-\infty\)</span> a <span class="math inline">\(\infty\)</span> e portanto <span class="math inline">\(P_i\)</span> entre 0 e 1.</p>
<p>Para estimarmos a MQO, precisamos linearizar a função:</p>
<p><span class="math display" id="eq:logitlinear">\[\begin{equation}
    L_i=ln(\frac{P_i}{1-P_i})=Z_i=\beta_0+\beta_1 X_i
    \tag{7.22}
\end{equation}\]</span></p>
<p>O modelo <strong>logit</strong> faz com que:</p>
<ul>
<li><p>A probabilidade varie entre 0 e 1, enquanto <span class="math inline">\(Z\)</span> e <span class="math inline">\(L\)</span> possam variar de <span class="math inline">\(-\infty\)</span> a <span class="math inline">\(\infty\)</span>;</p></li>
<li><p>Mesmo que as probabilides não sejam lineares, <span class="math inline">\(L\)</span> é linear em <span class="math inline">\(X\)</span>;</p></li>
<li><p>Pode-se aplicar com mais regressores e com mesma interpretação angular medindo a variação de <span class="math inline">\(L\)</span> para uma unidade variação em <span class="math inline">\(X\)</span> e para o intercepto;</p></li>
<li><p>Se <span class="math inline">\(L\)</span> torna-se maior e positivo quando as chances do evento de interesse ocorrer aumenta, do contrário (maior e negativo) de não ocorrer;</p></li>
<li><p>Como em MPL, o modelo Logit é heterocedástico precisa-se ponderar <span class="citation">(Gujarati and Porter <a href="#ref-gujarati2011econometria">2011</a>; Cox <a href="#ref-cox1970analysis">1970</a>)</span>:
<span class="math display" id="eq:mqplogit">\[\begin{equation}
  \sqrt{w_i}L_i=\beta_0 \sqrt{w_i}+\beta_1\sqrt{w_i}X_i+\sqrt{w_i}\mu_i 
  \tag{7.23}
\end{equation}\]</span></p></li>
</ul>
<p>em que, com a variância <span class="math inline">\(\hat{\sigma}^2=\frac{1}{N_i\hat{P_i}(1-\hat{P_i})}\)</span>, <span class="math inline">\(W_i\)</span> é o peso <span class="math inline">\(N_i\hat{P_i}(1-\hat{P_i})\)</span>. Por fim, aplicar o mínimos quadrados ponderados (da mesma forma que MQO, porém com a nova transformação de dados) e estimarmos os parâmetros normalmente.</p>
<p>Como o <span class="math inline">\(R^2\)</span> não é significativa nos modelos binários. É comum utilizar as <strong>pseudo $R^2</strong> [long1997regression] - existe uma variedade delas - ou o <strong>Count <span class="math inline">\(R^2\)</span></strong> que nada mais é que o número de previsões corretas com o número total de observações. Para a hiótese nula de que todos os coeficientes angulares são simultâneamente iguais a zero, utiliza-se a <strong>estatística da razão de verossimilhança</strong> que segue a distribuição <span class="math inline">\(\chi^2\)</span> que equivale ao teste F.</p>
<p>Ressalto que existe muitos outros modelo de regressão, como por exemplo os modelos <strong>Probit</strong> e <strong>Tobit</strong>. Podemos dizer que possuem em geral os mesmos fundamentos da regressão que conhecemos, porém possuem algumas particularidades como a distribuição acumulada adequada dependendo da situação. O modelo <strong>Probit</strong> é muito utilizado quando supomos de que na distribuição do termo de erro, segue uma distribuição normal e utilizamos um limiar como referência para podermos estimar a probabilidade (possui resultados semelhantes de Logit). Ao caso da <strong>Tobit</strong> é muito utilizada para estimar relações com variáveis dependentes censuradas (por exemplo <span class="math inline">\(Y_i=Y_i^* \ \mbox{para} \ Y_i^* &gt; 0 \ \mbox{e} \ Y_i=0 \ \mbox{para} \ Y_i^*\leq 0\)</span>). Importante o pesquisador preparar seus dados e ter ciência de qual problema tratar e como lidar com este problema, para que se aplique um modelo adequado à situação.</p>

</div>
</div>
<div id="exemplo1reg" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Exemplos</h3>
<p>Com base em <span class="citation">Morettin and BUSSAB (<a href="#ref-morettin2017estatistica">2017</a>)</span>, vamos a alguns exemplos de regressão linear simples:</p>
<ol style="list-style-type: decimal">
<li>A tabela a seguir, apresenta dados sobre o índice de mortalidade por câncer de pulmão (100=média) e o índice de consumo de fumo (100=média) para 25 grupos ocupacionais.</li>
</ol>
<table>
<caption><span id="tab:cancercigarro">Tabela 7.3: </span> Índice de mortalidade por câncer de pulmão e índice de consumo de fumo para 25 grupos sociais. Disponível em: <a href="http://lib.stat.cmu.edu/datasets/" class="uri">http://lib.stat.cmu.edu/datasets/</a> <em>apud</em> <span class="citation">(Morettin and BUSSAB <a href="#ref-morettin2017estatistica">2017</a>)</span>, p. 160.</caption>
<thead>
<tr class="header">
<th align="center"><strong>Ocupação</strong></th>
<th align="center"><strong>Câncer</strong></th>
<th align="center"><strong>Fumo</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Fazendeiro, profissionais de atividades florestais, pescador</td>
<td align="center">84</td>
<td align="center">77</td>
</tr>
<tr class="even">
<td align="center">Minerador, cavouqueiro</td>
<td align="center">116</td>
<td align="center">137</td>
</tr>
<tr class="odd">
<td align="center">Operários da produção de combustíveis, coque e produtos químicos</td>
<td align="center">123</td>
<td align="center">117</td>
</tr>
<tr class="even">
<td align="center">Vidraceiro e ceramista</td>
<td align="center">128</td>
<td align="center">94</td>
</tr>
<tr class="odd">
<td align="center">Fundidor</td>
<td align="center">155</td>
<td align="center">116</td>
</tr>
<tr class="even">
<td align="center">Operários da fabricação de eletroeletrônicos</td>
<td align="center">101</td>
<td align="center">102</td>
</tr>
<tr class="odd">
<td align="center">Profissionais de engenharia e atividades associadas</td>
<td align="center">118</td>
<td align="center">111</td>
</tr>
<tr class="even">
<td align="center">Madereiros, marceneiros</td>
<td align="center">113</td>
<td align="center">93</td>
</tr>
<tr class="odd">
<td align="center">Curtidores em confecção de artigos de couro</td>
<td align="center">104</td>
<td align="center">88</td>
</tr>
<tr class="even">
<td align="center">Operários da fabricação de artigos têxtis</td>
<td align="center">88</td>
<td align="center">102</td>
</tr>
<tr class="odd">
<td align="center">Operários da confecção de vestuário</td>
<td align="center">104</td>
<td align="center">91</td>
</tr>
<tr class="even">
<td align="center">Profissionais da produção de alimentos, bebidas e tabaco</td>
<td align="center">129</td>
<td align="center">104</td>
</tr>
<tr class="odd">
<td align="center">Operários da fabricação de papel e atividades gráficas</td>
<td align="center">86</td>
<td align="center">107</td>
</tr>
<tr class="even">
<td align="center">Operários da fabricação de outros produtos</td>
<td align="center">96</td>
<td align="center">112</td>
</tr>
<tr class="odd">
<td align="center">Operários da construção civil</td>
<td align="center">144</td>
<td align="center">113</td>
</tr>
<tr class="even">
<td align="center">Pintores e decoradores</td>
<td align="center">139</td>
<td align="center">110</td>
</tr>
<tr class="odd">
<td align="center">Operadores de máquinas, guindastes etc.</td>
<td align="center">113</td>
<td align="center">125</td>
</tr>
<tr class="even">
<td align="center">Operários não incluídos nestas categorias</td>
<td align="center">146</td>
<td align="center">113</td>
</tr>
<tr class="odd">
<td align="center">Profissionais de transportes e comunicações</td>
<td align="center">128</td>
<td align="center">115</td>
</tr>
<tr class="even">
<td align="center">Estoquistas em armazéns, depósitos e lojas, almoxarifes…</td>
<td align="center">115</td>
<td align="center">105</td>
</tr>
<tr class="odd">
<td align="center">Escreventes, escriturários, funcionários de escritórios</td>
<td align="center">79</td>
<td align="center">87</td>
</tr>
<tr class="even">
<td align="center">Vendedores</td>
<td align="center">85</td>
<td align="center">91</td>
</tr>
<tr class="odd">
<td align="center">Profisisonais de seviços, esportes e recreadores</td>
<td align="center">120</td>
<td align="center">100</td>
</tr>
<tr class="even">
<td align="center">Administradores e gerentes</td>
<td align="center">60</td>
<td align="center">76</td>
</tr>
<tr class="odd">
<td align="center">Artistas e proissionais e técnicos em geral</td>
<td align="center">51</td>
<td align="center">66</td>
</tr>
</tbody>
</table>
<div style="page-break-after: always;"></div>
<p><strong>a.</strong> Trace o gráfico de mortalidade por câncer de pulmão em relação ao índice de fumo. Que padrão podemos observar?</p>
<div class="figure" style="text-align: center"><span id="fig:cancerfumo"></span>
<img src="Figuras/cancerfumo.png" alt="Gráfico de mortalidade por câncer em relação ao índice de fumo." width="70%" />
<p class="caption">
Figura 7.7: Gráfico de mortalidade por câncer em relação ao índice de fumo.
</p>
</div>

<p>Podemos verificar que conforme aumenta o número do índice de fumo, aumenta o índice de mortalidade por câncer. Visualizando o gráfico podemos perceber que possui uma correlação alta e positiva entre as variáveis.</p>
<p><strong>b.</strong> Considerando Y = índice de mortalidade por câncer de pulmão e X = índice de fumo, estime um mdelo de regressão linear e obtenha as estatísticas de regressão.</p>
<p>Ao calcularmos a média de X e Y, <span class="math inline">\(\overline{Y}=109,0\)</span> e <span class="math inline">\(\overline{X}=102,08\)</span>, podemos calcular <span class="math inline">\(y_i=Y_i-\overline{Y}\)</span> e <span class="math inline">\(x_i=X_i-\overline{X}\)</span> entre outras estatísticas habituais.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Câncer</th>
<th align="center">Fumo</th>
<th align="center"><span class="math inline">\(y_i\)</span></th>
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(x^2_i\)</span></th>
<th align="center"><span class="math inline">\(y_ix_i\)</span></th>
<th align="center"><span class="math inline">\(y^2_i\)</span></th>
<th align="center"><span class="math inline">\(X^2_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td align="center">84</td>
<td align="center">77</td>
<td align="center">-25</td>
<td align="center">-25</td>
<td align="center">629.01</td>
<td align="center">627</td>
<td align="center">625</td>
<td align="center">5929</td>
</tr>
<tr class="even">
<td></td>
<td align="center">116</td>
<td align="center">137</td>
<td align="center">7</td>
<td align="center">35</td>
<td align="center">1219.41</td>
<td align="center">244.44</td>
<td align="center">49</td>
<td align="center">18769</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">123</td>
<td align="center">117</td>
<td align="center">14</td>
<td align="center">15</td>
<td align="center">222.61</td>
<td align="center">208.88</td>
<td align="center">196</td>
<td align="center">13689</td>
</tr>
<tr class="even">
<td></td>
<td align="center">128</td>
<td align="center">94</td>
<td align="center">19</td>
<td align="center">-8</td>
<td align="center">65.29</td>
<td align="center">-153.52</td>
<td align="center">361</td>
<td align="center">8836</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">155</td>
<td align="center">116</td>
<td align="center">46</td>
<td align="center">14</td>
<td align="center">193.77</td>
<td align="center">640.32</td>
<td align="center">2116</td>
<td align="center">13456</td>
</tr>
<tr class="even">
<td></td>
<td align="center">101</td>
<td align="center">102</td>
<td align="center">-8</td>
<td align="center">0</td>
<td align="center">0.01</td>
<td align="center">0.64</td>
<td align="center">64</td>
<td align="center">10404</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">118</td>
<td align="center">111</td>
<td align="center">9</td>
<td align="center">9</td>
<td align="center">79.57</td>
<td align="center">80.28</td>
<td align="center">81</td>
<td align="center">12321</td>
</tr>
<tr class="even">
<td></td>
<td align="center">113</td>
<td align="center">93</td>
<td align="center">4</td>
<td align="center">-9</td>
<td align="center">82.45</td>
<td align="center">-36.32</td>
<td align="center">16</td>
<td align="center">8649</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">104</td>
<td align="center">88</td>
<td align="center">-5</td>
<td align="center">-14</td>
<td align="center">198.25</td>
<td align="center">70.4</td>
<td align="center">25</td>
<td align="center">7744</td>
</tr>
<tr class="even">
<td></td>
<td align="center">88</td>
<td align="center">102</td>
<td align="center">-21</td>
<td align="center">0</td>
<td align="center">0.01</td>
<td align="center">1.68</td>
<td align="center">441</td>
<td align="center">10404</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">104</td>
<td align="center">91</td>
<td align="center">-5</td>
<td align="center">-11</td>
<td align="center">122.77</td>
<td align="center">55.4</td>
<td align="center">25</td>
<td align="center">8281</td>
</tr>
<tr class="even">
<td></td>
<td align="center">129</td>
<td align="center">104</td>
<td align="center">20</td>
<td align="center">2</td>
<td align="center">3.69</td>
<td align="center">38.4</td>
<td align="center">400</td>
<td align="center">10816</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">86</td>
<td align="center">107</td>
<td align="center">-23</td>
<td align="center">5</td>
<td align="center">24.21</td>
<td align="center">-113.16</td>
<td align="center">529</td>
<td align="center">11449</td>
</tr>
<tr class="even">
<td></td>
<td align="center">96</td>
<td align="center">112</td>
<td align="center">-13</td>
<td align="center">10</td>
<td align="center">98.41</td>
<td align="center">-128.96</td>
<td align="center">169</td>
<td align="center">12544</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">144</td>
<td align="center">113</td>
<td align="center">35</td>
<td align="center">11</td>
<td align="center">119.25</td>
<td align="center">382.2</td>
<td align="center">1225</td>
<td align="center">12769</td>
</tr>
<tr class="even">
<td></td>
<td align="center">139</td>
<td align="center">110</td>
<td align="center">30</td>
<td align="center">8</td>
<td align="center">62.73</td>
<td align="center">237.6</td>
<td align="center">900</td>
<td align="center">12100</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">113</td>
<td align="center">125</td>
<td align="center">4</td>
<td align="center">23</td>
<td align="center">525.33</td>
<td align="center">91.68</td>
<td align="center">16</td>
<td align="center">15625</td>
</tr>
<tr class="even">
<td></td>
<td align="center">146</td>
<td align="center">113</td>
<td align="center">37</td>
<td align="center">11</td>
<td align="center">119.25</td>
<td align="center">404.04</td>
<td align="center">1369</td>
<td align="center">12769</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">128</td>
<td align="center">115</td>
<td align="center">19</td>
<td align="center">13</td>
<td align="center">166.93</td>
<td align="center">245.48</td>
<td align="center">361</td>
<td align="center">13225</td>
</tr>
<tr class="even">
<td></td>
<td align="center">115</td>
<td align="center">105</td>
<td align="center">6</td>
<td align="center">3</td>
<td align="center">8.53</td>
<td align="center">17.52</td>
<td align="center">36</td>
<td align="center">11025</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">79</td>
<td align="center">87</td>
<td align="center">-30</td>
<td align="center">-15</td>
<td align="center">227.41</td>
<td align="center">452.4</td>
<td align="center">900</td>
<td align="center">7569</td>
</tr>
<tr class="even">
<td></td>
<td align="center">85</td>
<td align="center">91</td>
<td align="center">-24</td>
<td align="center">-11</td>
<td align="center">122.77</td>
<td align="center">265.92</td>
<td align="center">576</td>
<td align="center">8281</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">120</td>
<td align="center">100</td>
<td align="center">11</td>
<td align="center">-2</td>
<td align="center">4.33</td>
<td align="center">-22.88</td>
<td align="center">121</td>
<td align="center">10000</td>
</tr>
<tr class="even">
<td></td>
<td align="center">60</td>
<td align="center">76</td>
<td align="center">-49</td>
<td align="center">-26</td>
<td align="center">680.17</td>
<td align="center">1277.92</td>
<td align="center">2401</td>
<td align="center">5776</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">51</td>
<td align="center">66</td>
<td align="center">-58</td>
<td align="center">-36</td>
<td align="center">1301.77</td>
<td align="center">2092.64</td>
<td align="center">3364</td>
<td align="center">4356</td>
</tr>
<tr class="even">
<td>Soma</td>
<td align="center">2725</td>
<td align="center">2552</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">6278</td>
<td align="center">6980</td>
<td align="center">16366</td>
<td align="center">266786</td>
</tr>
</tbody>
</table>
<p>Podemos agora calcular <span class="math inline">\(\hat{\beta_1}=\frac{\sum x_i y_i}{\sum x_i^2}=\frac{6980}{6278}\approx 1,112\)</span> e portanto, podemos encontrar <span class="math inline">\(\hat{\beta}_0=\overline{Y}_i-\hat{\beta}_1\overline{X}_i \rightarrow 109,000-1.112 \ . \ 102,080\approx -4,50\)</span>.</p>
<p>Agora que temos os estimadores <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span>, podemos encontrar <span class="math inline">\(\hat{y}_i=\beta_0+\beta_1 . X_i\)</span>, o erro <span class="math inline">\(\mu_i=Y_i-\hat{y}_i\)</span> e seu quadrado <span class="math inline">\(\mu^2\)</span>.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(\hat{y}\)</span></th>
<th align="center"><span class="math inline">\(\mu_i=Y_i-\hat{y}_i\)</span></th>
<th align="center"><span class="math inline">\(\mu^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">81.11</td>
<td align="center">2.89</td>
<td align="center">8.32</td>
</tr>
<tr class="even">
<td align="center">147.83</td>
<td align="center">-31.83</td>
<td align="center">1012.88</td>
</tr>
<tr class="odd">
<td align="center">125.59</td>
<td align="center">-2.59</td>
<td align="center">6.70</td>
</tr>
<tr class="even">
<td align="center">100.02</td>
<td align="center">27.98</td>
<td align="center">783.09</td>
</tr>
<tr class="odd">
<td align="center">124.48</td>
<td align="center">30.52</td>
<td align="center">931.66</td>
</tr>
<tr class="even">
<td align="center">108.91</td>
<td align="center">-7.91</td>
<td align="center">62.58</td>
</tr>
<tr class="odd">
<td align="center">118.92</td>
<td align="center">-0.92</td>
<td align="center">0.84</td>
</tr>
<tr class="even">
<td align="center">98.90</td>
<td align="center">14.10</td>
<td align="center">198.69</td>
</tr>
<tr class="odd">
<td align="center">93.35</td>
<td align="center">10.65</td>
<td align="center">113.53</td>
</tr>
<tr class="even">
<td align="center">108.91</td>
<td align="center">-20.91</td>
<td align="center">437.27</td>
</tr>
<tr class="odd">
<td align="center">96.68</td>
<td align="center">7.32</td>
<td align="center">53.57</td>
</tr>
<tr class="even">
<td align="center">111.13</td>
<td align="center">17.87</td>
<td align="center">319.17</td>
</tr>
<tr class="odd">
<td align="center">114.47</td>
<td align="center">-28.47</td>
<td align="center">810.56</td>
</tr>
<tr class="even">
<td align="center">120.03</td>
<td align="center">-24.03</td>
<td align="center">577.42</td>
</tr>
<tr class="odd">
<td align="center">121.14</td>
<td align="center">22.86</td>
<td align="center">522.52</td>
</tr>
<tr class="even">
<td align="center">117.81</td>
<td align="center">21.19</td>
<td align="center">449.19</td>
</tr>
<tr class="odd">
<td align="center">134.48</td>
<td align="center">-21.48</td>
<td align="center">461.54</td>
</tr>
<tr class="even">
<td align="center">121.14</td>
<td align="center">24.86</td>
<td align="center">617.95</td>
</tr>
<tr class="odd">
<td align="center">123.37</td>
<td align="center">4.63</td>
<td align="center">21.48</td>
</tr>
<tr class="even">
<td align="center">112.25</td>
<td align="center">2.75</td>
<td align="center">7.58</td>
</tr>
<tr class="odd">
<td align="center">92.23</td>
<td align="center">-13.23</td>
<td align="center">175.12</td>
</tr>
<tr class="even">
<td align="center">96.68</td>
<td align="center">-11.68</td>
<td align="center">136.44</td>
</tr>
<tr class="odd">
<td align="center">106.69</td>
<td align="center">13.31</td>
<td align="center">177.23</td>
</tr>
<tr class="even">
<td align="center">80.00</td>
<td align="center">-20.00</td>
<td align="center">400.12</td>
</tr>
<tr class="odd">
<td align="center">68.88</td>
<td align="center">-17.88</td>
<td align="center">319.86</td>
</tr>
<tr class="even">
<td align="center">2725</td>
<td align="center">0</td>
<td align="center">8605,31</td>
</tr>
</tbody>
</table>
<p>Sua variância, com <span class="math inline">\(n=25\)</span> observações, <span class="math inline">\(\sigma^2=\frac{\sum \mu_i^2}{n-2}=\frac{8605,31}{23}=374,144\)</span> e o coeficiente de determinação <span class="math inline">\(r^2=1-\frac{\sum \mu^2_i}{\sum y^2_i}=1-\frac{8605}{16366}=0,4742\)</span>.</p>
<p>O ajuste da reta de regressão linear será portanto <span class="math inline">\(\hat{y}=-4,50+1,11x\)</span>.</p>
<p><strong>c.</strong> Teste a hipótese de que o fumo não tem influência sobre o câncer de pulmão com nível de significância <span class="math inline">\(\alpha=5\%\)</span>.</p>
<p>Queremos
<span class="math display">\[H_0: \ \hat{\beta_1}=0\]</span>
<span class="math display">\[H_1: \ \hat{\beta_1}\neq 0\]</span></p>
<p>Temos que <span class="math inline">\(SQR = \sum \hat{\mu}^2 = 8605,31\)</span> e <span class="math inline">\(SQ Total = \sum \hat{y}^2_i=16366\)</span>. Logo <span class="math inline">\(SQE = SQT - SQR \approx 7760,695.\)</span> Por fim, os Quadrados médios que referem-se a divisão das somas dos quadrados pelos seus respectivos graus de liberdade, podemos calcular e verificar o valor F tabelado.</p>
<table style="width:100%;">
<colgroup>
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th>C.V</th>
<th>G.L</th>
<th>S.Q</th>
<th>Q.M</th>
<th>F.C (<span class="math inline">\(\frac{Q.M. \  SQE}{Q.M.\ SQR}\)</span>)</th>
<th>F. Tabelado (<span class="math inline">\(1,n-2,\alpha=5\%\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regressão</td>
<td>1</td>
<td>7760,695</td>
<td>7760,695</td>
<td>20,743</td>
<td>4,280</td>
</tr>
<tr class="even">
<td>Erro</td>
<td>23</td>
<td>8605</td>
<td>374,144</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td>24</td>
<td>16366</td>
<td>681,917</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Como <span class="math inline">\(F.C &gt; F. Tabelado\)</span>. Rejeita-se <span class="math inline">\(H_0\)</span> e há relação linear e influência sobre o câncer de pulmão com 5% de significância. Por meio do uso do software R, chegou-se a um p-valor de 0.000141, pode-se rejeitar <span class="math inline">\(H_0\)</span> (é significativo) com baixa porcentagem de significância.</p>
<ol start="2" style="list-style-type: decimal">
<li>A seguir, os dados são referentes a porcentagem da população economicamente ativa empregada no setor primário e o respectivo índice de analfabetismo para algumas regiões metropoliatanas brasileiras.</li>
</ol>
<table>
<caption><span id="tab:ecoativa">Tabela 7.4: </span> Indicadores Sociais para Áreas Urbanas, IBGE, 1977 <em>apud</em> <span class="citation">(Morettin and BUSSAB <a href="#ref-morettin2017estatistica">2017</a>)</span>, p. 92.</caption>
<thead>
<tr class="header">
<th><strong>Regiões Metropolitanas</strong></th>
<th><strong>Setor Primário (X)</strong></th>
<th><strong>Índice de Analfabetismo (Y)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>São Paulo</td>
<td>2,0</td>
<td>17,5</td>
</tr>
<tr class="even">
<td>Rio de Janerio</td>
<td>2,5</td>
<td>18,5</td>
</tr>
<tr class="odd">
<td>Belém</td>
<td>2,9</td>
<td>19,5</td>
</tr>
<tr class="even">
<td>Belo Horizonte</td>
<td>3,3</td>
<td>22,2</td>
</tr>
<tr class="odd">
<td>Salvador</td>
<td>4,1</td>
<td>26,5</td>
</tr>
<tr class="even">
<td>Porto Alegre</td>
<td>4,3</td>
<td>16,6</td>
</tr>
<tr class="odd">
<td>Recife</td>
<td>7,0</td>
<td>36,6</td>
</tr>
<tr class="even">
<td>Fortaleza</td>
<td>13,0</td>
<td>38,4</td>
</tr>
</tbody>
</table>
<p><strong>a.</strong> Sabedo que a reta de regressão linear simples ajustada é <span class="math inline">\(\hat{y}=13,561+2,289x\)</span>, faça o teste de significância:</p>
<p><span class="math display">\[SQT=\sum \hat{y}^2_i - \overline{Y}=5305,85-\frac{38298,49}{8}=518,538\]</span>
<span class="math display">\[SQE = 400,26 \ \mbox{e} \ SQR=118,12\]</span>
<span class="math display">\[\mbox{(encorajo-o ao leitor verificar)}\]</span>
Queremos
<span class="math display">\[H_0: \ \hat{\beta_1}=0\]</span>
<span class="math display">\[H_1: \ \hat{\beta_1}\neq 0\]</span></p>
<table style="width:100%;">
<colgroup>
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th>C.V</th>
<th>G.L</th>
<th>S.Q</th>
<th>Q.M</th>
<th>F.C (<span class="math inline">\(\frac{Q.M. \  SQE}{Q.M.\ SQR}\)</span>)</th>
<th>F. Tabelado (<span class="math inline">\(1,n-2,\alpha\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regressão (SQE)</td>
<td>1</td>
<td>400,26</td>
<td>400,26</td>
<td>20,33</td>
<td>5,99</td>
</tr>
<tr class="even">
<td>Erro (SQR)</td>
<td>6</td>
<td>118,12</td>
<td>19,686</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td>7</td>
<td>518,54</td>
<td>-</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Como <span class="math inline">\(F.C &gt; F. Tabelado\)</span>, rejeita-se <span class="math inline">\(H_0\)</span> à <span class="math inline">\(5\%\)</span> do nível de significância, à inclinação <span class="math inline">\(\beta_1\)</span> é diferente de zero e assim há relação linear simples entre o porcentual de analfabetismo e porcentual do setor primário.</p>
<p><strong>b.</strong> Estime o índice de analfabetismo para 20% da população empregada no setor primário.</p>
<p><span class="math display">\[\hat{y}=13,561+2,289 \ . \ 20\%=59,341\%\]</span>
O porcentual do valor estimado do índice de analfabetismo é de 59,341% para 20% da população empregada no setor primário.</p>
<p><strong>c.</strong> Determine o coeficiente de determinação e interprete seu resultado.</p>
<p><span class="math display">\[r^2=\frac{SQE}{SQT}=\frac{400,26}{518,54}=0,7719\]</span>
Logo, da variação total do índice de analfabetismo, 77% é explicado pela equação da reta.</p>

</div>
</div>
<div id="GD" class="section level2">
<h2><span class="header-section-number">7.3</span> Gradiente Descendente (GD)</h2>
<p>Para a obtenção dos parâmetros de forma analítica, como regressões, muitas vezes é difícil obter os parâmetros que minimizam determinada função de interesse. Dificuldades em obter a solução do sistema na forma fechada (ou não existir) ou quando <span class="math inline">\(n\)</span> é muito grande, o cálculo da inversa (estimando os parâmetros matricialmente) pode ser muito caro computacionalmente.</p>
<p>O <strong>Gradiente Descendente (GD)</strong> pode ser muito útil dependendo da situação, conhecido também como <strong>máximo declive</strong>, é um método númerico utilizado em otimização. Tem como finalidade identificar um mínimo local de uma função de modo iterativo, no qual a cada iteração toma-se a direção do gradiente. Muitas vezes serve como base para algoritmos de segunda ordem como Métodos de Newton, por exemplo.</p>
<p>É uma função para casos gerais, por praticidade vamos supor que temos uma função denominada custo com apenas dois parâmetros <span class="math inline">\(J(\theta_0,\theta_1)\)</span> e queremos estimar seus parâmetros que minimizam seus erros. Inicialmente atribuímos quaisquer estimativas iniciais para valores de <span class="math inline">\(\theta_0\)</span> e <span class="math inline">\(\theta_1\)</span>, com o GD vamos alterandos os valores dos <span class="math inline">\(\theta&#39;s\)</span> para reduzirmos <span class="math inline">\(J(\theta_0,\theta_1)\)</span> até que se chegue a um valor mínimo local.</p>
<p>Um exemplo que gosto muito, por <span class="citation">NG, Andrew Y. (<a href="#ref-andrewcoursera">2019</a>)</span>: observe a Figura <a href="Algoritmosaprendizagem.html#fig:gd">7.8</a> e imagine que você está em um campo, com dois montes. Mantenha sua imaginação de que está situado na cruz preta - ponto 0 - no primeiro monte vermelho. Com o GD vamos olhar 360 graus ao redor do ponto em que você está situado apenas para descobrir a resposta de que “se você fosse dar um pequeno passo em alguma direção ao seu redor com o objetivo de ir para o ponto mais baixo do campo o mais rápido possível, para qual direção você deve andar?”</p>
<p>Supondo que após olhar para todos os lados, com análise de GD você descobriu que seu primeiro passo será no ponto 1 da Figura <a href="Algoritmosaprendizagem.html#fig:gd">7.8</a>. Após isso, você observa novamente para todos os lados e faz outra análise de GD para verificar aonde você vai se deslocar em seu segundo passo para chegar o mais rápido possível até concluir que será o ponto 2. Assim, sucessivamente, você vai se deslocando para os respectivos pontos 3, 4 e sucessivamente até convergir em seu objetivo Z, porém caso você iniciasse pelo ponto K, é bem possível que por meio do GD você descesse o monte por outro trajeto, encontrando outros pontos ótimos locais até chegar a outro ponto otimizado (descer por completo o monte). Esta é a ideia do Gradiente Descendente, por meio de iterações, o algoritmo vai identificando os pontos ótimos (estimadores mínimos) até convergir num ótimo local da função.</p>
<p>Em caso de funções simples como regressão linear, não é necessário o uso de GD. Mas em casos com muitas variáveis e ordens, pode ser bem viável.</p>
<div class="figure" style="text-align: center"><span id="fig:gd"></span>
<img src="Figuras/gd.png" alt="Gráfico tridimensional a exemplo de Gradiente Descendente (NG, Andrew Y. 2019)." width="70%" />
<p class="caption">
Figura 7.8: Gráfico tridimensional a exemplo de Gradiente Descendente <span class="citation">(NG, Andrew Y. <a href="#ref-andrewcoursera">2019</a>)</span>.
</p>
</div>

<p>O algoritmo pode ser expresso como:</p>
<p><span class="math display" id="eq:GD">\[\begin{equation}
    \theta_j := \theta_j - \alpha \frac{d}{d \theta_j}J(\theta_0,\theta_1) \ \mbox{com} \ j=\theta_0 \ \mbox{e} \ j=\theta_1 
    \tag{7.24}
\end{equation}\]</span></p>
<p>com <span class="math inline">\(j\)</span> referindo-se à quantidade de observações (parâmetros que pretendemos estimar) da amostra.</p>
<p>O algoritmo é processado da seguinte forma: imagine na mesma Figura <a href="Algoritmosaprendizagem.html#fig:gd">7.8</a> que você irá dar seu primeiro passo, olhou os 360 graus e inseriu as variáveis em seu algoritmo de GD e seu destino é em <span class="math inline">\(Z=10\)</span>. Seu algoritmo calcula se você passou seu destino mais do que devia ou se você está atrás de <span class="math inline">\(Z\)</span> ainda e também verifica se precisa dar passos grandes por estar bem longe de seu destino, ou passos menores. Supondo que seu <span class="math inline">\(\alpha\)</span> um pouquinho alto, podemos dar um passo grande para descer o monte (1) pela diferença da observação que você inseriu com <span class="math inline">\(\alpha \frac{d}{d \theta_j}J(\theta_0,\theta_1)\)</span>. Caso fosse uma taxa pequena de <span class="math inline">\(\alpha\)</span>, seu passo seria menor e sua derivada (taxa de variação) vai lhe dizer se você passou do ponto ótimo de <span class="math inline">\(Z\)</span> (o quão a frente) ou está para trás (quão para trás) desse ponto ótimo.</p>
<p>Com o primeiro passo dado (supor passo <span class="math inline">\(1 = 40\)</span>), você precisa fazer o mesmo procedimento tomando agora o passo 1 como se fosse o inicial novamente, ou seja, atualizando sua função para cada <span class="math inline">\(\theta\)</span> <strong>simultâneamente</strong> (caso dois <span class="math inline">\(\theta\)</span>’s de entrada para a função, atualiza-se para ambos) até encontrar o novo valor ótimo do próximo passo no ponto <span class="math inline">\(2=15\)</span>. Conforme vai se aproximando de <span class="math inline">\(Z\)</span>, seus passos vão ficando cada vezes menores ( de 15 para 11; de 11 para 10,50; de 10,50 para 10,10; de 10,10 para 10,05; etc) até chegar na melhor aproximação de <span class="math inline">\(Z=10\)</span> que é o ponto ótimo da função.</p>
<p>Assim o algoritmo encontra os melhores parâmetros para buscar o ponto otimizado, com a estimatiza dos melhores parâmetros para a aproximação com os menores erros (sim! Podemos encontrar os parâmetros dos exemplos de regressão com este algoritmo também!)</p>
<p>Desta forma, atribuímos (“<span class="math inline">\(:=\)</span>”) para a própria observação de entrada da função receber ela mesma subtraída <span class="math inline">\(\alpha\)</span> que multiplica a derivada da função em relação a observação de entrada. Para que atualize a cada passo (iteração). <strong><span class="math inline">\(\alpha\)</span>( learning rate - taxa de aprendizagem)</strong> é um valor fixo que controla o tamanho do passo em cada iteração: quando <span class="math inline">\(\alpha\)</span> for pequeno, o método fica lento, quando grande ele pode falhar na convergência e até mesmo divergir. Seu valor depende muito da pesquisa e de suas fundamentações teóricas, o que recomendo o leitor quando utilizar este método verificar um valor adequado, pode ser que dependendo do valor da taxa demore muito para finalizar o algoritmo pela quantidade de iterações (tamanhos de passos muito pequenos) ou divergir (tamanho de passos muito grandes). <span class="citation">Rendle and Schmidt-Thieme (<a href="#ref-rendle2008online">2008</a>)</span> divulgaram que a fatoração de matrizes para a predição de <em>ratings</em> nos dados do desafio <em>Netflix</em> precisou de 200 iterações, usando uma taxa de aprendizagem de 0,01.</p>
<p>Para facilitar a compreensão do efeito da taxa de variação, observe a Figura <a href="Algoritmosaprendizagem.html#fig:gd1">7.9</a>. No primeiro gráfico você inicia seu algoritmo com o valor <span class="math inline">\(\theta\)</span> e com a derivada podemos observar que inclinação da reta tangente ao ponto é positiva (<span class="math inline">\(\frac{d}{d\theta}j(\theta)\geq 0\)</span>), portanto em <span class="math inline">\(\theta=\theta-\alpha.\mbox{um valor positivo}\)</span>, faz que com que esse novo <span class="math inline">\(\theta\)</span> (segunda iteração) seja menor que o da primeira iteração, visto que terá que subtrair e deslocar-se para esquerda para tender ao ponto mínimo. Da mesma forma, ao segundo gráfico, podemos verificar que a inclinação é negativa (<span class="math inline">\(\frac{d}{d\theta}j(\theta)\leq 0\)</span>), portanto <span class="math inline">\(\theta=\theta-\alpha.\mbox{um valor negativo}\)</span>, fará com que o novo <span class="math inline">\(\theta\)</span> seja maior do que da primeira iteração, pois irá somar e deslocar-se para direita tendendo ao ponto mínimo.</p>
<div class="figure" style="text-align: center"><span id="fig:gd1"></span>
<img src="Figuras/gd1.png" alt="Efeito da taxa de variação no Gradiente Descendente." width="70%" />
<p class="caption">
Figura 7.9: Efeito da taxa de variação no Gradiente Descendente.
</p>
</div>

<p>Como pode-se perceber, a taxa de aprendizagem e a taxa de variação são fundamentais e complementares para o algoritmo de GD, pois elas dizem o tamanho do passo e em que posição estamos em relação ao ponto ótimo da função.</p>
<div id="exemplos" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Exemplos</h3>
<ol style="list-style-type: decimal">
<li><strong>Uma variável:</strong>Vamos supor a seguinte função custo:</li>
</ol>
<p><span class="math display">\[j(\theta)=\theta^2\]</span></p>
<p>Queremos minimizá-la <span class="math inline">\(min \ j(\theta)\)</span>. Portanto precisamos inicialmente colocar um número aleatório para nosso parâmetro - não ótimo - para que o algoritmo atualize a cada iteração. Vamos supor a taxa de aprendizagem (<em>learning rate</em>) <span class="math inline">\(\alpha=0,1\)</span> e <span class="math inline">\(\theta=4\)</span> para facilitar. Ou seja, <span class="math inline">\(j(\theta)=4^2=16\)</span>. Vamos atualizar os parâmetros:</p>
<p><span class="math display">\[\theta := \theta-\alpha.\frac{d}{d\theta}j(\theta) \]</span>
<span class="math display">\[\mbox{derivando a função} \ j(\theta)=\theta^2 \ \mbox{e substituindo:}\]</span>
<span class="math display">\[\theta:= \theta -\alpha.2\theta \]</span>
<span class="math display">\[\mbox{substituindo os valores de}\ \alpha\ \mbox{e}\ \theta: \]</span>
<span class="math display">\[\theta:=4-0,1 \ .\ 2\ .\ 4 \]</span>
<span class="math display">\[\rightarrow \theta:=3,2\]</span></p>
<p>Na iteração obtemos <span class="math inline">\(\theta=3,2\)</span>. Se subsituirmos em <span class="math inline">\(j(\theta)\)</span> novamente, iremos obter <span class="math inline">\(j(\theta)=(3,2)^2=10,24\)</span>. Agora atualizando novamente para a próxima iteração:
<span class="math display">\[\theta:= \theta -\alpha.2\theta \]</span>
<span class="math display">\[\theta:=3,2-0,1\ .\ 2\ .\ 3,2\]</span>
<span class="math display">\[\theta:= 2,56\]</span></p>
<p>Portanto, <span class="math inline">\(j(\theta)=(2,56)^2=6,55\)</span>. Sucessivamente, vamos fazendo as iterações até convergir:</p>
<table>
<thead>
<tr class="header">
<th align="center"><strong><span class="math inline">\(\theta\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(j(\theta)\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">4</td>
<td align="center">16</td>
</tr>
<tr class="even">
<td align="center">3,2</td>
<td align="center">10,24</td>
</tr>
<tr class="odd">
<td align="center">2,56</td>
<td align="center">6,55</td>
</tr>
<tr class="even">
<td align="center">2,04</td>
<td align="center">4,19</td>
</tr>
<tr class="odd">
<td align="center">1,632</td>
<td align="center">2,663</td>
</tr>
<tr class="even">
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Da mesma forma, se iniciarmos o algoritmo com -4:</p>
<table>
<thead>
<tr class="header">
<th align="center"><strong><span class="math inline">\(\theta\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(j(\theta)\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-4</td>
<td align="center">16</td>
</tr>
<tr class="even">
<td align="center">-3,2</td>
<td align="center">10,24</td>
</tr>
<tr class="odd">
<td align="center">-2,56</td>
<td align="center">6,55</td>
</tr>
<tr class="even">
<td align="center">-2,04</td>
<td align="center">4,19</td>
</tr>
<tr class="odd">
<td align="center">-1,632</td>
<td align="center">2,663</td>
</tr>
<tr class="even">
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Note que conforme <span class="math inline">\(\theta\)</span> diminui, o custo também. Conforme mais iterações são aplicadas, mais “ótimo” será. Graficamente para -4 em vermelho e +4 em azul:</p>
<div class="figure" style="text-align: center"><span id="fig:gdx2"></span>
<img src="Figuras/gdx2.png" alt="Função \(X^2\) com valores de entrada -4 e +4." width="70%" />
<p class="caption">
Figura 7.10: Função <span class="math inline">\(X^2\)</span> com valores de entrada -4 e +4.
</p>
</div>

<ol start="2" style="list-style-type: decimal">
<li><strong>Duas variáveis:</strong> Vamos supor a seguinte função de custo com <span class="math inline">\(\alpha=0,1\)</span>, <span class="math inline">\(\theta_1=1\)</span> e <span class="math inline">\(\theta_2=2\)</span>:
<span class="math display">\[j(\theta_1,\theta_2)=\theta_1^2+\theta_2^2\]</span>
<span class="math display">\[j(\theta_1,\theta_2)=1^2+2^2=5\]</span>
Queremos <span class="math inline">\(min \ j(\theta_1,\theta_2)\)</span>Como explicado, ao caso de haver mais de um parâmetro precisamos separar atualizar cada um simultâneamente e aplicar derivada parcial em sua função:</li>
</ol>
<p><span class="math display">\[\theta_1:=\theta_1-\alpha \frac{d}{d\theta_1}j(\theta_1,\theta_2) \ \ \mbox{e}\ \ \theta_2:=\theta_2-\alpha \frac{d}{d\theta_2}j(\theta_1,\theta_2)\]</span>
<span class="math display">\[\mbox{calculando as derivadas parciais de}\ j(\theta_1,\theta_2)=\theta_1^2+\theta_2^2\ \mbox{obtemos:}\]</span>
<span class="math display">\[\frac{d}{d\theta_1}j(\theta_1,\theta_2)=2\theta_1 \ \
\mbox{e}\ \ \frac{d}{d\theta_2}j(\theta_1,\theta_2)=2\theta_2\]</span></p>
<p><span class="math display">\[\mbox{substituindo:}\]</span>
<span class="math display">\[\theta_1:=\theta_1-\alpha.\ 2\theta_1 \ \ \mbox{e}\ \ \theta_2:=\theta_2-\alpha .\ 2\theta_2 \]</span>
<span class="math display">\[\mbox{inserindo os valores:}\]</span>
<span class="math display">\[\theta_1:=1-0,1.\ 2.\ 1 \ \ \mbox{e}\ \ \theta_2:=2-0,1.\ 2.\ 2\]</span>
<span class="math display">\[\theta_1:=0,8 \ \mbox{e} \ \theta_2=1,6\]</span></p>
<p>Portanto após a iteração, temos que <span class="math inline">\(j(\theta_1,\theta_2)=0,8^2+1,6^2=3,2\)</span>. Da mesma forma, para a próxima iteração temos:</p>
<p><span class="math display">\[\theta_1:=0,8-0,1.\ 2.\ 0,8 \ \ \mbox{e}\ \ \theta_2:=1,6-0,1.\ 2.\ 1,6\]</span>
<span class="math display">\[\theta_1:=0,64 \ \mbox{e} \ \theta_2=1,28\]</span></p>
<p>Portanto teremos <span class="math inline">\(j(\theta_1,\theta_2)=0,64^2+1,28^2=2,048\)</span>. Assim sucessivamente:</p>
<table>
<thead>
<tr class="header">
<th align="center"><strong><span class="math inline">\(\theta_1\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(\theta_2\)</span></strong></th>
<th align="center"><strong><span class="math inline">\(j(\theta_1,\theta_2)\)</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">2</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">0,8</td>
<td align="center">1,6</td>
<td align="center">3,2</td>
</tr>
<tr class="odd">
<td align="center">0,64</td>
<td align="center">1,28</td>
<td align="center">2,48</td>
</tr>
<tr class="even">
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center"></td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<ol start="3" style="list-style-type: decimal">
<li><strong>Erro quadrado médio (Regressão Linear Simples:)</strong> Observe a função de regressão linear:
<span class="math display">\[f_\theta(X)=\theta_0+\theta_1*X\]</span></li>
</ol>
<p>A função de custo:
<span class="math display">\[j(\theta)=\frac{1}{m}\displaystyle \sum^m_{i=1}(f_\theta(x^i)-y^i)^2\]</span></p>
<p>Primeiramente vamos encontrar a derivada parcial de <span class="math inline">\(j(\theta_0,\theta_1)\)</span>:
<span class="math display">\[\frac{d}{d\theta_0}j(\theta_0,\theta_1=\frac{d}{d\theta_0}(\frac{1}{m}\displaystyle \sum^m_{i=1}(f_{\theta}(x^i)-y^i)^2) \rightarrow \frac{2}{m}\displaystyle \sum^m_{i=1}(f_\theta(x^i)-y^i) \]</span>
<span class="math display">\[\frac{d}{d\theta_1}j(\theta_0,\theta_1=\frac{d}{d\theta_1}(\frac{1}{m}\displaystyle \sum^m_{i=1}(f_{\theta}(x^i)-y^i)^2) \rightarrow \frac{2}{m}\displaystyle \sum^m_{i=1}(f_\theta(x^i)-y^i)x^i\]</span>
Pode-se também multiplicar a função de custo por <span class="math inline">\(\frac{1}{2}\)</span> para que quando faz-se a derivada, facilite no cálculo e multiplicar a função de custo por um escalar não irá afetar a localização do mínimo.
<span class="math display">\[j(\theta)=\frac{1}{2m}\displaystyle \sum^m_{i=1}(f_\theta(x^i)-y^i)^2\]</span>
Com isso em foco de minimizarmos, basta aplicarmos o banco de dados de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> em seu modelo e de seus dois <span class="math inline">\(\theta&#39;s\)</span> de entrada. Repetindo as iterações para atualizar seus valores até a convergência e identificando os parâmetros que se aproximam.</p>

</div>
</div>
<div id="regularizacao" class="section level2">
<h2><span class="header-section-number">7.4</span> Regularização</h2>
<p>Como sabe-se, um Modelo de Regressão Linear Simples (MRLS) pode ser expresso como:</p>
<p><span class="math display">\[\begin{equation}
    y_i=\beta_0+\displaystyle \sum^p_{j=1}x_{ij}\beta_j+\mu_i, \mbox{com} \ i=1,2,...,n
\end{equation}\]</span></p>
<p>em que <span class="math inline">\(y_i\)</span> é variável resposta, e cada vetor <span class="math inline">\(x_i=(x_{i1},x_{i2},...,x_{ip})^T\)</span> representa as <span class="math inline">\(p\)</span> características observadas para cada observação <span class="math inline">\(i\)</span> da amostra; <span class="math inline">\(\beta_0\)</span> o intercepto e o vetor dos coeficientes <span class="math inline">\(\beta=(\beta_1,\beta_2,...,\beta_p)^T\)</span> trata-se dos parâmetros a serem estimados e estabelecem a relação entre a variável resposta e as preditoras.</p>
<p>Sabemos também que métodos de seleção de variáveis como <em>backward, forward</em> e <em>stepwise</em> buscam amenizar o erro da predição e melhorar a interpretação do modelo, porém visto que o processo é discreto na escolha das variáveis regressoras (retendo ou descartando), o modelo resultante pode apresentar grande variância e portanto, não diminuir esse erro de predição quando compararmos com o modelo completo. A <strong>regularização</strong> é uma outra abordagem com estes mesmos propósitos, ela desestímula o ajuste excessivo dos dados, afim de diminuir sua variância. A regressão <strong><em>Lasso</em></strong> e a regressão <strong><em>Ridge</em></strong> são métodos utilizados para regularizar o modelo por meio de penalidades, alterando alguns fatores de modo a priorizar (ou não) partes da equação e por fim, melhorar a qualidade de predição.</p>
<div id="penalizacoes" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Penalizações - Regressão <em>Lasso</em> e a Regressão <em>Ridge</em></h3>
<p>O método dos mínimos quadrados, como sabemos, consiste em minimizar a soma dos quadrados dos resíduos do modelo e fatores como multicolinearidade por exemplo, que pode ser ou não controlados pelo pesquisador, pode influenciar na pesquisa e muitas vezes apontar como não significativas variáveis importantes. O MMQ quando penalizado e, partindo das mesmas suposições do MRLM, pode apresentar uma melhoria. Ela será expressa como:</p>
<p><span class="math display" id="eq:penalizacao">\[\begin{equation}
    min_\beta \bigg\{\displaystyle \sum^n_{i=1}(y_i-\displaystyle \sum^p_{j=1}x_{ij}\beta_j)^2+s\displaystyle \sum^p_{j=1}|\beta_j|^q\bigg\}
    \tag{7.25}
\end{equation}\]</span></p>
<p>onde <span class="math inline">\(s\)</span> é o fator de penalização, um parâmetro de ajuste (<em>tuning parameter</em>) no qual deve ser maior do que a soma dos valores absolutos dos coeficientes do modelo e o termo <span class="math inline">\(s\displaystyle \sum^p_{j=1}|\beta_j|^q\)</span> representa a penalização. Ao caso de <span class="math inline">\(q=0\)</span>, é contabilizado a quantidade de coeficientes não-nulos presentes no modelo, uma penalizção do tipo <strong><span class="math inline">\(L_0\)</span></strong>.</p>
<p>Para <span class="math inline">\(q=1\)</span> é a penalização do tipo <strong><span class="math inline">\(L_1\)</span></strong>, conhecida como <strong>Lasso</strong>. Robert Tibshirani propôs em seu artigo <em>Regression Shrinkage and Selection via the LASSO</em> <span class="citation">(Tibshirani <a href="#ref-tibshirani1996regression">1996</a>)</span>, é uma técnica que a cada dia torna maior seu uso no Aprendizado de Máquina com sua interessante possibilidade para seleção de variáveis. O <em>Lasso</em> pode ser usado em análises com um grande banco de dados, especialmente se a quantidade de covariáveis (qualquer variável contínua e geralmente não controlada durante a coleta de dados) for maior do que o número de observações e também garante que uma boa parte dos coeficientes destas covariáveis seja nula, sugerindo que as demais são as características importantes a serem analisadas <span class="citation">(Silva <a href="#ref-silva2018tecnica">2018</a>)</span>. Um modelo é <strong>esparso (sparse model)</strong> quando apenas alguns dos coeficientes possuem estimações diferentes de zero <span class="citation">(Hastie, Tibshirani, and Wainwright <a href="#ref-hastie2015statistical">2015</a>)</span>, ele melhora na interpretação com a vantagem de facilitar as estimações computacionais e pode fornecer maior precisão de predição.</p>
<p>A técnica <em>Lasso</em> basicamente minimiza a soma dos quadrados dos resíduos do modelo utilizando o parâmetro de ajuste <em>tuning parameter</em> <span class="math inline">\(s\)</span> que deve ser maior de que a soma dos valores absolutos dos coeficientes do modelo. Este parâmetro desempenha um papel fundamental, pois serve como um “grau” de quanto irá encolher durante a estimação, quanto maior seu valor, menor a distância entre os estimadores. Existe diversas técnicas para estimar seu valor.</p>
<p>Uma penalização <strong><span class="math inline">\(L_1\)</span></strong> que faz com que essa regularização <em>Lasso</em> force os coeficientes a zerar (quando há múltiplas atributos altamente correlacionados, selecionam apenas um desses atributos e zeram o coefiente das menos importantes, de forma a minimizar a penalização <span class="math inline">\(L_1\)</span>). Podemos entender que esse modelo realiza uma seleção de atributos (por este motivo o termo inglês <em>Shrinkage</em>, encolhimento), gerando vários coeficientes com peso zero e ignorados, facilitando na interpretação do modelo e diminuindo a variância. O <em>Lasso</em> pode conduzir a uma região de restrição convexa e connsequentemente a um problema de otimização convexo e usa seu mecanismo de penalizar os coeficientes de acordo com o seu valor absoluto.</p>
<p>Quando <span class="math inline">\(q=2\)</span>, temos a penalização do tipo <strong><span class="math inline">\(L_2\)</span></strong> que corresponde à regressão <strong><em>Ridge</em></strong> (regressão em cristas). O método <em>Ridge</em> foi introduzido originalmente por <span class="citation">Hoerl (<a href="#ref-hoerl1959optimum">1959</a>)</span> para examinar superfícies de resposta quadráticas <span class="math inline">\(k\)</span>-dimensionais, ou seja é um método gráfico e de inferência sobre os níveis ótimos de um fator de um superfície de resposta a distâncias fixas do centro da região experimental expecíficada <span class="citation">(Nascimento CHAGAS et al. <a href="#ref-do2009metodo">2009</a>)</span>.</p>
<p>Muitos também a utilizam como método alternativo ao MMQ no caso de haver multicolinearidade em uma amostra. Como a penalização <span class="math inline">\(L_2\)</span> é maior para coeficientes maiores por haver <span class="math inline">\(q=2\)</span>, ela faz com que os atributos que contribuem menos para o poder preditivo do modelo sejam levados para uma “irrelevância” em relação às de maior contribuição - como sabe-se o impacto de um expoente quadrático é maior em valores altos do que o impacto do mesmo expoente em valores bem pequenos - e têm como objetivo suavizar os atributos que sejam relacionados uns aos outros.</p>
<p>Pode-se dizer que a diferença entre <span class="math inline">\(L_1\)</span> e <span class="math inline">\(L_2\)</span>, num geral, é que a regressão Lasso não é diferenciável e a <span class="math inline">\(L_2\)</span> é, para <span class="math inline">\(q&lt;1\)</span> as regiões serão não convexas e para <span class="math inline">\(q&lt;1\)</span> os problemas serão convexos <span class="citation">(Silva <a href="#ref-silva2018tecnica">2018</a>)</span>. A penalização Ridge encolhe os parâmetros, mas não a seleção de variáveis. Ao caso da penalização de <em>Lasso</em> que possui uma penalização pelos valores absolutos ela encolhe e faz a seleção dos atributos.</p>
<div class="figure" style="text-align: center"><span id="fig:lassoridge2d"></span>
<img src="Figuras/lassoridge2d.png" alt="Regiões de restrição para valores diferentes de \(q\) em \(\mathbb{R}^2\). de Silva (2018) com base em Hastie, Tibshirani, and Wainwright (2015)." width="70%" />
<p class="caption">
Figura 7.11: Regiões de restrição para valores diferentes de <span class="math inline">\(q\)</span> em <span class="math inline">\(\mathbb{R}^2\)</span>. de <span class="citation">Silva (<a href="#ref-silva2018tecnica">2018</a>)</span> com base em <span class="citation">Hastie, Tibshirani, and Wainwright (<a href="#ref-hastie2015statistical">2015</a>)</span>.
</p>
</div>

<div class="figure" style="text-align: center"><span id="fig:lassoridge3d"></span>
<img src="Figuras/lassoridge3d.png" alt="Regiões de restrição para valores de \(q=\{2;1;0,8\}\) em \(\mathbb{R}^3\). de Silva (2018) com base em Hastie, Tibshirani, and Wainwright (2015)." width="70%" />
<p class="caption">
Figura 7.12: Regiões de restrição para valores de <span class="math inline">\(q=\{2;1;0,8\}\)</span> em <span class="math inline">\(\mathbb{R}^3\)</span>. de <span class="citation">Silva (<a href="#ref-silva2018tecnica">2018</a>)</span> com base em <span class="citation">Hastie, Tibshirani, and Wainwright (<a href="#ref-hastie2015statistical">2015</a>)</span>.
</p>
</div>

</div>
<div id="elasticnet" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Elastic Net - <span class="math inline">\(L_1+L_2\)</span></h3>
<p>A técnica <strong><em>Elastic Net</em></strong> <span class="citation">(Zou and Hastie <a href="#ref-zou2005regularization">2005</a>)</span> também tem como propósito a penalização para que se melhore a acurácia dos estimadores de mínimos quadrados. Ela se trata exatamente de combinar os termos de regularização de <span class="math inline">\(L_1\)</span> e de <span class="math inline">\(L_2\)</span>.</p>
<p><span class="math display" id="eq:elsticnet">\[\begin{equation}
    \lambda\bigg[ \frac{1}{2}(1-\alpha)||\beta||_2^2+\alpha||\beta||_1\bigg]
  \tag{7.26}
\end{equation}\]</span></p>
<p><span class="citation">Hastie, Tibshirani, and Wainwright (<a href="#ref-hastie2015statistical">2015</a>)</span> citam que o principal estímulo para este estudo é que quando as variáveis são muito correlacionadas, o <em>Lasso</em> não tem um bom desempenho. A Adiçaõ do termo <span class="math inline">\(\frac{1}{2}(1-\alpha)||\beta||^2_2\)</span> auxilia a controlar estas fortes correlações entre os grupos de variáveis e manter a característica da regressão Lasso de tornar modelos esparsos <span class="citation">(Silva <a href="#ref-silva2018tecnica">2018</a>)</span>. Como consequência do uso dos dois métodos, é preciso determinar dois hiperparâmetros para obter soluções ótimas.</p>
<p>Ao observar um gráfico de <em>Elastic Net</em>, pode-se observar de que este método compartilha de atributos gráficos de <span class="math inline">\(L_1\)</span> e <span class="math inline">\(L_2\)</span>: bordas e cantos afiados indicando a seleção e um contorno curvado para o compartilhamento de coeficientes.</p>
<div class="figure" style="text-align: center"><span id="fig:elasticnet"></span>
<img src="Figuras/elasticnet.png" alt="Elastic Net em \(\mathbb{R}^3\). de Silva (2018) com base em Hastie, Tibshirani, and Wainwright (2015)." width="70%" />
<p class="caption">
Figura 7.13: <em>Elastic Net</em> em <span class="math inline">\(\mathbb{R}^3\)</span>. de <span class="citation">Silva (<a href="#ref-silva2018tecnica">2018</a>)</span> com base em <span class="citation">Hastie, Tibshirani, and Wainwright (<a href="#ref-hastie2015statistical">2015</a>)</span>.
</p>
</div>


</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-cox1970analysis">
<p>Cox, DR. 1970. “Analysis of Binary Data London: Methuen &amp;Co.” Ltd.</p>
</div>
<div id="ref-gujarati2011econometria">
<p>Gujarati, Damodar N, and Dawn C Porter. 2011. <em>Econometria Básica-5</em>. Amgh Editora.</p>
</div>
<div id="ref-hastie2015statistical">
<p>Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC press.</p>
</div>
<div id="ref-hoerl1959optimum">
<p>Hoerl, Arthur E. 1959. “Optimum Solution of Many Variables Equations.” <em>Chemical Engineering Progress</em> 55 (11): 69–78.</p>
</div>
<div id="ref-kennedy1981ballentine">
<p>Kennedy, Peter E. 1981. “The ‘Ballentine’: A Graphical Aid for Econometrics.” <em>Australian Economic Papers</em> 20 (37): 414–16.</p>
</div>
<div id="ref-maroco2014analise">
<p>Maroco, João. 2014. “Análise Estatı́stica Com O Spss.” <em>Statistics</em> 6.</p>
</div>
<div id="ref-montgomery2012introduction">
<p>Montgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2012. <em>Introduction to Linear Regression Analysis</em>. Vol. 821. John Wiley &amp; Sons.</p>
</div>
<div id="ref-morettin2017estatistica">
<p>Morettin, Pedro Alberto, and WILTON OLIVEIRA BUSSAB. 2017. <em>Estatı́stica Básica</em>. Saraiva Educação SA.</p>
</div>
<div id="ref-do2009metodo">
<p>Nascimento CHAGAS, Elcio do, Camila Carvalho MENEZES, Marcelo Angelo CIRILLO, and Soraia Vilela BORGES. 2009. “Método ‘Ridge’ Em Modelo de Superfı́cie de Resposta: Otimização de Condições Experimentais Na Elaboração de Doce de Goiaba.” <em>Rev. Bras. Biom</em> 26 (4): 71–81.</p>
</div>
<div id="ref-andrewcoursera">
<p>NG, Andrew Y. 2019. “Gradient Descent Algorithm.” In. <a href="https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM">https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM</a>.</p>
</div>
<div id="ref-organica">
<p>Orgânica Digital. 2019. “Algoritmo de Classificação Naive Bayes.” In. <a href="https://www.organicadigital.com/blog/algoritmo-de-classificacao-naive-bayes/">https://www.organicadigital.com/blog/algoritmo-de-classificacao-naive-bayes/</a>.</p>
</div>
<div id="ref-rendle2008online">
<p>Rendle, Steffen, and Lars Schmidt-Thieme. 2008. “Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems.” In <em>Proceedings of the 2008 Acm Conference on Recommender Systems</em>, 251–58.</p>
</div>
<div id="ref-shapiro1965analysis">
<p>Shapiro, Samuel Sanford, and Martin B Wilk. 1965. “An Analysis of Variance Test for Normality (Complete Samples).” <em>Biometrika</em> 52 (3/4): 591–611.</p>
</div>
<div id="ref-silva2018tecnica">
<p>Silva, Carina Brunehilde Pinto da. 2018. “A Técnica Lasso E Suas Potencialidades Na Seleção de Variáveis Para Modelos Lineares.”</p>
</div>
<div id="ref-tibshirani1996regression">
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 58 (1): 267–88.</p>
</div>
<div id="ref-zou2005regularization">
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="valid.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ptII.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07.1-ModeloIFilho.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
